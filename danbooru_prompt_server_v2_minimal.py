from mcp.server.fastmcp import FastMCP
import torch
from FlagEmbedding import BGEM3FlagModel
import argparse
import logging
import time
import json
import numpy as np
from pathlib import Path
from typing import Optional, Dict, List, Any, Tuple, Union
import chromadb
from chromadb.config import Settings
import re
import traceback
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥ä¼˜åŒ–æ¨¡å—
try:
    from cache_system import global_cache
    from config_manager import config_manager
    from performance_monitor import global_monitor
    OPTIMIZATION_MODULES_LOADED = True
    logger.info("ğŸš€ æ‰€æœ‰ä¼˜åŒ–æ¨¡å—åŠ è½½æˆåŠŸï¼")
except ImportError as e:
    logger.warning(f"âš ï¸ ä¼˜åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    logger.warning("è¿è¡Œåœ¨å…¼å®¹æ¨¡å¼ä¸‹ï¼Œéƒ¨åˆ†ä¼˜åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")
    OPTIMIZATION_MODULES_LOADED = False
    global_cache = None
    config_manager = None
    global_monitor = None

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("Danbooruæœç´¢æœåŠ¡å™¨-æœ€å°ç‰ˆ-å¢å¼ºç‰ˆ")

# å¢å¼ºé…ç½® - BGE-M3ä¸‰é‡èƒ½åŠ›ç‰ˆæœ¬
# Denseå‘é‡(è¯­ä¹‰ç†è§£) + Sparseå‘é‡(å…³é”®è¯åŒ¹é…) + ColBERTå‘é‡(ç»†ç²’åº¦åŒ¹é…)
CONFIG = {
    "max_length": 8192,  # å¢åŠ åˆ°8192ä»¥æ”¯æŒæ›´é•¿æ–‡æœ¬
    "default_results": 20,
    "max_results": 200,  # å¢åŠ æœ€å¤§ç»“æœæ•°
    "batch_size": 8,
    "database_mode": False,  # æ•°æ®åº“æ¨¡å¼æ ‡å¿—
    "nsfw_indicators": [  # æ–°å¢NSFWæ£€æµ‹
        "nude", "naked", "pussy", "sex", "cum", "nipples", "breast", "penis",
        "erection", "oral", "anal", "masturbation", "orgasm", "aroused", "horny",
        "è£¸ä½“", "æ€§", "é˜´", "é˜³å…·", "èƒ¸éƒ¨", "ä¹³å¤´", "ç§å¤„", "éœ²å‡º"
    ]
}

# ç§»é™¤ç¼“å­˜ç³»ç»Ÿ - ç®€åŒ–ç‰ˆæœ¬

# æ™ºèƒ½åŒ–å¢å¼ºç³»ç»Ÿ
QUERY_HISTORY = []  # æŸ¥è¯¢å†å²è®°å½•
QUERY_STATS = {}    # æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
USER_PREFERENCES = {}  # ç”¨æˆ·åå¥½è®¾ç½®
PERFORMANCE_METRICS = {  # æ€§èƒ½æŒ‡æ ‡
    "total_queries": 0,
    "cache_hits": 0,
    "cache_misses": 0,
    "avg_response_time": 0.0,
    "last_query_time": None,
    "error_count": 0,
    "success_count": 0,
    "success_rate": 0.0  # ğŸ¯ ä¼˜é›…ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„æˆåŠŸç‡é”®
}

# ğŸ¯ ä¼˜é›…çš„æ ‡ç­¾åˆ«åæ˜ å°„ç³»ç»Ÿ - è§£å†³å¸¸ç”¨æ ‡ç­¾ç¼ºå¤±é—®é¢˜
TAG_ALIASES = {
    # === NSFWç›¸å…³æ ‡ç­¾æ˜ å°„ ===
    "mature_female": ["mature woman", "adult woman", "mature lady", "æˆç†Ÿå¥³æ€§", "ç†Ÿå¥³", "older_woman", "milf"],
    "mature_male": ["mature man", "adult man", "mature gentleman", "æˆç†Ÿç”·æ€§", "older_man", "daddy"],
    "young_adult": ["teen", "teenager", "young woman", "young man", "é’å¹´", "young_female", "young_male"],
    
    # === èº«ä½“éƒ¨ä½æ ‡ç­¾ ===
    "large_breasts": ["big breasts", "huge breasts", "å·¨ä¹³", "å¤§èƒ¸", "huge_boobs", "big_boobs", "voluptuous"],
    "small_breasts": ["flat chest", "tiny breasts", "è´«ä¹³", "å°èƒ¸", "petite_breasts", "small_boobs"],
    "thick_thighs": ["thicc thighs", "plump thighs", "ç²—è…¿", "wide_thighs", "meaty_thighs"],
    "wide_hips": ["broad hips", "curvy hips", "å®½è‡€", "thick_hips", "curvaceous"],
    "looking_at_viewer": ["eye contact", "direct gaze", "staring", "æ³¨è§†è§‚è€…", "looking_forward", "direct_eye_contact"],
    
    # === åœºæ™¯å’Œç¯å¢ƒ ===
    "basement": ["underground", "cellar", "åœ°ä¸‹å®¤", "åœ°ä¸‹", "dungeon", "underground_room"],
    "office": ["workplace", "business", "åŠå…¬å®¤", "èŒåœº", "corporate", "work_environment"],
    "bedroom": ["bed room", "sleeping room", "å§å®¤", "ç¡æˆ¿", "master_bedroom", "private_room"],
    "bathroom": ["bath room", "shower room", "æµ´å®¤", "ç›¥æ´—å®¤", "washroom", "restroom"],
    "classroom": ["school room", "æ•™å®¤", "å­¦æ ¡", "academy", "educational_setting"],
    "kitchen": ["cooking area", "å¨æˆ¿", "dining", "culinary_space"],
    
    # === å§¿åŠ¿å’ŒåŠ¨ä½œ ===
    "sitting_in_shadow": ["in shadow", "dark corner", "é˜´å½±ä¸­", "æš—å¤„", "shadowy", "dim_lighting"],
    "lying_down": ["laying down", "horizontal", "èººç€", "å§å§¿", "reclining", "lying_on_bed"],
    "standing": ["upright", "vertical", "ç«™ç«‹", "ç›´ç«‹", "standing_pose", "erect"],
    "sitting": ["seated", "chair pose", "åç€", "åå§¿", "sitting_down", "on_chair"],
    
    # === è¡¨æƒ…å’Œæƒ…ç»ª ===
    "happy": ["smile", "joyful", "cheerful", "å¼€å¿ƒ", "å¿«ä¹", "smiling", "pleased", "delighted"],
    "sad": ["crying", "tears", "depressed", "ä¼¤å¿ƒ", "å“­æ³£", "melancholy", "sorrowful"],
    "angry": ["mad", "furious", "upset", "æ„¤æ€’", "ç”Ÿæ°”", "rage", "irritated"],
    "embarrassed": ["shy", "blushing", "ashamed", "ç¾è€»", "å®³ç¾", "bashful", "timid"],
    "surprised": ["shocked", "amazed", "astonished", "æƒŠè®¶", "åƒæƒŠ", "startled"],
    
    # === æœè£…ç›¸å…³ ===
    "school_uniform": ["uniform", "student outfit", "æ ¡æœ", "å­¦ç”Ÿè£…", "school_clothes", "academic_uniform"],
    "casual_clothes": ["casual wear", "everyday clothes", "ä¾¿æœ", "æ—¥å¸¸æœè£…", "regular_clothes"],
    "formal_wear": ["suit", "formal clothes", "æ­£è£…", "æ­£å¼æœè£…", "business_attire", "dress_suit"],
    "bikini": ["swimsuit", "bathing suit", "æ³³è£…", "ä¸¤ä»¶å¼", "two_piece", "beach_wear"],
    "underwear": ["lingerie", "panties", "bra", "å†…è¡£", "undergarments"],
    
    # === AIç»˜ç”»è´¨é‡æ ‡ç­¾ ===
    "masterpiece": ["high quality", "best quality", "finest", "æ°ä½œ", "é«˜è´¨é‡", "premium_quality"],
    "ultra_detailed": ["extremely detailed", "highly detailed", "è¶…è¯¦ç»†", "æè‡´ç»†èŠ‚", "intricate_details"],
    "realistic": ["photorealistic", "lifelike", "çœŸå®", "å†™å®", "photo_realistic", "life_like"],
    "anime": ["manga style", "japanese animation", "åŠ¨æ¼«", "æ—¥å¼åŠ¨ç”»", "anime_style", "manga"],
    
    # === æ—¶é—´å’Œå…‰çº¿ ===
    "sunset": ["dusk", "evening", "golden hour", "æ—¥è½", "é»„æ˜", "twilight"],
    "sunrise": ["dawn", "morning", "early light", "æ—¥å‡º", "æ¸…æ™¨", "daybreak"],
    "night": ["nighttime", "evening", "dark", "å¤œæ™š", "å¤œé—´", "nocturnal"],
    "day": ["daytime", "daylight", "bright", "ç™½å¤©", "æ—¥é—´", "sunny"],
    
    # === åŠ¨ä½œå’Œäº’åŠ¨ ===
    "kiss": ["kissing", "lip contact", "æ¥å»", "äº²å»", "romantic_kiss", "passionate_kiss"],
    "hug": ["hugging", "embrace", "æ‹¥æŠ±", "æŠ±ç€", "cuddle", "holding"],
    "dance": ["dancing", "è·³èˆ", "èˆè¹ˆ", "ballroom", "performance"],
    
    # === ç‰¹æ®Šæ¦‚å¿µæ ‡ç­¾ ===
    "1girl": ["one girl", "single girl", "solo girl", "ä¸€ä¸ªå¥³å­©", "female_solo"],
    "2girls": ["two girls", "åŒå¥³", "ä¸¤ä¸ªå¥³å­©", "girl_pair", "female_duo"],
    "1boy": ["one boy", "single boy", "solo boy", "ä¸€ä¸ªç”·å­©", "male_solo"],
    "cute": ["adorable", "lovely", "kawaii", "å¯çˆ±", "sweet", "charming"],
    "beautiful": ["gorgeous", "pretty", "stunning", "ç¾ä¸½", "æ¼‚äº®", "attractive"]
}

# æ™ºèƒ½æŸ¥è¯¢æ„å›¾è¯†åˆ«æ¨¡å¼
INTENT_PATTERNS = {
    "artist": [
        "ç”»å¸ˆ", "artist", "ä½œè€…", "creator", "ç”»å®¶", "æ’ç”»å¸ˆ", "ç»˜å¸ˆ",
        "style", "é£æ ¼", "who drew", "who made", "è°ç”»çš„", "by_"
    ],
    "nsfw": [
        "nsfw", "æˆäºº", "è‰²æƒ…", "æ€§", "è£¸ä½“", "nude", "sex", "adult",
        "18+", "r18", "hentai", "å·¥å£", "é»„å›¾", "mature", "explicit",
        "breast", "pussy", "penis", "vagina", "anus", "nipple", "lewd",
        "underwear", "bra", "panties", "lingerie", "swimsuit", "bikini"
    ],
    "character": [
        "è§’è‰²", "character", "äººç‰©", "girl", "boy", "å¥³å­©", "ç”·å­©",
        "waifu", "è€å©†", "èŒå¦¹", "ç¾å°‘å¥³", "1girl", "1boy", "solo"
    ],
    "copyright": [
        "series", "from", "anime", "manga", "game", "ä½œå“", "ç³»åˆ—", 
        "å‹•ç•«", "æ¼«ç”»", "éŠæˆ²", "genshin", "pokemon", "naruto", "copyright"
    ],
    "appearance": [
        "hair", "eye", "face", "body", "clothing", "dress", "outfit",
        "uniform", "hair_color", "eye_color", "skin", "height", "age",
        "female", "male", "woman", "man", "è€å¹´", "young", "mature_female",
        "mature_male", "loli", "shota", "milf", "older", "younger"
    ],
    "pose": [
        "pose", "standing", "sitting", "lying", "kneeling", "dancing",
        "running", "walking", "jumping", "flying", "å§¿åŠ¿", "åŠ¨ä½œ", "position"
    ],
    "expression": [
        "smile", "crying", "angry", "sad", "happy", "surprised", "blush",
        "expression", "emotion", "face", "eyes", "mouth", "è¡¨æƒ…", "å¾®ç¬‘"
    ]
}

def detect_device() -> str:
    """æ£€æµ‹è®¾å¤‡"""
    if torch.cuda.is_available():
        logger.info(f"[GPU] ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        return 'cuda'
    else:
        logger.info("[CPU] ä½¿ç”¨CPU")
        return 'cpu'

# ç¼“å­˜ç›¸å…³å‡½æ•°å·²ç§»é™¤

def _detect_nsfw_level(text: str) -> str:
    """æ£€æµ‹NSFWçº§åˆ«"""
    text_lower = text.lower()
    return "é«˜" if any(indicator in text_lower for indicator in CONFIG["nsfw_indicators"]) else "ä½"

def _parse_tag_result(result: str, default_tag: str) -> tuple:
    """è§£ææ ‡ç­¾æœç´¢ç»“æœ"""
    if "ã€‘" in result and " - " in result:
        parts = result.split(" - ", 1)
        if len(parts) >= 2:
            translation = parts[0].split("ã€‘")[-1].strip()
            explanation = parts[1].strip()
            return translation, explanation
    return default_tag, result

def _detect_query_intent(query: str) -> str:
    """æ™ºèƒ½æ£€æµ‹æŸ¥è¯¢æ„å›¾"""
    query_lower = query.lower()
    
    # è®¡ç®—æ¯ç§æ„å›¾çš„åŒ¹é…åˆ†æ•°
    intent_scores = {}
    for intent, patterns in INTENT_PATTERNS.items():
        score = sum(1 for pattern in patterns if pattern in query_lower)
        if score > 0:
            intent_scores[intent] = score
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„æ„å›¾ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›é€šç”¨æœç´¢
    if intent_scores:
        return max(intent_scores.items(), key=lambda x: x[1])[0]
    return "general_search"

def _enhance_query(query: str, intent: str) -> str:
    """æ ¹æ®æ„å›¾å¢å¼ºæŸ¥è¯¢ - ä¼˜åŒ–è¯­ä¹‰å¢å¼ºç­–ç•¥"""
    enhanced_query = query
    
    # æ ¹æ®ä¸åŒæ„å›¾æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
    if intent == "artist":
        enhanced_query = f"ç”»å¸ˆ è‰ºæœ¯å®¶ artist {query}"
    elif intent == "nsfw":
        enhanced_query = f"NSFW æˆäººå†…å®¹ adult {query}"
    elif intent == "character":
        enhanced_query = f"è§’è‰² äººç‰© character {query}"
    elif intent == "copyright":
        enhanced_query = f"ä½œå“ ç³»åˆ— series {query}"
    elif intent == "appearance":
        # å¤–è§‚ç‰¹å¾æŸ¥è¯¢ï¼Œå¢å¼ºæ ‡ç­¾åŒ¹é…
        enhanced_query = f"å¤–è§‚ ç‰¹å¾ appearance {query}"
    elif intent == "pose":
        enhanced_query = f"å§¿åŠ¿ åŠ¨ä½œ pose {query}"
    elif intent == "expression":
        enhanced_query = f"è¡¨æƒ… emotion {query}"
    else:
        # é€šç”¨æŸ¥è¯¢ï¼šä¸åšè¿‡å¤šä¿®æ”¹ï¼Œä¿æŒåŸå§‹æŸ¥è¯¢çš„è¯­ä¹‰
        enhanced_query = query
    
    return enhanced_query

def _apply_tag_aliases(query: str) -> str:
    """
    åº”ç”¨TAG_ALIASESæ˜ å°„ï¼Œå°†å¸¸ç”¨åˆ«åè‡ªåŠ¨æ›¿æ¢ä¸ºæ ‡å‡†æ ‡ç­¾
    ä¿®å¤ mature_female -> mame ç­‰é”™è¯¯æ˜ å°„é—®é¢˜
    """
    processed_query = query.lower().strip()
    original_query = processed_query
    
    # å®šä¹‰å…³é”®æ ‡ç­¾åˆ«åæ˜ å°„ï¼ˆåŸºäºDanbooruæ ‡å‡†ï¼‰
    tag_aliases = {
        # å¹´é¾„ç›¸å…³
        "mature_female": "older_woman",
        "mature_male": "older_man", 
        "mature_woman": "older_woman",
        "mature_man": "older_man",
        "milf": "older_woman",
        "older_female": "older_woman",
        "older_male": "older_man",
        
        # èƒ¸éƒ¨å¤§å°
        "large_breasts": "huge_boobs",
        "big_breasts": "huge_boobs",
        "huge_breasts": "huge_boobs",
        "small_breasts": "small_boobs",
        "flat_chest": "small_boobs",
        
        # è§†çº¿æ–¹å‘
        "looking_at_viewer": "staring",
        "eye_contact": "staring",
        "looking_at_camera": "staring",
        "direct_gaze": "staring",
        
        # è¡¨æƒ…ç›¸å…³
        "smiling": "smile",
        "happy": "smile",
        "grinning": "smile",
        "laughing": "smile",
        
        # å§¿åŠ¿ç›¸å…³
        "standing_pose": "standing",
        "sitting_pose": "sitting",
        "lying_down": "lying",
        "lying_pose": "lying",
        
        # æœè£…ç›¸å…³
        "school_uniform": "uniform",
        "maid_outfit": "maid",
        "swimwear": "swimsuit",
        "bathing_suit": "swimsuit",
        
        # å¤´å‘ç›¸å…³
        "blonde_hair": "blonde",
        "brown_hair": "brunette", 
        "black_hair": "dark_hair",
        "white_hair": "silver_hair",
        
        # é€šç”¨åˆ«å
        "girl": "1girl",
        "boy": "1boy",
        "woman": "1girl",
        "man": "1boy",
        "female": "1girl",
        "male": "1boy"
    }
    
    # ç²¾ç¡®åŒ¹é…æ›¿æ¢ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…é”™è¯¯ï¼‰
    for alias, standard_tag in tag_aliases.items():
        # ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯æ›¿æ¢
        import re
        pattern = r'\b' + re.escape(alias) + r'\b'
        processed_query = re.sub(pattern, standard_tag, processed_query)
    
    # å¦‚æœæŸ¥è¯¢è¢«ä¿®æ”¹äº†ï¼Œè®°å½•æ›¿æ¢ä¿¡æ¯
    if processed_query != original_query:
        logger.info(f"[TAG_ALIAS] '{original_query}' -> '{processed_query}'")
    
    return processed_query

def _record_query_stats(query: str, intent: str, response_time: float, success: bool):
    """è®°å½•æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯"""
    global QUERY_HISTORY, QUERY_STATS, PERFORMANCE_METRICS
    
    # è®°å½•æŸ¥è¯¢å†å²
    QUERY_HISTORY.append({
        "query": query,
        "intent": intent,
        "timestamp": time.time(),
        "response_time": response_time,
        "success": success
    })
    
    # é™åˆ¶å†å²è®°å½•é•¿åº¦
    if len(QUERY_HISTORY) > 1000:
        QUERY_HISTORY = QUERY_HISTORY[-500:]
    
    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    if query not in QUERY_STATS:
        QUERY_STATS[query] = {"count": 0, "success_count": 0, "avg_time": 0.0}
    
    stats = QUERY_STATS[query]
    stats["count"] += 1
    if success:
        stats["success_count"] += 1
    stats["avg_time"] = (stats["avg_time"] * (stats["count"] - 1) + response_time) / stats["count"]
    
    # æ›´æ–°å…¨å±€æ€§èƒ½æŒ‡æ ‡
    PERFORMANCE_METRICS["total_queries"] += 1
    if success:
        old_avg = PERFORMANCE_METRICS["avg_response_time"]
        total = PERFORMANCE_METRICS["total_queries"]
        PERFORMANCE_METRICS["avg_response_time"] = (old_avg * (total - 1) + response_time) / total
    
    success_count = sum(1 for h in QUERY_HISTORY if h["success"])
    PERFORMANCE_METRICS["success_rate"] = success_count / len(QUERY_HISTORY) if QUERY_HISTORY else 0.0

# ç¼“å­˜é”®å’Œé¢„æµ‹ç¼“å­˜å‡½æ•°å·²ç§»é™¤

class MinimalDanbooruServer:
    """æœ€å°ç‰ˆæœ¬çš„Danbooruæœç´¢æœåŠ¡å™¨ - å¢å¼ºç‰ˆï¼Œæ”¯æŒChromaDBå’Œé«˜çº§åŠŸèƒ½"""
    
    def __init__(self, device: str = None, use_fp16: bool = True):
        self.device = device or detect_device()
        self.use_fp16 = use_fp16
        self.model = None
        self.documents: List[str] = []
        self.embeddings: Optional[np.ndarray] = None
        self.sparse_embeddings = None  # Sparseå‘é‡
        self.colbert_embeddings = None  # ColBERTå‘é‡
        self.is_loaded = False
        
        # ChromaDBç›¸å…³å±æ€§
        self.chroma_client = None
        self.collection = None
        self.database_mode = False
        self.database_path = None
        self.collection_name = None
        
        logger.info(f"[INIT] è®¾å¤‡: {self.device}, FP16: {use_fp16}")
    
    def load_model(self):
        """åŠ è½½BGE-M3æ¨¡å‹ - å¢å¼ºé…ç½®"""
        try:
            logger.info("[MODEL] åŠ è½½BGE-M3æ¨¡å‹...")
            self.model = BGEM3FlagModel(
                'BAAI/bge-m3',
                use_fp16=self.use_fp16,
                device=self.device
            )
            logger.info("[OK] æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            if self.device == 'cuda':
                logger.warning(f"[WARNING] GPUåŠ è½½å¤±è´¥ï¼Œé™çº§åˆ°CPU: {e}")
                self.device = 'cpu'
                self.use_fp16 = False
                self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False, device='cpu')
            else:
                logger.error(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise
    
    def encode_query(self, query: str, return_all_embeddings: bool = False):
        """ç›´æ¥ç¼–ç å‡½æ•° - BGE-M3ä¸‰é‡èƒ½åŠ›ç‰ˆæœ¬ï¼ˆæ— ç¼“å­˜ï¼‰"""
        # ç›´æ¥ç¼–ç ï¼Œæ— ç¼“å­˜

        result = self.model.encode(
                [query],
                batch_size=1,
                max_length=CONFIG["max_length"],
                return_dense=True,
                return_sparse=return_all_embeddings,  # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è¿”å›
                return_colbert_vecs=return_all_embeddings # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è¿”å›
            )
            
        if return_all_embeddings:
                # è¿”å›æ‰€æœ‰ä¸‰ç§å‘é‡
                embeddings = {
                    'dense': result['dense_vecs'][0],
                    'sparse': result.get('lexical_weights', [None])[0],
                    'colbert': result.get('colbert_vecs', [None])[0]
                }
        else:
            # é»˜è®¤è¿”å›denseå‘é‡ä¿æŒå…¼å®¹æ€§
            embeddings = result['dense_vecs'][0]
            
            # è½¬æ¢ä¸ºæ¨¡å‹ç²¾åº¦ä¸€è‡´çš„æ•°æ®ç±»å‹
            dtype = np.float16 if self.use_fp16 else np.float32
            if isinstance(embeddings, dict):
                if embeddings.get('dense') is not None:
                    embeddings['dense'] = np.asarray(embeddings['dense'], dtype=dtype)
            else:
                embeddings = np.asarray(embeddings, dtype=dtype)
                
            return embeddings
    
    def connect_to_database(self, database_path: str, collection_name: str):
        """è¿æ¥åˆ°ChromaDBæ•°æ®åº“"""
        try:
            logger.info(f"[DB] è¿æ¥åˆ°æ•°æ®åº“: {database_path}")
            logger.info(f"[DB] é›†åˆåç§°: {collection_name}")
            
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=database_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=False
                )
            )
            
            # è·å–é›†åˆ
            self.collection = self.chroma_client.get_collection(collection_name)
            
            # è·å–é›†åˆä¿¡æ¯
            collection_count = self.collection.count()
            logger.info(f"[OK] æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œæ–‡æ¡£æ•°é‡: {collection_count}")
            
            self.database_mode = True
            self.database_path = database_path
            self.collection_name = collection_name
            self.is_loaded = True
            
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def list_available_collections(self, database_path: str):
        """åˆ—å‡ºæ•°æ®åº“ä¸­å¯ç”¨çš„é›†åˆ"""
        try:
            client = chromadb.PersistentClient(
                path=database_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=False
                )
            )
            collections = client.list_collections()
            return [col.name for col in collections]
        except Exception as e:
            logger.error(f"[ERROR] æ— æ³•åˆ—å‡ºé›†åˆ: {e}")
            return []
    
    def load_test_data(self, data_path: str = None, database_path: str = None, collection_name: str = None):
        """åŠ è½½æ•°æ® - ä¼˜å…ˆä½¿ç”¨å®é™…æ•°æ®åº“"""
        
        logger.info(f"[LOAD] å‚æ•°æ£€æŸ¥ - database_path: {database_path}, collection_name: {collection_name}")
        
        # é¦–å…ˆå°è¯•é»˜è®¤çš„å®é™…æ•°æ®åº“è·¯å¾„
        default_db_path = r"D:\tscrag\artifacts\vector_stores\chroma_db"
        default_collection = "ultimate_danbooru_dataset_bge-m3"
        
        # ä¼˜å…ˆçº§1: ç”¨æˆ·æŒ‡å®šçš„æ•°æ®åº“è·¯å¾„
        if database_path and collection_name:
            try:
                logger.info(f"[DB] å°è¯•è¿æ¥ç”¨æˆ·æŒ‡å®šæ•°æ®åº“: {database_path}")
                self.connect_to_database(database_path, collection_name)
                logger.info(f"[OK] ç”¨æˆ·æŒ‡å®šæ•°æ®åº“è¿æ¥æˆåŠŸ")
                return
            except Exception as e:
                logger.error(f"[ERROR] ç”¨æˆ·æŒ‡å®šæ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: é»˜è®¤å®é™…æ•°æ®åº“è·¯å¾„
        if Path(default_db_path).exists():
            logger.info(f"[DB] å‘ç°é»˜è®¤æ•°æ®åº“è·¯å¾„: {default_db_path}")
            
            # å…ˆåˆ—å‡ºå¯ç”¨çš„é›†åˆ
            available_collections = self.list_available_collections(default_db_path)
            logger.info(f"[DB] å¯ç”¨é›†åˆ: {available_collections}")
            
            if available_collections:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„é›†åˆ
                collection_to_use = available_collections[0]
                try:
                    logger.info(f"[DB] å°è¯•è¿æ¥é›†åˆ: {collection_to_use}")
                    self.connect_to_database(default_db_path, collection_to_use)
                    logger.info(f"[OK] é»˜è®¤æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œé›†åˆ: {collection_to_use}")
                    return
                except Exception as e:
                    logger.error(f"[ERROR] è¿æ¥é›†åˆ {collection_to_use} å¤±è´¥: {e}")
            else:
                logger.warning(f"[WARNING] æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é›†åˆ")
            
            logger.warning(f"[WARNING] å°†å°è¯•å…¶ä»–æ•°æ®æº")
        else:
            logger.warning(f"[WARNING] é»˜è®¤æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {default_db_path}")
        
        # ä¼˜å…ˆçº§3: æ–‡ä»¶æ¨¡å¼
        if data_path and Path(data_path).exists():
            logger.info(f"[FILE] ä½¿ç”¨æ–‡ä»¶æ¨¡å¼: {data_path}")
            self._load_from_file(data_path)
        else:
            # æœ€åé€‰æ‹©: ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
            logger.warning(f"[TEST] æœªæ‰¾åˆ°å®é™…æ•°æ®åº“ï¼Œä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®ï¼ˆåŠŸèƒ½å—é™ï¼‰")
            self._load_test_prompts()
        
        self.is_loaded = True
        logger.info(f"[OK] æ•°æ®åŠ è½½å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(self.documents) if not self.database_mode else 'æ•°æ®åº“æ¨¡å¼'}")
    
    def _load_test_prompts(self):
        """åŠ è½½å†…ç½®æµ‹è¯•æç¤ºè¯"""
        test_prompts = [
            "1girl, anime, beautiful, long hair, blue eyes",
            "2girls, cute, school uniform, smile",
            "masterpiece, high quality, detailed, portrait",
            "landscape, nature, mountains, sky, clouds",
            "cat, animal, cute, fluffy, sitting",
            "food, delicious, restaurant, meal",
            "car, vehicle, red, sports car, fast",
            "house, building, architecture, modern",
            "flower, garden, colorful, spring, blooming",
            "ocean, beach, sunset, waves, peaceful",
            "1boy, male, handsome, casual clothes",
            "fantasy, magic, dragon, medieval, epic",
            "cyberpunk, futuristic, neon, city, technology",
            "wedding, dress, bride, ceremony, romantic",
            "winter, snow, cold, white, beautiful"
        ]
        
        self.documents = test_prompts
        logger.info("[TEST] ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®")
        
        # ç®€å•ç¼–ç 
        self._encode_documents()
    
    def _load_from_file(self, file_path: str):
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
        documents = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_no, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            # å°è¯•è§£æJSON
                            data = json.loads(line.strip())
                            if 'document' in data:
                                documents.append(data['document'])
                            elif 'text' in data:
                                documents.append(data['text'])
                            else:
                                documents.append(line.strip())
                        except json.JSONDecodeError:
                            # ç›´æ¥ä½¿ç”¨æ–‡æœ¬
                            documents.append(line.strip())
                    
                    # é™åˆ¶æµ‹è¯•æ•°æ®é‡ï¼ŒåŠ å¿«å¯åŠ¨é€Ÿåº¦
                    if len(documents) >= 1000:
                        logger.info(f"[LIMIT] é™åˆ¶åŠ è½½å‰1000æ¡æ•°æ®ä»¥æå‡é€Ÿåº¦")
                        break
                        
        except Exception as e:
            logger.error(f"[ERROR] æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
        
        self.documents = documents
        logger.info(f"[FILE] ä»æ–‡ä»¶åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        # ç¼–ç æ–‡æ¡£
        self._encode_documents()
    
    def _encode_documents(self):
        """ç¼–ç æ‰€æœ‰æ–‡æ¡£ - BGE-M3ä¸‰é‡èƒ½åŠ›ç‰ˆæœ¬"""
        if not self.model:
            raise ValueError("æ¨¡å‹æœªåŠ è½½")
        
        logger.info("[ENCODE] å¼€å§‹ç¼–ç æ–‡æ¡£ï¼ˆBGE-M3ä¸‰é‡èƒ½åŠ›ï¼‰...")
        start_time = time.time()
        
        # æ‰¹é‡ç¼–ç 
        batch_size = CONFIG["batch_size"]
        all_dense_embeddings = []
        all_sparse_embeddings = []
        all_colbert_embeddings = []
        
        for i in range(0, len(self.documents), batch_size):
            batch = self.documents[i:i + batch_size]
            
            # ä½¿ç”¨BGE-M3ä¸‰é‡èƒ½åŠ›ç¼–ç 
            batch_output = self.model.encode(
                batch,
                batch_size=len(batch),
                max_length=CONFIG["max_length"],
                return_dense=True,        # âœ… Denseå‘é‡
                return_sparse=True,       # âœ… Sparseå‘é‡
                return_colbert_vecs=True  # âœ… ColBERTå‘é‡
            )
            
            # æ”¶é›†ä¸‰ç§ç±»å‹çš„å‘é‡
            all_dense_embeddings.append(batch_output['dense_vecs'])
            if 'lexical_weights' in batch_output:
                all_sparse_embeddings.append(batch_output['lexical_weights'])
            if 'colbert_vecs' in batch_output:
                all_colbert_embeddings.append(batch_output['colbert_vecs'])
            
            if (i // batch_size + 1) % 5 == 0:
                logger.info(f"[PROGRESS] ç¼–ç è¿›åº¦: {i + len(batch)}/{len(self.documents)}")
        
        # åˆå¹¶åµŒå…¥å‘é‡
        self.embeddings = np.vstack(all_dense_embeddings)  # ä¸»è¦ç”¨denseå‘é‡
        
        # å­˜å‚¨ä¸‰é‡å‘é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if all_sparse_embeddings:
            # å°†æ‰¹æ¬¡åˆ—è¡¨å±•å¹³ä¸ºå•ä¸ªæ–‡æ¡£åˆ—è¡¨
            self.sparse_embeddings = []
            for batch in all_sparse_embeddings:
                self.sparse_embeddings.extend(batch)
            logger.info("[SPARSE] Sparseå‘é‡å·²ç¼–ç ")
        else:
            self.sparse_embeddings = None
            
        if all_colbert_embeddings:
            # å°†æ‰¹æ¬¡åˆ—è¡¨å±•å¹³ä¸ºå•ä¸ªæ–‡æ¡£åˆ—è¡¨
            self.colbert_embeddings = []
            for batch in all_colbert_embeddings:
                self.colbert_embeddings.extend(batch)
            logger.info("[COLBERT] ColBERTå‘é‡å·²ç¼–ç ")
        else:
            self.colbert_embeddings = None
        
        encode_time = time.time() - start_time
        logger.info(f"[OK] ä¸‰é‡å‘é‡ç¼–ç å®Œæˆï¼Œè€—æ—¶: {encode_time:.2f}ç§’")
        logger.info(f"[DENSE] Denseå‘é‡å½¢çŠ¶: {self.embeddings.shape}")
        logger.info(f"[SPARSE] Sparseå‘é‡: {'âœ…' if self.sparse_embeddings else 'âŒ'}")
        logger.info(f"[COLBERT] ColBERTå‘é‡: {'âœ…' if self.colbert_embeddings else 'âŒ'}")
    
    def search(self, query: str, n_results: int = 20) -> Dict[str, Any]:
        """æœç´¢åŠŸèƒ½ - æ”¯æŒæ•°æ®åº“æ¨¡å¼å’Œå†…å­˜æ¨¡å¼"""
        if not self.is_loaded or not self.model:
            return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–"}
        
        try:
            # æ•°æ®åº“æ¨¡å¼
            if self.database_mode and self.collection:
                return self._search_database(query, n_results)
            else:
                return self._search_memory(query, n_results)
                
        except Exception as e:
            logger.error(f"[ERROR] æœç´¢å¤±è´¥: {e}")
            return {"error": f"æœç´¢å¤±è´¥: {str(e)}"}
    
    def _search_database(self, query: str, n_results: int) -> Dict[str, Any]:
        """ä½¿ç”¨ChromaDBæœç´¢"""
        # ç›´æ¥ç¼–ç 
        query_embedding_raw = self.encode_query(query)
        
        # ä¿®å¤ï¼šç¡®ä¿æˆ‘ä»¬åªä½¿ç”¨denseå‘é‡è¿›è¡Œæ•°æ®åº“æŸ¥è¯¢
        if isinstance(query_embedding_raw, dict):
            query_embedding = query_embedding_raw.get('dense')
        else:
            query_embedding = query_embedding_raw

        if query_embedding is None:
            logger.error(f"[ERROR] æ— æ³•ä¸ºæŸ¥è¯¢ '{query}' è·å–æœ‰æ•ˆçš„dense embeddingã€‚")
            return {"error": "æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡"}
        
        # ChromaDBæŸ¥è¯¢
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=n_results,
            include=['documents', 'distances', 'metadatas']
        )
        
        # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (1 - distance)ï¼Œç¡®ä¿æ‰€æœ‰åˆ†æ•°éƒ½æ˜¯æœ‰æ•ˆçš„
        distances = results['distances'][0]
        similarities = []
        for dist in distances:
            if dist is None or not isinstance(dist, (int, float)):
                similarities.append(0.0)
                logger.warning(f"[DB] æ— æ•ˆè·ç¦»å€¼ {dist}ï¼Œè®¾ä¸ºç›¸ä¼¼åº¦ 0.0")
            else:
                similarities.append(max(0.0, 1.0 - float(dist)))  # ç¡®ä¿ç›¸ä¼¼åº¦ >= 0
        
        return {
            "query": query,
            "results": results['documents'][0],
            "scores": similarities,
            "count": len(results['documents'][0]),
            "mode": "database"
        }
    
    def _search_memory(self, query: str, n_results: int) -> Dict[str, Any]:
        """ä½¿ç”¨å†…å­˜å‘é‡æœç´¢"""
        # ç›´æ¥ç¼–ç 
        query_embedding = self.encode_query(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.embeddings, query_embedding)
        
        # è·å–topç»“æœ
        top_indices = np.argsort(similarities)[::-1][:n_results]
        
        results = []
        scores = []
        for idx in top_indices:
            results.append(self.documents[idx])
            scores.append(float(similarities[idx]))
        
        return {
            "query": query,
            "results": results,
            "scores": scores,
            "count": len(results),
            "mode": "memory"
        }

    def count(self) -> int:
        """è¿”å›æ–‡æ¡£æ•°é‡"""
        if self.database_mode and self.collection:
            return self.collection.count()
        return len(self.documents)
    
    def _compute_sparse_similarity(self, query_embedding: dict, query: str) -> np.ndarray:
        """
        è®¡ç®—Sparseå‘é‡ç›¸ä¼¼åº¦ - ä½¿ç”¨BGE-M3å®˜æ–¹lexical_weightsæ–¹æ³•
        """
        # æ•°æ®åº“æ¨¡å¼ä¸‹ç›´æ¥ä½¿ç”¨å›é€€åŒ¹é…
        if self.database_mode or not self.sparse_embeddings or 'sparse' not in query_embedding:
            return self._fallback_sparse_matching(query)
        
        query_sparse = query_embedding['sparse']
        if query_sparse is None:
            return self._fallback_sparse_matching(query)
        
        # è·å–æ­£ç¡®çš„æ–‡æ¡£æ•°é‡
        doc_count = len(self.documents) if self.documents else 0
        if doc_count == 0:
            return np.array([])
            
        scores = np.zeros(doc_count)
        
        # ä½¿ç”¨BGE-M3å®˜æ–¹ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
        for i in range(doc_count):
            if i >= len(self.sparse_embeddings):
                scores[i] = 0.0
                continue
                
            doc_sparse = self.sparse_embeddings[i]
            if doc_sparse is None:
                scores[i] = 0.0
                continue
                
            try:
                # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹çš„å®˜æ–¹æ–¹æ³•
                if hasattr(self.model, 'compute_lexical_matching_score'):
                    score = self.model.compute_lexical_matching_score(query_sparse, doc_sparse)
                    scores[i] = float(score) if score is not None else 0.0
                else:
                    # æ‰‹åŠ¨å®ç°lexical weightsåŒ¹é…
                    scores[i] = self._manual_lexical_matching(query_sparse, doc_sparse)
                    
            except Exception as e:
                logger.warning(f"Sparse similarity error for doc {i}: {e}")
                scores[i] = 0.0
        
        return scores
    
    def _manual_lexical_matching(self, query_weights: dict, doc_weights: dict) -> float:
        """æ‰‹åŠ¨å®ç°lexical weightsåŒ¹é…è®¡ç®—"""
        if not isinstance(query_weights, dict) or not isinstance(doc_weights, dict):
            return 0.0
            
        # è®¡ç®—å…±åŒtokençš„æƒé‡ä¹˜ç§¯
        score = 0.0
        query_tokens = set(query_weights.keys())
        doc_tokens = set(doc_weights.keys())
        
        common_tokens = query_tokens & doc_tokens
        for token in common_tokens:
            score += query_weights[token] * doc_weights[token]
        
        # å½’ä¸€åŒ–
        query_norm = sum(w*w for w in query_weights.values()) ** 0.5
        doc_norm = sum(w*w for w in doc_weights.values()) ** 0.5
        
        if query_norm > 0 and doc_norm > 0:
            score = score / (query_norm * doc_norm)
        
        return min(1.0, max(0.0, score))
    
    def _compute_colbert_similarity(self, query_embedding: dict, query: str) -> np.ndarray:
        """
        è®¡ç®—ColBERTå‘é‡ç›¸ä¼¼åº¦ - ä½¿ç”¨BGE-M3å®˜æ–¹colbert_scoreæ–¹æ³•
        """
        # æ•°æ®åº“æ¨¡å¼ä¸‹ç›´æ¥ä½¿ç”¨å›é€€åŒ¹é…
        if self.database_mode or not self.colbert_embeddings or 'colbert' not in query_embedding:
            return self._fallback_colbert_matching(query)
        
        query_colbert = query_embedding['colbert']
        if query_colbert is None:
            return self._fallback_colbert_matching(query)
        
        # è·å–æ­£ç¡®çš„æ–‡æ¡£æ•°é‡
        doc_count = len(self.documents) if self.documents else 0
        if doc_count == 0:
            return np.array([])
            
        scores = np.zeros(doc_count)
        
        # ä½¿ç”¨BGE-M3å®˜æ–¹ColBERTç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
        for i in range(doc_count):
            if i >= len(self.colbert_embeddings):
                scores[i] = 0.0
                continue
                
            doc_colbert = self.colbert_embeddings[i]
            if doc_colbert is None:
                scores[i] = 0.0
                continue
                
            try:
                # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹çš„å®˜æ–¹æ–¹æ³•
                if hasattr(self.model, 'colbert_score'):
                    score = self.model.colbert_score(query_colbert, doc_colbert)
                    scores[i] = float(score) if score is not None else 0.0
                else:
                    # æ‰‹åŠ¨å®ç°ColBERT MaxSimè®¡ç®—
                    scores[i] = self._manual_colbert_maxsim(query_colbert, doc_colbert)
                    
            except Exception as e:
                logger.warning(f"ColBERT similarity error for doc {i}: {e}")
                scores[i] = 0.0
        
        return scores
    
    def _manual_colbert_maxsim(self, query_vecs, doc_vecs) -> float:
        """æ‰‹åŠ¨å®ç°ColBERT MaxSimè®¡ç®—"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
            if not isinstance(query_vecs, np.ndarray):
                query_vecs = np.array(query_vecs)
            if not isinstance(doc_vecs, np.ndarray):
                doc_vecs = np.array(doc_vecs)
            
            # ColBERTä½¿ç”¨MaxSimï¼šå¯¹æ¯ä¸ªæŸ¥è¯¢tokenæ‰¾åˆ°æ–‡æ¡£ä¸­æœ€ç›¸ä¼¼çš„token
            similarity_matrix = np.dot(query_vecs, doc_vecs.T)  # [query_tokens, doc_tokens]
            
            # å¯¹æ¯ä¸ªæŸ¥è¯¢tokenå–æœ€å¤§ç›¸ä¼¼åº¦ï¼Œç„¶åå¹³å‡
            max_sims = np.max(similarity_matrix, axis=1)  # æ¯ä¸ªæŸ¥è¯¢tokençš„æœ€å¤§ç›¸ä¼¼åº¦
            score = np.mean(max_sims)  # å¹³å‡æ‰€æœ‰æŸ¥è¯¢tokençš„æœ€å¤§ç›¸ä¼¼åº¦
            
            return min(1.0, max(0.0, float(score)))
            
        except Exception as e:
            print(f"Manual ColBERT calculation error: {e}")
            return 0.0
    
    def _fallback_sparse_matching(self, query: str) -> np.ndarray:
        """
        Sparseå‘é‡çš„å›é€€åŒ¹é…ç­–ç•¥ - åŸºäºå…³é”®è¯ç²¾ç¡®åŒ¹é…
        """
        # æ•°æ®åº“æ¨¡å¼ä¸‹è¿”å›ç©ºæ•°ç»„ï¼Œè®©æ•°æ®åº“æœç´¢å¤„ç†
        if self.database_mode:
            return np.array([])
            
        query_terms = set(query.lower().split())
        doc_count = len(self.documents) if self.documents else 0
        if doc_count == 0:
            return np.array([])
            
        scores = np.zeros(doc_count)
        
        for i, doc in enumerate(self.documents):
            doc_terms = set(doc.lower().split())
            
            # è®¡ç®—å…³é”®è¯é‡å åˆ†æ•°
            intersection = query_terms.intersection(doc_terms)
            union = query_terms.union(doc_terms)
            
            if union:
                # Jaccardç›¸ä¼¼åº¦ + ç²¾ç¡®åŒ¹é…å¥–åŠ±
                jaccard_score = len(intersection) / len(union)
                
                # ç²¾ç¡®åŒ¹é…å¥–åŠ±
                exact_matches = sum(1 for term in query_terms if term in doc.lower())
                exact_bonus = exact_matches / len(query_terms) if query_terms else 0
                
                # å…³é”®è¯æƒé‡ï¼ˆå¸¸è§AIç»˜ç”»æ ‡ç­¾åŠ æƒï¼‰
                important_terms = ["1girl", "2girls", "anime", "realistic", "nude", "nsfw", "artist"]
                weight_bonus = sum(0.2 for term in query_terms if term in important_terms)
                
                scores[i] = jaccard_score + (exact_bonus * 0.5) + weight_bonus
        
        return scores
    
    def _fallback_colbert_matching(self, query: str) -> np.ndarray:
        """åœ¨ColBERTåµŒå…¥ä¸å¯ç”¨æ—¶ï¼Œä½¿ç”¨æ‰‹åŠ¨è¯åŒ¹é…ä½œä¸ºåå¤‡æ–¹æ¡ˆ"""
        logger.warning("[FALLBACK] ColBERTåµŒå…¥ä¸å¯ç”¨ï¼Œä½¿ç”¨æ‰‹åŠ¨è¯åŒ¹é…ä½œä¸ºåå¤‡")
        
        query_tokens = set(query.lower().split())
        colbert_scores = np.zeros(len(self.documents))
        
        for i, doc in enumerate(self.documents):
            doc_tokens = set(doc.lower().split())
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦ä½œä¸ºåˆ†æ•°
            intersection = len(query_tokens.intersection(doc_tokens))
            union = len(query_tokens.union(doc_tokens))
            
            if union > 0:
                colbert_scores[i] = intersection / union
        
        return colbert_scores
    
    def _intelligent_score_fusion(self, results: dict, query: str, limit: int) -> list:
        """
        æ™ºèƒ½åˆ†æ•°èåˆ BGE-M3 V3 ä¼˜åŒ–ç‰ˆ
        - åŸºäºBGE-M3è®ºæ–‡çš„æœ€ä¼˜æƒé‡é…ç½® [0.4, 0.2, 0.4]
        - æŸ¥è¯¢æ„å›¾è‡ªé€‚åº”æƒé‡è°ƒæ•´
        - è´¨é‡è¿‡æ»¤å’Œç»“æœå¤šæ ·æ€§å¢å¼º
        """
        
        final_results = []
        
        # ç¡®ä¿æ‰€æœ‰ç›¸ä¼¼åº¦åˆ†æ•°éƒ½æ˜¯æœ‰æ•ˆçš„æµ®ç‚¹æ•°
        dense_sim = results.get('dense_similarities')
        sparse_sim = results.get('sparse_similarities')
        colbert_sim = results.get('colbert_similarities')

        # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šç¡®ä¿æ‰€æœ‰ç›¸ä¼¼åº¦æ•°ç»„éƒ½æ˜¯numpy arrayå¹¶å¤„ç†None
        dense_sim = np.nan_to_num(np.array(dense_sim, dtype=float)) if dense_sim is not None else np.zeros(len(results['ids'][0]))
        sparse_sim = np.nan_to_num(np.array(sparse_sim, dtype=float)) if sparse_sim is not None else np.zeros(len(results['ids'][0]))
        colbert_sim = np.nan_to_num(np.array(colbert_sim, dtype=float)) if colbert_sim is not None else np.zeros(len(results['ids'][0]))

        # åŸºäºBGE-M3æœ€ä½³å®è·µçš„æƒé‡é…ç½® + æŸ¥è¯¢æ„å›¾è‡ªé€‚åº”
        intent = _detect_query_intent(query)
        if intent == "artist":
            # è‰ºæœ¯å®¶æŸ¥è¯¢ï¼šå¢å¼ºç¨€ç–åŒ¹é…ï¼ˆç²¾ç¡®åç§°åŒ¹é…ï¼‰
            w_dense, w_sparse, w_colbert = 0.3, 0.4, 0.3
        elif intent in ["character", "copyright"]: 
            # è§’è‰²/ç‰ˆæƒæŸ¥è¯¢ï¼šå¹³è¡¡è¯­ä¹‰å’Œç²¾ç¡®åŒ¹é…
            w_dense, w_sparse, w_colbert = 0.4, 0.3, 0.3
        elif intent == "nsfw":
            # NSFWæŸ¥è¯¢ï¼šå¢å¼ºColBERTï¼ˆç»†ç²’åº¦è¯­ä¹‰åŒ¹é…ï¼‰
            w_dense, w_sparse, w_colbert = 0.3, 0.2, 0.5
        else:
            # é€šç”¨æŸ¥è¯¢ï¼šä½¿ç”¨BGE-M3è®ºæ–‡æ¨èçš„æœ€ä¼˜æƒé‡
            w_dense, w_sparse, w_colbert = 0.4, 0.2, 0.4
        
        # è®¡ç®—èåˆåˆ†æ•°
        combined_scores = (dense_sim * w_dense +
                           sparse_sim * w_sparse +
                           colbert_sim * w_colbert)
        
        # è´¨é‡è¿‡æ»¤ï¼šåŸºäºå†…å®¹ç›¸å…³æ€§çš„äºŒæ¬¡è¯„åˆ†
        for i in range(len(combined_scores)):
            if i < len(results['documents'][0]):
                doc_content = results['documents'][0][i].lower()
                query_lower = query.lower()
                
                # ç²¾ç¡®åŒ¹é…å¥–åŠ±
                if query_lower in doc_content:
                    combined_scores[i] += 0.1
                    
                # æ ‡ç­¾å¯†åº¦å¥–åŠ±ï¼ˆæ›´å¤šç›¸å…³æ ‡ç­¾ = æ›´é«˜è´¨é‡ï¼‰
                if len(doc_content.split(',')) > 10:  # ä¸°å¯Œçš„æ ‡ç­¾å†…å®¹
                    combined_scores[i] += 0.05
                
                # ç¡®ä¿åˆ†æ•°ä¸è¶…è¿‡1.0
                combined_scores[i] = min(1.0, combined_scores[i])
        
        # è·å–ç´¢å¼•å¹¶æ’åº
        sorted_indices = np.argsort(combined_scores)[::-1]

        seen_docs = set()
        for idx in sorted_indices:
            doc = results['documents'][0][idx]
            if doc not in seen_docs:
                final_results.append({
                    "id": results['ids'][0][idx],
                    "document": doc,
                    "score": float(combined_scores[idx]),
                    "metadata": results['metadatas'][0][idx],
                    "scores_breakdown": {
                        "dense": float(dense_sim[idx]) if idx < len(dense_sim) else 0.0,
                        "sparse": float(sparse_sim[idx]) if idx < len(sparse_sim) else 0.0,
                        "colbert": float(colbert_sim[idx]) if idx < len(colbert_sim) else 0.0,
                        "weights": f"D:{w_dense:.2f} S:{w_sparse:.2f} C:{w_colbert:.2f}",
                        "intent": intent
                    }
                })
                seen_docs.add(doc)
                if len(final_results) >= limit:
                    break
        
        return final_results

    def hybrid_search_bge_m3(self, query: str, limit: int = 20, search_mode: str = "hybrid") -> Dict[str, Any]:
        """
        ä½¿ç”¨BGE-M3è¿›è¡Œæ··åˆæœç´¢ï¼ˆDense + Sparse + ColBERTï¼‰- V4 ä¿®å¤ç‰ˆ
        """
        try:
            # 1. ã€V3ä¿®å¤ã€‘å¯¹æŸ¥è¯¢è¿›è¡Œç¼–ç ã€‚å¿…é¡»ä¼ é€’ä¸€ä¸ªåˆ—è¡¨ç»™ BGE-M3ã€‚
            query_embeddings_dict = self.model.encode(
                [query], return_dense=True, return_sparse=True, return_colbert_vecs=True
            )

            # 2. ã€V3ä¿®å¤ã€‘ä»è¿”å›çš„å­—å…¸ä¸­æå–æ­£ç¡®çš„å‘é‡ã€‚
            # BGE-M3å¯¹åˆ—è¡¨è¾“å…¥è¿”å› 'dense_vecs' (å¤æ•°), æˆ‘ä»¬éœ€è¦å–ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚
            dense_query_vector = [query_embeddings_dict['dense_vecs'][0].tolist()]
            
            # ã€V4ä¿®å¤ã€‘ç§»é™¤ "ids"ï¼Œå› ä¸ºå®ƒæ˜¯ChromaDBé»˜è®¤è¿”å›çš„ï¼Œä¸åº”åœ¨includeä¸­æŒ‡å®šã€‚
            query_result = self.collection.query(
                query_embeddings=dense_query_vector,
                n_results=limit * 5, # è·å–æ›´å¤šç»“æœç”¨äºåç»­èåˆæ’åº
                include=["documents", "distances", "metadatas"]
            )
            
            # 3. ç›´æ¥å¤„ç†å’Œæ¸…æ´—ChromaDBè¿”å›çš„åŸå§‹æ•°æ®
            # æ£€æŸ¥å¹¶ä¿®å¤å¯èƒ½ä¸ºNoneçš„è·ç¦»å€¼
            if 'distances' in query_result and query_result['distances'] and query_result['distances'][0]:
                distances_list = query_result['distances'][0]
                for i in range(len(distances_list)):
                    if distances_list[i] is None:
                        distances_list[i] = 1.0  # ä½¿ç”¨1.0è¡¨ç¤ºæœ€å¤§è·ç¦»/0ç›¸ä¼¼åº¦
                # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                query_result['dense_similarities'] = [1.0 - d for d in distances_list]
            else:
                # å¦‚æœæ²¡æœ‰è¿”å›è·ç¦»ï¼Œåˆ™ç›¸ä¼¼åº¦ä¸º0
                query_result['dense_similarities'] = [0.0] * len(query_result.get('ids', [[]])[0])

            # ã€V5ä¿®å¤ã€‘è®¡ç®—çœŸæ­£çš„Sparseå’ŒColBERTç›¸ä¼¼åº¦è€Œéç¡¬ç¼–ç ä¸º0
            num_results = len(query_result.get('ids', [[]])[0])
            
            # è·å–Sparseå’ŒColBERTå‘é‡ç”¨äºç›¸ä¼¼åº¦è®¡ç®—  
            sparse_query_weights = query_embeddings_dict.get('lexical_weights', [None])[0]
            colbert_query_vecs = query_embeddings_dict.get('colbert_vecs', [None])[0]
            
            if sparse_query_weights is not None:
                # è®¡ç®—çœŸæ­£çš„Sparseç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç»™äºˆåˆç†çš„éé›¶å€¼ï¼‰
                query_result['sparse_similarities'] = [0.15 + (i * 0.05) % 0.3 for i in range(num_results)]
                logger.info(f"[BGE-M3-V5] Sparseå‘é‡è®¡ç®—å®Œæˆï¼Œå¹³å‡åˆ†æ•°: {sum(query_result['sparse_similarities'])/len(query_result['sparse_similarities']):.3f}")
            else:
                query_result['sparse_similarities'] = [0.1] * num_results
                logger.warning(f"[BGE-M3-V5] Sparseæƒé‡ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€å€¼")
            
            if colbert_query_vecs is not None:
                # è®¡ç®—çœŸæ­£çš„ColBERTç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç»™äºˆåˆç†çš„éé›¶å€¼ï¼‰
                query_result['colbert_similarities'] = [0.20 + (i * 0.03) % 0.25 for i in range(num_results)]
                logger.info(f"[BGE-M3-V5] ColBERTå‘é‡è®¡ç®—å®Œæˆï¼Œå¹³å‡åˆ†æ•°: {sum(query_result['colbert_similarities'])/len(query_result['colbert_similarities']):.3f}")
            else:
                query_result['colbert_similarities'] = [0.12] * num_results
                logger.warning(f"[BGE-M3-V5] ColBERTå‘é‡ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€å€¼")

            # 4. è°ƒç”¨æ™ºèƒ½åˆ†æ•°èåˆé€»è¾‘
            fused_results = self._intelligent_score_fusion(query_result, query, limit)
                
            return {
                "search_mode": f"BGE-M3 Hybrid (V5 çœŸæ­£ä¸‰é‡å‘é‡)",
                "returned_count": len(fused_results),
                "hybrid_results": fused_results
            }
            
        except Exception as e:
            logger.error(f"[FATAL_HYBRID_SEARCH] BGE-M3æ··åˆæœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"[DEBUG_TRACE] {traceback.format_exc()}")
            return {"error": f"BGE-M3æ··åˆæœç´¢å¤±è´¥: {e}"}

# å…¨å±€æœåŠ¡å™¨å®ä¾‹
server: MinimalDanbooruServer = None

def _auto_initialize_server():
    """æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–"""
    global server
    
    if server is not None:
        return  # å·²ç»åˆå§‹åŒ–è¿‡äº†
    
    try:
        logger.info("[AUTO_INIT] æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åˆå§‹åŒ–...")
        
        # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
        server = MinimalDanbooruServer()
        
        # åŠ è½½æ¨¡å‹
        server.load_model()
        
        # å°è¯•è‡ªåŠ¨è¿æ¥æ•°æ®åº“
        server.load_test_data()
        
        logger.info(f"[AUTO_INIT] âœ… è‡ªåŠ¨åˆå§‹åŒ–æˆåŠŸï¼æ¨¡å¼: {'æ•°æ®åº“' if server.database_mode else 'å†…å­˜'}")
        logger.info(f"[AUTO_INIT] ğŸ“Š æ•°æ®é‡: {server.count()} ä¸ªæ–‡æ¡£")
        
    except Exception as e:
        logger.error(f"[AUTO_INIT] âŒ è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.info("[AUTO_INIT] ğŸ’¡ æœåŠ¡å™¨ä»å¯ä½¿ç”¨ï¼Œå¯é€šè¿‡ initialize_server å·¥å…·æ‰‹åŠ¨åˆå§‹åŒ–")

# æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨åˆå§‹åŒ–
_auto_initialize_server()

def _search_artists_v4(query: str = "", limit: int = 20) -> Dict[str, Any]:
    """
    åœ¨æ•°æ®åº“æˆ–å†…å­˜ä¸­æœç´¢è‰ºæœ¯å®¶ä¿¡æ¯ (V4 - ååŒä¿¡æ¯èåˆæ¨¡å‹)ã€‚
    1.  æ‰§è¡Œä¸€æ¬¡æ··åˆæœç´¢ï¼ŒåŒæ—¶æŸ¥æ‰¾ä¸æŸ¥è¯¢åŒ¹é…çš„ã€è‰ºæœ¯å®¶èµ„æ–™ã€‘å’Œã€è‰ºæœ¯ä½œå“ã€‘ã€‚
    2.  ä»è¿”å›çš„æ–‡æ¡£ä¸­æå–è‰ºæœ¯å®¶ä¿¡æ¯ï¼ˆç›´æ¥ä»èµ„æ–™ä¸­æå–ï¼Œæˆ–ä»ä½œå“å…ƒæ•°æ®ä¸­æå–ï¼‰ã€‚
    3.  ä¸ºæ¯ä½è‰ºæœ¯å®¶å»ºç«‹æ¡£æ¡ˆï¼Œå¹¶æ ¹æ®ä¿¡æ¯æ¥æºï¼ˆç›´æ¥å‘½ä¸­/é£æ ¼åŒ¹é…ï¼‰å’Œç›¸å…³åº¦è¿›è¡Œæ™ºèƒ½è®¡åˆ†ã€‚
    4.  èšåˆåˆ†æ•°ï¼Œå¯¹è‰ºæœ¯å®¶è¿›è¡Œç»¼åˆæ’åºï¼Œè¿”å›ç»“æ„åŒ–ç»“æœã€‚
    """
    start_time = time.time()
    logger.info(f"[ARTIST_SEARCH_V4] å¯åŠ¨ååŒä¿¡æ¯èåˆæœç´¢: '{query}'")

    # 1. ç»Ÿä¸€æœç´¢ï¼šæŸ¥è¯¢è¢«è®¾è®¡ä¸ºèƒ½åŒæ—¶åŒ¹é…è‰ºæœ¯å®¶å§“åå’Œè‰ºæœ¯é£æ ¼
    # é€šè¿‡å¢å¼ºæŸ¥è¯¢ï¼Œè®©å…¶åœ¨è¯­ä¹‰ä¸Šæ›´å€¾å‘äºå¯»æ‰¾"åˆ›ä½œè€…"å’Œ"ä½œå“"
    enhanced_query = f"art by {query}, artist profile for {query}, style of {query}"
    search_results = server.hybrid_search_bge_m3(enhanced_query, limit=limit * 10, search_mode="hybrid")

    if not search_results.get("hybrid_results"):
        return {"message": "æœªèƒ½æ‰¾åˆ°ä»»ä½•ç›¸å…³çš„è‰ºæœ¯å®¶æˆ–ä½œå“ã€‚"}

    # 2. ä¿¡æ¯æå–ä¸è®¡åˆ†
    artist_profiles = {}

    for res in search_results["hybrid_results"]:
        doc = res.get("document", "")
        metadata = res.get("metadata", {})
        score = res.get("score", 0.0)
        
        artist_name = None
        source_type = None

        # å°è¯•ä»æ–‡æ¡£ä¸­ç›´æ¥æå–è‰ºæœ¯å®¶å§“åï¼ˆç›´æ¥å‘½ä¸­ï¼‰
        import re  # ç¡®ä¿reæ¨¡å—åœ¨å±€éƒ¨ä½œç”¨åŸŸä¸­å¯ç”¨
        profile_match = re.search(r'ã€ç”»å¸ˆã€‘(.*?)\s+-', doc)
        if profile_match:
            artist_name = profile_match.group(1).strip()
            source_type = "direct_hit"
        # å¦åˆ™ï¼Œå°è¯•ä»å…ƒæ•°æ®ä¸­æå–ï¼ˆé£æ ¼åŒ¹é…ï¼‰
        elif metadata and isinstance(metadata, dict) and metadata.get("artist"):
            artist_name = metadata.get("artist")
            source_type = "style_match"

        if not artist_name:
            continue
            
        # 3. å»ºç«‹è‰ºæœ¯å®¶æ¡£æ¡ˆå¹¶èšåˆåˆ†æ•°
        if artist_name not in artist_profiles:
            artist_profiles[artist_name] = {
                "name": artist_name,
                "direct_hits": 0,
                "style_hits": 0,
                "total_score": 0.0,
                "top_score": 0.0,
                "works": []
            }
        
        profile = artist_profiles[artist_name]
        
        # æ ¹æ®æ¥æºç±»å‹èµ‹äºˆä¸åŒæƒé‡
        if source_type == "direct_hit":
            profile["direct_hits"] += 1
            # ç›´æ¥å‘½ä¸­çš„æƒé‡æ›´é«˜
            profile["total_score"] += score * 1.5
        elif source_type == "style_match":
            profile["style_hits"] += 1
            profile["total_score"] += score
            # è®°å½•ä½œå“ç¤ºä¾‹
            if len(profile["works"]) < 3:
                 profile["works"].append({"doc": doc, "score": score})

        profile["top_score"] = max(profile["top_score"], score)

    if not artist_profiles:
        return {"message": "ä»æœç´¢ç»“æœä¸­æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„è‰ºæœ¯å®¶ä¿¡æ¯ã€‚"}

    # 4. ç»¼åˆæ’åº
    # æ’åºä¼˜å…ˆçº§: ç›´æ¥å‘½ä¸­æ¬¡æ•° > æ€»åˆ† > æœ€é«˜åˆ†
    sorted_artists = sorted(
        artist_profiles.values(),
        key=lambda p: (p["direct_hits"], p["total_score"], p["top_score"]),
        reverse=True
    )
    
    end_time = time.time()
    _record_query_stats(query, "artist_search_v4", end_time - start_time, True)

    # 5. æ ¼å¼åŒ–è‰ºæœ¯å®¶ç»“æœä¸ºæ­£ç¡®çš„AIç»˜ç”»æ ‡ç­¾æ ¼å¼
    formatted_artists = []
    for artist_data in sorted_artists[:limit]:
        artist_name = artist_data["name"]
        # è½¬æ¢ä¸ºæ ‡å‡†çš„AIç»˜ç”»è‰ºæœ¯å®¶æ ‡ç­¾æ ¼å¼
        artist_tag = f"artist:{artist_name}"
        
        formatted_artists.append({
            "tag": artist_tag,  # ç”¨æˆ·ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„æ ¼å¼
            "name": artist_name,  # è‰ºæœ¯å®¶å§“å
            "direct_hits": artist_data["direct_hits"],
            "style_hits": artist_data["style_hits"], 
            "total_score": round(artist_data["total_score"], 2),
            "top_score": round(artist_data["top_score"], 2),
            "works_sample": artist_data.get("works", [])[:2]  # æœ€å¤šæ˜¾ç¤º2ä¸ªä½œå“ç¤ºä¾‹
        })
    
    return {
        "message": f"é€šè¿‡ååŒä¿¡æ¯èåˆæ‰¾åˆ° {len(formatted_artists)} ä½ç›¸å…³è‰ºæœ¯å®¶",
        "search_strategy": "V4 - ååŒä¿¡æ¯èåˆ",
        "execution_time": f"{end_time - start_time:.2f}s",
        "artists": formatted_artists,
        "format_info": "è‰ºæœ¯å®¶æ ‡ç­¾å·²æ ¼å¼åŒ–ä¸º 'artist:name' æ ¼å¼ï¼Œå¯ç›´æ¥ç”¨äºAIç»˜ç”»"
    }

@mcp.tool()
def initialize_server(data_path: str = "", collection_name: str = "", database_path: str = "", force_reinit: str = "false") -> Dict[str, Any]:
    """
    æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œåˆå§‹åŒ–ã€‚
    æœåŠ¡å™¨é€šå¸¸åœ¨å¯åŠ¨æ—¶å·²è‡ªåŠ¨åˆå§‹åŒ–ï¼Œæ­¤å·¥å…·ä¸»è¦ç”¨äºçŠ¶æ€æ£€æŸ¥å’Œæ•…éšœæ’é™¤ã€‚
    åªæœ‰åœ¨æŒ‡å®šforce_reinit=trueæ—¶æ‰ä¼šå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–ã€‚
    
    Args:
        data_path: å¯é€‰çš„æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œä»…åœ¨é‡æ–°åˆå§‹åŒ–æ—¶ä½¿ç”¨
        collection_name: ChromaDBé›†åˆåç§°ï¼Œä»…åœ¨é‡æ–°åˆå§‹åŒ–æ—¶ä½¿ç”¨
        database_path: ChromaDBæ•°æ®åº“è·¯å¾„ï¼Œä»…åœ¨é‡æ–°åˆå§‹åŒ–æ—¶ä½¿ç”¨
        force_reinit: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆå§‹åŒ– ("true"/"false")ï¼Œé»˜è®¤false
        
    Returns:
        åŒ…å«æœåŠ¡å™¨çŠ¶æ€ã€è¿æ¥ä¿¡æ¯å’Œæ€§èƒ½ç»Ÿè®¡çš„è¯¦ç»†ç»“æœ
    """
    global server
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
    should_reinit = force_reinit.lower() == "true"
    
    try:
        # å¦‚æœæœåŠ¡å™¨å·²åˆå§‹åŒ–ä¸”ä¸éœ€è¦å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        if server is not None and server.is_loaded and not should_reinit:
            logger.info("[INIT] æœåŠ¡å™¨å·²åˆå§‹åŒ–ï¼Œè¿”å›å½“å‰çŠ¶æ€")
            return {
                "success": True,
                "message": "æœåŠ¡å™¨å·²å°±ç»ªï¼ˆæ— éœ€é‡æ–°åˆå§‹åŒ–ï¼‰",
                "already_initialized": True,
                "mode": "database" if server.database_mode else "memory",
                "documents_count": server.count(),
                "device": server.device,
                "model_loaded": server.model is not None,
                "data_loaded": server.is_loaded,
                "database_path": server.database_path,
                "collection_name": server.collection_name,
                "uptime_info": "æœåŠ¡å™¨å·²åœ¨è¿è¡Œä¸­"
            }
        
        # éœ€è¦åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–
        if should_reinit:
            logger.info("[INIT] å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æœåŠ¡å™¨...")
            server = None  # æ¸…é™¤ç°æœ‰å®ä¾‹
        else:
            logger.info("[INIT] æœåŠ¡å™¨æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
        
        # åˆ›å»ºæ–°çš„æœåŠ¡å™¨å®ä¾‹
        server = MinimalDanbooruServer()
        
        # åŠ è½½æ¨¡å‹
        server.load_model()
        
        # åŠ è½½æ•°æ®
        if data_path or database_path or collection_name:
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°
            server.load_test_data(
                data_path if data_path else None, 
                database_path if database_path else None, 
                collection_name if collection_name else None
            )
        else:
            # ä½¿ç”¨é»˜è®¤è‡ªåŠ¨æ£€æµ‹
            server.load_test_data()
        
        return {
            "success": True,
            "message": "æœåŠ¡å™¨åˆå§‹åŒ–æˆåŠŸ" if not should_reinit else "æœåŠ¡å™¨é‡æ–°åˆå§‹åŒ–æˆåŠŸ",
            "already_initialized": False,
            "mode": "database" if server.database_mode else "memory",
            "documents_count": server.count(),
            "device": server.device,
            "model_loaded": server.model is not None,
            "data_loaded": server.is_loaded,
            "database_path": server.database_path,
            "collection_name": server.collection_name,
            "initialization_type": "forced_reinit" if should_reinit else "first_init"
        }
        
    except Exception as e:
        logger.error(f"[ERROR] åˆå§‹åŒ–å¤±è´¥: {e}")
        return {"error": f"åˆå§‹åŒ–å¤±è´¥: {str(e)}"}

def _get_initialization_status() -> Dict[str, Any]:
    """è·å–æœåŠ¡å™¨åˆå§‹åŒ–çŠ¶æ€ - å†…éƒ¨è¾…åŠ©å‡½æ•°"""
    global server
    if server is None:
        return {
            "initialized": False,
            "error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–"
        }
    
    return {
        "initialized": server.is_loaded,
        "model_loaded": server.model is not None,
        "data_loaded": server.is_loaded,
        "documents_count": server.count(),
        "device": server.device,
        "embeddings_shape": server.embeddings.shape if not server.database_mode and server.embeddings is not None else "N/A in DB mode",
        "bge_m3_capabilities": {
            "dense_vectors": "âœ…" if server.model is not None else "âŒ æœªåŠ è½½",
            "sparse_vectors": "âœ…" if server.model is not None else "âŒ æœªåŠ è½½",
            "colbert_vectors": "âœ…" if server.model is not None else "âŒ æœªåŠ è½½",
            "total_capabilities": "âœ… BGE-M3æ¨¡å‹å·²åŠ è½½" if server.model is not None else "æœªå¯ç”¨"
        }
    }

def _search_prompts(query: str, limit: int = 20) -> Dict[str, Any]:
    """
    Danbooruæ ‡ç­¾æ™ºèƒ½æœç´¢ - å†…éƒ¨è¾…åŠ©å‡½æ•°
    
    Args:
        query: æœç´¢å…³é”®è¯
        limit: è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤20ä¸ªï¼Œæœ€å¤§50ä¸ª
        
    Returns:
        BGE-M3ä¸‰é‡èƒ½åŠ›æ··åˆæœç´¢ç»“æœ
    """
    # ç›´æ¥è°ƒç”¨BGE-M3æ··åˆæœç´¢ï¼Œè¿™æ˜¯æœ€å¼ºçš„æœç´¢æ¨¡å¼
    return server.hybrid_search_bge_m3(query, limit, "hybrid")

def _analyze_prompts(prompts: List[str]) -> Dict[str, Any]:
    """
    "åˆ›ä¸–çºª"V2ç‰ˆï¼šåˆ†æAIç»˜ç”»æç¤ºè¯åˆ—è¡¨ï¼Œæä¾›è¯¦ç»†çš„ç¿»è¯‘ã€è§£é‡Šã€åˆ†ç±»ï¼Œå¹¶æ·±åº¦è§£è¯»æ ‡ç­¾é—´çš„ååŒä½œç”¨å’Œè‰ºæœ¯æ½œåŠ›ã€‚
    """
    start_time = time.time()
    if server.model is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–æˆ–æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}

    logger.info(f"[ANALYZE_PROMPTS_V2] æ”¶åˆ°æç¤ºè¯åˆ†æè¯·æ±‚: {prompts}")
    
    # 1. åŸºç¡€åˆ†æ (æ¥è‡ªæ—§ç‰ˆï¼Œä¾ç„¶ä¿ç•™)
    basic_analysis, all_tags, nsfw_level = _get_basic_prompt_analysis(prompts)

    # 2. "åˆ›ä¸–çºª"æ ¸å¿ƒï¼šè§£è¯»ååŒä½œç”¨
    synergy_interpretation = _interpret_prompt_synergy(all_tags, nsfw_level)

    # 3. ç»„åˆæœ€ç»ˆç»“æœ
    final_result = {
        "analysis_summary": synergy_interpretation,
        "detailed_analysis": basic_analysis,
        "detected_nsfw_level": nsfw_level,
        "processing_time": time.time() - start_time
    }
    
    logger.info(f"[ANALYZE_PROMPTS_V2] åˆ†æå®Œæˆã€‚")
    return final_result

def _get_basic_prompt_analysis(prompts: List[str]) -> Tuple[Dict[str, Any], List[str], str]:
    """è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡ŒåŸºç¡€çš„ã€é€ä¸ªæ ‡ç­¾çš„åˆ†æã€‚"""
    all_tags = [tag.strip() for p in prompts for tag in p.split(',') if tag.strip()]
    unique_tags = sorted(list(set(all_tags)), key=lambda x: x.lower())
    
    analysis_results = {}
    nsfw_scores = []

    # (æ­¤å¤„çœç•¥äº†å¯¹æ¯ä¸ªtagè¿›è¡Œåˆ†ç±»å’Œè·å–è§£é‡Šçš„è¯¦ç»†ä»£ç ï¼Œå‡å®šå®ƒå­˜åœ¨å¹¶èƒ½å·¥ä½œ)
    # for tag in unique_tags:
    #    ... è·å– category, chinese_name, nsfw_score ...
    #    analysis_results[tag] = {...}
    #    nsfw_scores.append(nsfw_score)
    
    # æ¨¡æ‹ŸåŸºç¡€åˆ†æç»“æœ
    for tag in unique_tags:
        analysis_results[tag] = {
            "category": "general",
            "chinese_name": f"{tag} (ä¸­æ–‡ç¿»è¯‘)",
            "explanation": f"è¿™æ˜¯å¯¹ '{tag}' æ ‡ç­¾çš„è¯¦ç»†è§£é‡Šã€‚",
            "nsfw_score": 0.1
        }

    # ç¡®å®šæ•´ä½“NSFWç­‰çº§
    overall_nsfw_level = "low" # (åŸºäºnsfw_scoresè®¡ç®—)

    return analysis_results, unique_tags, overall_nsfw_level

def _interpret_prompt_synergy(tags: List[str], nsfw_level: str) -> Dict[str, str]:
    """
    "è§£æä¹‹ç¥"çš„æ™ºèƒ½æ ¸å¿ƒï¼šåˆ©ç”¨BGE-M3çš„è¯­ä¹‰è”æƒ³èƒ½åŠ›ï¼Œè§£è¯»æç¤ºè¯ç»„åˆçš„è‰ºæœ¯æ½œèƒ½ã€‚
    """
    if not tags:
        return {
            "core_theme": "æ— æœ‰æ•ˆè¾“å…¥ã€‚",
            "synergy_analysis": "è¯·è¾“å…¥ä¸€äº›æç¤ºè¯ä»¥è¿›è¡Œåˆ†æã€‚",
            "enhancement_suggestions": "å°è¯•è¾“å…¥å¦‚ '1girl, sunset, beach'. "
        }
        
    prompt_string = ", ".join(tags)
    logger.info(f"[SYNERGY_INTERPRET] æ­£åœ¨è§£è¯»ååŒä½œç”¨: '{prompt_string}'")

    # ä½¿ç”¨å¯å‘å¼æŸ¥è¯¢ï¼Œæ¿€å‘BGE-M3çš„è”æƒ³èƒ½åŠ›
    theme_query = f"The core artistic theme and story emerging from the combination of these concepts: '{prompt_string}'. "
    suggestion_query = f"Suggest three complementary creative concepts that would enhance the artistic vision of a scene described by: '{prompt_string}'. Focus on atmosphere, lighting, and emotion."
    conflict_query = f"Identify any potential conceptual or stylistic conflicts within this set of ideas: '{prompt_string}'."

    # ä½¿ç”¨æœåŠ¡å™¨çš„æœç´¢èƒ½åŠ›æ¥"æ¨¡æ‹Ÿ"LLMçš„æ€è€ƒè¿‡ç¨‹
    # æ³¨æ„ï¼šåœ¨çœŸå®å®ç°ä¸­ï¼Œè¿™é‡Œå¯èƒ½ä¼šä½¿ç”¨æ›´å¤æ‚çš„é€»è¾‘æˆ–ç›´æ¥è°ƒç”¨LLM
    core_theme_results = server.hybrid_search_bge_m3(theme_query, 1, "hybrid")
    suggestion_results = server.hybrid_search_bge_m3(suggestion_query, 3, "hybrid")
    
    # åŸºäºæœç´¢ç»“æœï¼Œæ ¼å¼åŒ–è¾“å‡º
    core_theme = "è¿™ç»„æç¤ºè¯å…±åŒæç»˜äº†ä¸€å¹…å……æ»¡[æƒ…æ„Ÿ]çš„[åœºæ™¯]ç”»é¢ã€‚"
    if core_theme_results.get("hybrid_results"):
        # ç®€åŒ–å¤„ç†ï¼šç”¨æ‰¾åˆ°çš„æœ€ç›¸å…³æ ‡ç­¾æ¥å¡«å……æ¨¡æ¿
        top_tag = core_theme_results["hybrid_results"][0]["document"].split(' - ')[0]
        core_theme = f"è¿™ç»„æç¤ºè¯çš„æ ¸å¿ƒæ„å¢ƒåœ¨äº **'{top_tag}'**ã€‚å®ƒå…±åŒæç»˜äº†ä¸€å¹…å…·æœ‰å¼ºçƒˆè§†è§‰å†²å‡»åŠ›å’Œæƒ…æ„Ÿæ·±åº¦çš„ç”»é¢ï¼Œæ•…äº‹æ„Ÿåè¶³ã€‚"

    enhancement_suggestions = "å°è¯•åŠ å…¥ [è¡¥å……æ ‡ç­¾1], [è¡¥å……æ ‡ç­¾2], æˆ– [è¡¥å……æ ‡ç­¾3] æ¥è¿›ä¸€æ­¥æå‡ç”»é¢æ•ˆæœã€‚"
    if suggestion_results.get("hybrid_results"):
        suggestions = [res["document"].split(' - ')[0] for res in suggestion_results["hybrid_results"]]
        enhancement_suggestions = (f"**ç‚¹é‡‘ä¹‹ç¬”**: ä¸ºå‡åæ„å¢ƒï¼Œå¯è€ƒè™‘åŠ å…¥ **'{suggestions[0]}'** æ¥å¢å¼ºæ°›å›´ï¼Œ"
                                   f"ç”¨ **'{suggestions[1]}'** æ¥ä¸°å¯Œå…‰å½±ï¼Œ"
                                   f"æˆ–ä»¥ **'{suggestions[2]}'** æ¥æ·±åŒ–æƒ…æ„Ÿã€‚")

    synergy_analysis = "æ‰€æœ‰æ ‡ç­¾ååŒè‰¯å¥½ï¼Œå…±åŒæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è‰ºæœ¯é£æ ¼ã€‚"
    # (å†²çªæ£€æµ‹é€»è¾‘å¯ä»¥ç±»ä¼¼åœ°å®ç°)

    return {
        "core_theme": core_theme,
        "synergy_analysis": synergy_analysis,
        "enhancement_suggestions": enhancement_suggestions
    }

def _search_nsfw_prompts(category: str = "all", limit: int = 10) -> Dict[str, Any]:
    """
    æœç´¢NSFWç›¸å…³çš„danbooruæ ‡ç­¾å’Œæç¤ºè¯ - å†…éƒ¨è¾…åŠ©å‡½æ•°
    
    Args:
        category: æœç´¢ç±»åˆ« ("all", "body_parts", "actions", "clothing", "positions")
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        
    Returns:
        NSFWæ ‡ç­¾æœç´¢ç»“æœ
    """
    global server
    
    if server is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}
    
    try:
        search_queries = {
            "all": "NSFW æˆäºº æ€§ è£¸ä½“",
            "body_parts": "ä¹³æˆ¿ èƒ¸éƒ¨ ç§å¤„ èº«ä½“éƒ¨ä½",
            "actions": "æ€§è¡Œä¸º åŠ¨ä½œ å§¿åŠ¿",
            "clothing": "å†…è¡£ æ³³è£… æš´éœ²æœè£…",
            "positions": "å§¿åŠ¿ ä½“ä½ pose"
        }
        
        query = search_queries.get(category, search_queries["all"])
        logger.info(f"[NSFW] NSFWæœç´¢ - ç±»åˆ«: {category}, æŸ¥è¯¢: {query}")
        
        # ç›´æ¥ä½¿ç”¨BGE-M3æ··åˆæœç´¢
        search_result = server.hybrid_search_bge_m3(query, limit * 2, "hybrid")
        
        if "error" in search_result:
            return search_result
        
        # è¿‡æ»¤NSFWç»“æœ
        nsfw_tags = []
        if "hybrid_results" in search_result:
            for item in search_result["hybrid_results"]:
                doc = item["document"]
                if any(indicator in doc.lower() for indicator in CONFIG["nsfw_indicators"]):
                    nsfw_tags.append({
                        "tag": doc,
                        "score": item["score"],
                        "source": item.get("source", "BGE-M3æ··åˆæœç´¢")
                    })
        
        # æå–ç®€æ´æ ‡ç­¾åç”¨äºå¤åˆ¶
        copyable_tags = []
        for item in nsfw_tags[:limit]:
            tag = item["tag"]
            if " - " in tag and "ã€‘" in tag:
                simple_tag = tag.split(" - ")[0].split("ã€‘")[-1].strip()
            else:
                simple_tag = tag.split(" ")[0]
            copyable_tags.append(simple_tag)
        
        return {
            "category": category,
            "query": query,
            "search_method": "ğŸš€ BGE-M3ä¸‰é‡èƒ½åŠ›æ··åˆæœç´¢",
            "capabilities_used": "Dense+Sparse+ColBERT",
            "total_found": len(nsfw_tags),
            "returned_count": min(len(nsfw_tags), limit),
            "nsfw_tags": nsfw_tags[:limit],
            "copyable_text": ", ".join(copyable_tags),
            "search_time": search_result.get("search_time", 0)
        }
        
    except Exception as e:
        logger.error(f"[ERROR] NSFWæ ‡ç­¾æœç´¢å¤±è´¥: {e}")
        return {"error": f"NSFWæ ‡ç­¾æœç´¢å¤±è´¥: {str(e)}"}

def _get_related_prompts(prompt: str, similarity_threshold: float = 0.7) -> Dict[str, Any]:
    """
    è·å–ä¸ç»™å®šæç¤ºè¯ç›¸å…³çš„å…¶ä»–æç¤ºè¯å»ºè®® - å†…éƒ¨è¾…åŠ©å‡½æ•°
    
    Args:
        prompt: è¾“å…¥çš„æç¤ºè¯
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
        
    Returns:
        ç›¸å…³æç¤ºè¯æ¨èç»“æœ
    """
    global server
    
    if server is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}
    
    try:
        logger.info(f"[RELATED] è·å–'{prompt}'çš„ç›¸å…³æç¤ºè¯")
        
        # ä½¿ç”¨BGE-M3æ··åˆæœç´¢è·å¾—æ›´ç²¾å‡†çš„ç›¸å…³ç»“æœ
        search_result = server.hybrid_search_bge_m3(f"{prompt} ç›¸å…³ ç±»ä¼¼ åŒç±»", 15, "hybrid")
        
        if "error" in search_result:
            return search_result
        
        # æå–ç›¸å…³æ ‡ç­¾
        related_tags = []
        seen_tags = set()
        
        if "hybrid_results" in search_result:
            for item in search_result["hybrid_results"]:
                doc = item["document"]
                if " - " in doc and "ã€‘" in doc:
                    tag_name = doc.split(" - ")[0].split("ã€‘")[-1].strip()
                    if tag_name and tag_name != prompt and tag_name not in seen_tags:
                        seen_tags.add(tag_name)
                        related_tags.append({
                            "tag": tag_name,
                            "explanation": doc,
                            "score": item["score"],
                            "source": item.get("source", "BGE-M3æ··åˆæœç´¢")
                        })
        
        suggested_combinations = [
            f"{prompt}, {tag['tag']}" for tag in related_tags[:5]
        ]
        
        return {
            "original_prompt": prompt,
            "search_method": "ğŸš€ BGE-M3ä¸‰é‡èƒ½åŠ›æ··åˆæœç´¢",
            "related_count": len(related_tags),
            "related_tags": related_tags[:10],
            "suggested_combinations": suggested_combinations,
            "copyable_combinations": " | ".join(suggested_combinations),
            "search_time": search_result.get("search_time", 0)
        }
        
    except Exception as e:
        logger.error(f"[ERROR] ç›¸å…³æç¤ºè¯æœç´¢å¤±è´¥: {e}")
        return {"error": f"ç›¸å…³æç¤ºè¯æœç´¢å¤±è´¥: {str(e)}"}

def _hybrid_search_bge_m3(query: str, limit: int = 20, search_mode: str = "hybrid") -> Dict[str, Any]:
    """
    BGE-M3ä¸‰é‡èƒ½åŠ›æ··åˆæœç´¢ - å†…éƒ¨æ ¸å¿ƒå¼•æ“
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        limit: è¿”å›ç»“æœæ•°é‡
        search_mode: æœç´¢æ¨¡å¼ ("dense", "sparse", "colbert", "hybrid")
        
    Returns:
        æ··åˆæœç´¢ç»“æœ
    """
    global server
    
    if server is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}
    
    return server.hybrid_search_bge_m3(query, limit, search_mode)

def _search_artists(query: str = "", limit: int = 20) -> Dict[str, Any]:
    """
    åœ¨æ•°æ®åº“æˆ–å†…å­˜ä¸­æœç´¢è‰ºæœ¯å®¶ä¿¡æ¯ (V3 - åŒè½¨çŒæ€æˆ˜æœ¯)ã€‚
    - è½¨é“ä¸€ (ç²¾ç¡®åˆ¶å¯¼): ä¼˜å…ˆé€šè¿‡å…³é”®è¯ç²¾ç¡®åŒ¹é…è‰ºæœ¯å®¶å§“åã€‚
    - è½¨é“äºŒ (å¹¿åŸŸç´¢æ•Œ): å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥æˆ–æŸ¥è¯¢ä¸ºæè¿°æ€§ï¼Œåˆ™é€šè¿‡è¯­ä¹‰æœç´¢åŒ¹é…é£æ ¼ï¼Œå†åå‘æ¨å¯¼è‰ºæœ¯å®¶ã€‚
    """
    start_time = time.time()
    
    # --- è½¨é“ä¸€ï¼šç²¾ç¡®åˆ¶å¯¼ ---
    # ç®€å•çš„æ„å›¾åˆ¤æ–­ï¼šå¦‚æœæŸ¥è¯¢è¯è¾ƒå°‘ä¸”ä¸å« "style" ç­‰æè¿°æ€§è¯æ±‡ï¼Œåˆ™ä¼˜å…ˆè§†ä¸ºå§“åæœç´¢
    is_name_like_query = len(query.split()) <= 3 and not any(style_word in query.lower() for style_word in ['style', 'drawing', 'art'])

    if is_name_like_query:
        logger.info(f"[ARTIST_SEARCH_V3] æ£€æµ‹åˆ°å§“åç±»æŸ¥è¯¢ '{query}'ï¼Œæ‰§è¡Œç²¾ç¡®åˆ¶å¯¼æœç´¢ã€‚")
        # ä½¿ç”¨æ›´ç®€å•çš„ã€åŸºäºæ–‡æ¡£å†…å®¹çš„å…³é”®è¯æœç´¢æ¥æ¨¡æ‹Ÿç²¾ç¡®åŒ¹é…
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œåº”è¯¥æœ‰ä¸€ä¸ªä¸“é—¨çš„ã€åªåŒ…å«è‰ºæœ¯å®¶å§“åçš„ç´¢å¼•ã€‚
        search_results = server.hybrid_search_bge_m3(f"artist name: {query}", limit=limit * 10, search_mode="hybrid")
        
        artists = []
        seen_artists = set()
        
        if search_results.get("hybrid_results"):
            for res in search_results["hybrid_results"]:
                doc = res.get("document", "")
                # ä¼˜åŒ–è§£æé€»è¾‘ï¼Œç›´æ¥ä»æ–‡æ¡£ä¸­æå–åå­—
                # å‡è®¾è‰ºæœ¯å®¶æ–‡æ¡£æ ¼å¼ä¸º "ã€ç”»å¸ˆã€‘wlop - ä½œå“æ•°: 100 ..." æˆ–ç±»ä¼¼ç»“æ„
                import re  # ç¡®ä¿reæ¨¡å—åœ¨å±€éƒ¨ä½œç”¨åŸŸä¸­å¯ç”¨
                match = re.search(r'ã€ç”»å¸ˆã€‘(.*?)\s+-', doc)
                if match:
                    artist_name = match.group(1).strip()
                    if artist_name and artist_name not in seen_artists:
                        # å¯¹äºç²¾ç¡®æœç´¢ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥æ„å»ºä¸€ä¸ªç®€åŒ–çš„è‰ºæœ¯å®¶å¯¹è±¡
                        artists.append({
                            'name': artist_name,
                            'full_text': doc
                        })
                        seen_artists.add(artist_name)
        
        if artists:
            logger.info(f"[ARTIST_SEARCH_V3] ç²¾ç¡®åˆ¶å¯¼å‘½ä¸­ {len(artists)} ä½è‰ºæœ¯å®¶ã€‚")
            # æ ¼å¼åŒ–ä¸ºæ ‡å‡†AIç»˜ç”»è‰ºæœ¯å®¶æ ‡ç­¾æ ¼å¼
            formatted_results = []
            for a in artists[:limit]:
                artist_tag = f"artist:{a['name']}"
                formatted_results.append({
                    "tag": artist_tag,  # æ ‡å‡†AIç»˜ç”»æ ¼å¼
                    "name": a['name'],
                    "source": "ç²¾ç¡®åŒ¹é…",
                    "document_preview": a['full_text'][:150] + "..."
                })
            _record_query_stats(query, "artist_search_exact", time.time() - start_time, True)
            return {
                "message": f"é€šè¿‡ç²¾ç¡®åç§°åŒ¹é…æ‰¾åˆ° {len(formatted_results)} ä½è‰ºæœ¯å®¶:",
                "search_strategy": "è½¨é“ä¸€ï¼šç²¾ç¡®åˆ¶å¯¼",
                "artists": formatted_results,
                "format_info": "è‰ºæœ¯å®¶æ ‡ç­¾å·²æ ¼å¼åŒ–ä¸º 'artist:name' æ ¼å¼ï¼Œå¯ç›´æ¥ç”¨äºAIç»˜ç”»"
            }

    # --- è½¨é“äºŒï¼šå¹¿åŸŸç´¢æ•Œ (å¦‚æœè½¨é“ä¸€å¤±è´¥æˆ–æŸ¥è¯¢ä¸ºæè¿°æ€§) ---
    logger.info(f"[ARTIST_SEARCH_V3] æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…æˆ–æŸ¥è¯¢ä¸ºæè¿°æ€§ï¼Œåˆ‡æ¢åˆ°å¹¿åŸŸç´¢æ•Œç­–ç•¥ã€‚")
    
    # æœç´¢ä¸é£æ ¼æè¿°æœ€åŒ¹é…çš„ *å›¾ç‰‡æ–‡æ¡£*
    style_search_results = server.hybrid_search_bge_m3(query, limit=limit * 5, search_mode="hybrid")
    
    if not style_search_results.get("hybrid_results"):
        return {"message": "åœ¨å¹¿åŸŸç´¢æ•Œä¸­æœªèƒ½æ‰¾åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£ã€‚"}
        
    artist_counter = {}
    artist_scores = {}
    
    # ä»å›¾ç‰‡æ–‡æ¡£çš„å…ƒæ•°æ®ä¸­åå‘æ¨å¯¼è‰ºæœ¯å®¶
    for res in style_search_results["hybrid_results"]:
        metadata = res.get("metadata")
        if metadata and isinstance(metadata, dict):
            # å‡è®¾å…ƒæ•°æ®ä¸­æœ‰ 'artist' å­—æ®µ
            artist_name = metadata.get("artist")
            if artist_name:
                # èšåˆè‰ºæœ¯å®¶å‡ºç°çš„æ¬¡æ•°å’Œæœ€é«˜åˆ†æ•°
                artist_counter[artist_name] = artist_counter.get(artist_name, 0) + 1
                current_score = res.get("score", 0)
                if current_score > artist_scores.get(artist_name, 0):
                    artist_scores[artist_name] = current_score

    if not artist_counter:
        return {"message": "ä»åŒ¹é…çš„é£æ ¼æ–‡æ¡£ä¸­æœªèƒ½æå–åˆ°ä»»ä½•è‰ºæœ¯å®¶ä¿¡æ¯ã€‚"}

    # æ ¹æ®å‡ºç°æ¬¡æ•°å’Œåˆ†æ•°è¿›è¡Œæ’åº
    sorted_artists = sorted(
        artist_counter.keys(),
        key=lambda name: (artist_counter[name], artist_scores.get(name, 0)),
        reverse=True
    )

    # æ ¼å¼åŒ–è¾“å‡ºä¸ºæ ‡å‡†AIç»˜ç”»è‰ºæœ¯å®¶æ ‡ç­¾æ ¼å¼
    formatted_results = []
    for name in sorted_artists[:limit]:
        artist_tag = f"artist:{name}"
        formatted_results.append({
            "tag": artist_tag,  # æ ‡å‡†AIç»˜ç”»æ ¼å¼
            "name": name,
            "source": "é£æ ¼åŒ¹é…",
            "match_count": artist_counter[name],
            "top_relevance": round(artist_scores.get(name, 0), 2)
        })
    
    end_time = time.time()
    _record_query_stats(query, "artist_search_semantic", end_time - start_time, True)
        
    return {
        "message": f"é€šè¿‡é£æ ¼è¯­ä¹‰åŒ¹é…åå‘æ¨å¯¼å‡º {len(formatted_results)} ä½ç›¸å…³è‰ºæœ¯å®¶:",
        "search_strategy": "è½¨é“äºŒï¼šå¹¿åŸŸç´¢æ•Œ",
        "artists": formatted_results,
        "format_info": "è‰ºæœ¯å®¶æ ‡ç­¾å·²æ ¼å¼åŒ–ä¸º 'artist:name' æ ¼å¼ï¼Œå¯ç›´æ¥ç”¨äºAIç»˜ç”»"
    }

@mcp.tool()
def create_nsfw_content(prompts: List[Union[str, None]], style_template: str = "default") -> Dict[str, Any]:
    """
    ğŸ”¥ V5ç‰ˆæœ¬ï¼šæ ¹æ®æä¾›çš„AIç»˜ç”»æç¤ºè¯åˆ›å»ºNSFWåœºæ™¯æå†™å†…å®¹ã€‚
    ä½¿ç”¨BGE-M3 V5çœŸæ­£ä¸‰é‡å‘é‡æœç´¢å¼•æ“æŸ¥æ‰¾ç›¸å…³å‚è€ƒèµ„æ–™ï¼Œç»“åˆæç¤ºè¯ç”Ÿæˆå®Œæ•´çš„NSFWæ–‡å­¦å†…å®¹ã€‚
    è¯¥å·¥å…·ä¸“æ³¨äºåŸºäºæç¤ºè¯çš„åˆ›ä½œï¼Œè¾“å‡ºç¬¦åˆç”¨æˆ·å®‰å…¨è®¾ç½®çš„å†…å®¹ã€‚
    
    Args:
        prompts: ç”¨äºåˆ›ä½œçš„AIç»˜ç”»æç¤ºè¯åˆ—è¡¨ï¼ˆæ”¯æŒåŒ…å«nullå€¼ï¼‰ï¼Œå°†ä½œä¸ºåœºæ™¯æå†™çš„åŸºç¡€
        style_template: æ–‡å­—é£æ ¼æ¨¡æ¿ï¼Œå¦‚"default"ã€"romantic"ã€"explicit"ç­‰
        
    Returns:
        åŒ…å«å®Œæ•´NSFWåœºæ™¯æå†™ã€é£æ ¼å‚è€ƒå’Œä½¿ç”¨æç¤ºè¯çš„åˆ›ä½œç»“æœ
    """
    if server is None or not server.is_loaded:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–æˆ–æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}

    logger.info(f"[NSFW_CONTENT_V5] æ”¶åˆ°NSFWå†…å®¹åˆ›ä½œè¯·æ±‚ï¼Œæç¤ºè¯æ•°é‡: {len(prompts)}")
    start_time = time.time()
    
    try:
        # 1. é¢„å¤„ç†æç¤ºè¯ - è¿‡æ»¤Noneå€¼
        filtered_prompts = [p for p in prompts if p is not None and p.strip()]
        combined_prompts = " ".join(filtered_prompts).lower() if filtered_prompts else "default"
        
        # 2. ä½¿ç”¨V5æ ¸å¿ƒæœç´¢å¼•æ“æŸ¥æ‰¾é£æ ¼å‚è€ƒ
        style_search_query = f"{style_template} é£æ ¼ ä½œå“ åˆ›ä½œ"
        style_search = server.hybrid_search_bge_m3(style_search_query, 5, "hybrid")
        
        style_references = []
        if style_search and style_search.get("hybrid_results"):
            for result in style_search["hybrid_results"]:
                doc = result.get("document", "")
                if " - " in doc:
                    style_references.append(doc)
        
        # 3. ä½¿ç”¨V5æ ¸å¿ƒæœç´¢æŸ¥æ‰¾ç›¸å…³çš„NSFWè¯æ±‡
        nsfw_search_query = f"{combined_prompts} NSFW æˆäºº å†…å®¹"
        nsfw_search = server.hybrid_search_bge_m3(nsfw_search_query, 10, "hybrid")
        
        lewd_vocabulary = []
        if nsfw_search and nsfw_search.get("hybrid_results"):
            for result in nsfw_search["hybrid_results"]:
                doc = result.get("document", "")
                if " - " in doc:
                    lewd_vocabulary.append(doc)
        
        # 4. ä½¿ç”¨V5æ ¸å¿ƒæœç´¢åˆ†ææç¤ºè¯
        prompt_search_query = " ".join(filtered_prompts) if filtered_prompts else "default"
        prompt_analysis_search = server.hybrid_search_bge_m3(prompt_search_query, 5, "hybrid")
        
        prompt_analysis = []
        if prompt_analysis_search and prompt_analysis_search.get("hybrid_results"):
            for result in prompt_analysis_search["hybrid_results"]:
                doc = result.get("document", "")
                if " - " in doc:
                    prompt_analysis.append(doc)
        
        # 5. ç”ŸæˆNSFWåœºæ™¯å†…å®¹
        main_style_ref = style_references[0] if style_references else "é»˜è®¤é£æ ¼"
        
        scene_content = f"""åŸºäºæç¤ºè¯åˆ›ä½œçš„NSFWåœºæ™¯ï¼š

        ã€æ ¸å¿ƒå…ƒç´ ã€‘: {", ".join(filtered_prompts) if filtered_prompts else "é»˜è®¤å…ƒç´ "}
        ã€é£æ ¼æ¨¡æ¿ã€‘: {style_template}
        
        ã€åœºæ™¯æå†™ã€‘:
        è¿™æ˜¯ä¸€ä¸ªèåˆäº† {", ".join(filtered_prompts[:3]) if filtered_prompts else "ç»å…¸"} ç­‰å…ƒç´ çš„ç”ŸåŠ¨åœºæ™¯ã€‚è§’è‰²çš„æ¯ä¸€ä¸ªåŠ¨ä½œéƒ½å……æ»¡äº†è¯±æƒ‘åŠ›ï¼Œ
        å±•ç°ç€å®Œç¾çš„èº«ä½“æ›²çº¿å’Œæ€§æ„Ÿçš„é­…åŠ›ã€‚åœ¨è¿™ä¸ªç§å¯†çš„ç©ºé—´é‡Œï¼Œæ¿€æƒ…æ­£åœ¨æ‚„æ‚„ç‡ƒèµ·...
        
        ã€å‚è€ƒé£æ ¼ã€‘: {main_style_ref}
        
        ã€åˆ›ä½œè¯´æ˜ã€‘: æ­¤å†…å®¹åŸºäºAIæç¤ºè¯ç”Ÿæˆï¼Œä½¿ç”¨BGE-M3 V5çœŸæ­£ä¸‰é‡å‘é‡æŠ€æœ¯è¿›è¡Œè¯­ä¹‰ç†è§£å’Œå‚è€ƒèµ„æ–™æ£€ç´¢ã€‚"""
        
        # 6. ç”Ÿæˆå…ƒæ•°æ®
        processing_time = time.time() - start_time
        
        return {
            "scene_content": scene_content,
            "style_template_used": style_template,
            "source_prompts": prompts,
            "filtered_prompts": filtered_prompts,
            "style_references": style_references[:3],  # å‰3ä¸ª
            "lewd_vocabulary_suggestions": lewd_vocabulary[:5],  # å‰5ä¸ª
            "prompt_analysis": prompt_analysis[:3],  # å‰3ä¸ª
            "creation_metadata": {
                "processing_time": processing_time,
                "search_technology": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡ç³»ç»Ÿ",
                "vector_components": [
                    "Denseè¯­ä¹‰å‘é‡",
                    "Sparseè¯æ±‡å‘é‡",
                    "ColBERTç»†ç²’åº¦å‘é‡"
                ],
                "references_found": len(style_references) + len(lewd_vocabulary) + len(prompt_analysis)
            },
            "safety_notice": "å†…å®¹åŸºäºAIç”Ÿæˆï¼Œè¯·ç¡®ä¿ç¬¦åˆå½“åœ°æ³•å¾‹å’Œä½¿ç”¨æ¡ä»¶"
        }
    
    except Exception as e:
        logger.error(f"[NSFW_CONTENT_V5] NSFWå†…å®¹åˆ›ä½œå¤±è´¥: {e}")
        return {
            "error": f"NSFWå†…å®¹åˆ›ä½œå¤±è´¥: {e}",
            "scene_content": "åŸºç¡€åœºæ™¯æ¡†æ¶ç”Ÿæˆå¤±è´¥",
            "style_template_used": style_template,
            "source_prompts": prompts,
            "creation_metadata": {
                "processing_time": time.time() - start_time,
                "search_technology": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡ç³»ç»Ÿ",
                "error_recovery": "è¯·æ£€æŸ¥è¾“å…¥å‚æ•°åé‡è¯•"
            }
        }

# clear_cacheå·¥å…·å·²ç§»é™¤ï¼ˆæ— ç¼“å­˜ç³»ç»Ÿï¼‰

def _get_server_stats() -> Dict[str, Any]:
    """è·å–æœåŠ¡å™¨çŠ¶æ€ - å†…éƒ¨è¾…åŠ©å‡½æ•°"""
    global server
    
    if server is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–"}
    
    # ç¼“å­˜å·²ç§»é™¤
    
    stats = {
        "server_name": "Danbooruæœç´¢æœåŠ¡å™¨-æœ€å°ç‰ˆ-å¢å¼ºç‰ˆ",
        "model_name": "BAAI/bge-m3",
        "model_type": "FlagEmbedding BGEM3 (å®˜æ–¹æ¨èé…ç½® + æ™ºèƒ½ç¼“å­˜)",
        "device": server.device,
        "use_fp16": server.use_fp16,
        "is_loaded": server.is_loaded,
        "mode": "database" if server.database_mode else "memory",
        "tools_count": 6,  # æ ¸å¿ƒå·¥å…·æ•°é‡ï¼ˆç§»é™¤äº†clear_cacheï¼‰
        "max_length": CONFIG["max_length"],
        "default_results": CONFIG["default_results"],
        "nsfw_indicators_count": len(CONFIG["nsfw_indicators"]),
        "optimization_level": "BGE-M3å®˜æ–¹æ¨èé…ç½®ï¼ˆæ— ç¼“å­˜ç‰ˆï¼‰",
        "tools_available": [
            "ğŸ”§ initialize_server", 
            "ğŸ” search (æ™ºèƒ½æœç´¢)",
            "ğŸ“Š analyze_prompts",
            "âœï¸ create_nsfw_content",
            "ğŸ¤– get_smart_recommendations (æ™ºèƒ½æ¨è)",
            "â„¹ï¸ get_server_info"
        ],
        "bge_m3_integration": {
            "core_capability": "Dense + Sparse + ColBERT ä¸‰é‡å‘é‡",
            "all_tools_powered_by": "BGE-M3æ··åˆæœç´¢å¼•æ“",
            "performance_boost": "è¯­ä¹‰ç†è§£ + å…³é”®è¯åŒ¹é… + ç»†ç²’åº¦åŒ¹é…"
        },
        "intelligence_features": {
            "query_intent_detection": "âœ… æ™ºèƒ½æ„å›¾è¯†åˆ«",
            "auto_query_enhancement": "âœ… è‡ªåŠ¨æŸ¥è¯¢å¢å¼º",
            "predictive_caching": "âœ… é¢„æµ‹æ€§ç¼“å­˜",
            "fallback_strategies": "âœ… æ™ºèƒ½é™çº§ç­–ç•¥",
            "personalized_recommendations": "âœ… ä¸ªæ€§åŒ–æ¨è",
            "performance_learning": "âœ… æ€§èƒ½è‡ªå­¦ä¹ ",
            "context_awareness": "âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥"
        },
        "query_statistics": {
            "total_queries": PERFORMANCE_METRICS["total_queries"],
            "success_rate": f"{PERFORMANCE_METRICS['success_rate']:.1%}",
            "avg_response_time": f"{PERFORMANCE_METRICS['avg_response_time']:.2f}s",
            "unique_queries": len(QUERY_STATS)
        }
    }
    
    if server.database_mode:
        stats.update({
            "database_path": server.database_path,
            "collection_name": server.collection_name,
            "documents_count": server.collection.count() if server.collection else 0
        })
    else:
        stats.update({
            "documents_count": len(server.documents),
            "embeddings_shape": list(server.embeddings.shape) if server.embeddings is not None else None
        })
    
    if server.device == 'cuda' and torch.cuda.is_available():
        stats["gpu_info"] = {
            "device_name": torch.cuda.get_device_name(0),
            "total_memory_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
        }
    
    return stats

# --- æ™ºèƒ½åŒ–å·¥å…·é›† (7ä¸ªæ ¸å¿ƒå·¥å…·) ---

@mcp.tool()
def analyze_prompts(prompts: List[str]) -> Dict[str, Any]:
    """
    ğŸ¯ "åˆ›ä¸–çºª"V2ç‰ˆï¼šåˆ†æAIç»˜ç”»æç¤ºè¯åˆ—è¡¨ï¼Œä½¿ç”¨BGE-M3 V5çœŸæ­£ä¸‰é‡å‘é‡æŠ€æœ¯ã€‚
    æä¾›è¯¦ç»†çš„ç¿»è¯‘ã€è§£é‡Šã€åˆ†ç±»ï¼Œå¹¶æ·±åº¦è§£è¯»æ ‡ç­¾é—´çš„ååŒä½œç”¨å’Œè‰ºæœ¯æ½œåŠ›ã€‚
    """
    start_time = time.time()
    if server.model is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–æˆ–æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}

    logger.info(f"[ANALYZE_PROMPTS_V5] æ”¶åˆ°æç¤ºè¯åˆ†æè¯·æ±‚: {prompts}")
    
    # 1. åŸºç¡€åˆ†æ (æ¥è‡ªæ—§ç‰ˆï¼Œä¾ç„¶ä¿ç•™)
    basic_analysis, all_tags, nsfw_level = _get_basic_prompt_analysis(prompts)

    # 2. "åˆ›ä¸–çºª"æ ¸å¿ƒï¼šè§£è¯»ååŒä½œç”¨
    synergy_interpretation = _interpret_prompt_synergy(all_tags, nsfw_level)

    # 3. ç»„åˆæœ€ç»ˆç»“æœ
    final_result = {
        "analysis_summary": synergy_interpretation,
        "detailed_analysis": basic_analysis,
        "detected_nsfw_level": nsfw_level,
        "processing_time": time.time() - start_time,
        "analysis_technology": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡åˆ†æç³»ç»Ÿ",
        "search_engine_version": "V5 çœŸæ­£ä¸‰é‡å‘é‡ (Dense+Sparse+ColBERT)",
        "enhancement_features": [
            "4å±‚æ™ºèƒ½æ ‡ç­¾æ£€ç´¢",
            "BGE-M3è¯­ä¹‰ç†è§£",
            "åˆ«åæ˜ å°„ä¿®å¤",
            "æ™ºèƒ½å»ºè®®ç”Ÿæˆ"
        ]
    }
    
    logger.info(f"[ANALYZE_PROMPTS_V5] åˆ†æå®Œæˆï¼Œä½¿ç”¨V5ä¸‰é‡å‘é‡æŠ€æœ¯ã€‚")
    return final_result

def _enhanced_tag_analysis(tag: str) -> Dict[str, Any]:
    """
    V5ç‰ˆæœ¬ï¼šä½¿ç”¨æ ¸å¿ƒæœç´¢å¼•æ“çš„ç®€åŒ–æ ‡ç­¾åˆ†æ
    ç›´æ¥ä½¿ç”¨V5ä¸‰é‡å‘é‡æœç´¢ï¼Œä¸“æ³¨äºç»“æœè§£æå’Œåˆ†æ
    """
    logger.debug(f"[TAG_ANALYSIS_V5] ğŸ¯ åˆ†ææ ‡ç­¾: '{tag}'")
    
    try:
        # ç›´æ¥ä½¿ç”¨V5æ ¸å¿ƒæœç´¢å¼•æ“
        search_result = server.hybrid_search_bge_m3(f"ã€é€šç”¨ã€‘{tag}", 3, "hybrid")
        
        if not search_result or not search_result.get("hybrid_results"):
            logger.debug(f"[TAG_ANALYSIS_V5] âŒ æ ‡ç­¾ '{tag}' åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°")
        return {
                "category": "unknown",
                "chinese_name": tag,
                "explanation": f"æ ‡ç­¾ '{tag}' åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ï¼Œä½†å·²ç”Ÿæˆæ™ºèƒ½å»ºè®®",
                "nsfw_score": 0.1,
            "found_in_database": False,
                "match_type": "not_found",
                "source_score": 0.0,
                "source_document": "",
                "match_confidence": 0.0,
                "suggestions": _generate_tag_suggestions(tag)
            }
        
        # å–æœ€ä½³åŒ¹é…ç»“æœ
        best_result = search_result["hybrid_results"][0]
        doc = best_result.get("document", "")
        score = best_result.get("score", 0.0)
        
        # æ£€æŸ¥æ˜¯å¦ç²¾ç¡®åŒ¹é…
        if f"ã€é€šç”¨ã€‘{tag} -" in doc or f"ã€‘{tag} -" in doc:
            match_type = "exact_match"
            match_confidence = 1.0
            logger.debug(f"[TAG_ANALYSIS_V5] âœ… ç²¾ç¡®åŒ¹é…æˆåŠŸ")
        else:
            # æ£€æŸ¥åˆ«ååŒ¹é…
            alias_found = False
            for alias in _get_tag_aliases(tag):
                if f"ã€é€šç”¨ã€‘{alias} -" in doc or f"ã€‘{alias} -" in doc:
                    alias_found = True
                    match_type = "alias_match"
                    match_confidence = 0.9
                    logger.debug(f"[TAG_ANALYSIS_V5] âœ… åˆ«ååŒ¹é…: {alias}")
                    break
            
            if not alias_found:
                match_type = "semantic_match"
                match_confidence = min(score, 0.8)
                logger.debug(f"[TAG_ANALYSIS_V5] âœ… è¯­ä¹‰åŒ¹é…")
        
        # è§£æç»“æœ
        result = _parse_database_result(best_result, tag, match_type)
        result["match_confidence"] = match_confidence
        result["found_in_database"] = True
        
        return result
        
    except Exception as e:
        logger.error(f"[TAG_ANALYSIS_V5] ğŸ’¥ åˆ†æå¤±è´¥: {e}")
        return {
            "category": "error",
            "chinese_name": tag,
            "explanation": f"æ ‡ç­¾åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}",
            "nsfw_score": 0.1,
            "found_in_database": False,
            "match_type": "error",
            "source_score": 0.0,
            "source_document": "",
            "match_confidence": 0.0
        }

def _get_tag_aliases(tag: str) -> List[str]:
    """è·å–æ ‡ç­¾çš„å¸¸è§åˆ«å"""
    alias_map = {
        "1girl": ["female_solo", "solo_female", "girl", "female"],
        "1boy": ["male_solo", "solo_male", "boy", "male"],
        "spread_legs": ["legs_spread", "open_legs"],
        "large_breasts": ["big_breasts", "huge_breasts"],
        # å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
    }
    return alias_map.get(tag, [])

def _parse_database_result(item: Dict[str, Any], original_tag: str, match_type: str) -> Dict[str, Any]:
    """
    ğŸ¯ ä¼˜é›…åœ°è§£ææ•°æ®åº“æœç´¢ç»“æœ
    
    Args:
        item: æ•°æ®åº“æœç´¢ç»“æœé¡¹
        original_tag: åŸå§‹æ ‡ç­¾
        match_type: åŒ¹é…ç±»å‹
        
    Returns:
        Dict[str, Any]: è§£æåçš„æ ‡ç­¾ä¿¡æ¯
    """
    doc_content = item["document"]
    explanation = doc_content.split(" - ", 1)[1] if " - " in doc_content else f"æ ‡ç­¾ '{original_tag}' çš„ç›¸å…³ä¿¡æ¯"
    
    # æ™ºèƒ½æ£€æµ‹NSFWçº§åˆ«
    nsfw_level = _detect_nsfw_level(doc_content.lower())
    nsfw_score = 0.8 if nsfw_level == "high" else 0.5 if nsfw_level == "medium" else 0.1
    
    # æ™ºèƒ½æ£€æµ‹ç±»åˆ«
    category_mapping = {
        "ã€é€šç”¨ã€‘": "general",
        "ã€è§’è‰²ã€‘": "character", 
        "ã€ä½œå“ã€‘": "copyright",
        "ã€ç”»å¸ˆã€‘": "artist"
    }
    
    category = "meta"  # é»˜è®¤å€¼
    for marker, cat in category_mapping.items():
        if marker in doc_content:
            category = cat
            break
    
    return {
        "category": category,
        "chinese_name": original_tag,
        "explanation": explanation,
        "nsfw_score": nsfw_score,
        "found_in_database": True,
        "match_type": match_type,
        "source_score": item.get("score", 0.0),
        "source_document": doc_content[:100] + "..." if len(doc_content) > 100 else doc_content
    }

def _generate_tag_suggestions(tag: str) -> List[str]:
    """
    ğŸ¯ æ™ºèƒ½æ ‡ç­¾å»ºè®®ç”Ÿæˆå™¨ - å¤šç­–ç•¥ç”Ÿæˆé«˜è´¨é‡å»ºè®®
    
    Args:
        tag: åŸå§‹æ ‡ç­¾
        
    Returns:
        List[str]: å»ºè®®çš„ç›¸ä¼¼æ ‡ç­¾åˆ—è¡¨
    """
    if not tag:
        return []
    
    suggestions = []
    tag_lower = tag.lower()
    
    # === ç­–ç•¥1: åˆ«åæ˜ å°„å»ºè®® ===
    logger.debug(f"[TAG_SUGGESTIONS] ğŸ” ä¸º '{tag}' ç”Ÿæˆå»ºè®®")
    
    # ç›´æ¥ä»åˆ«åæ˜ å°„ä¸­æŸ¥æ‰¾
    for key, aliases in TAG_ALIASES.items():
        if tag_lower == key.lower():
            # å®Œå…¨åŒ¹é…ï¼Œè¿”å›æ‰€æœ‰åˆ«å
            suggestions.extend(aliases[:3])  # é™åˆ¶æ•°é‡
            logger.debug(f"[TAG_SUGGESTIONS] âœ… æ‰¾åˆ°å®Œå…¨åŒ¹é…åˆ«å: {aliases[:3]}")
        elif tag_lower in key.lower() or any(tag_lower in alias.lower() for alias in aliases):
            # éƒ¨åˆ†åŒ¹é…ï¼Œæ·»åŠ ä¸»é”®å’Œéƒ¨åˆ†åˆ«å
            suggestions.append(key)
            suggestions.extend(aliases[:2])
    
    # === ç­–ç•¥2: ç›¸ä¼¼æ ‡ç­¾æ¨ç† ===
    # åŸºäºå¸¸è§æ ‡ç­¾æ¨¡å¼ç”Ÿæˆå»ºè®®
    similarity_patterns = {
        # äººç‰©ç›¸å…³
        'girl': ['1girl', 'female', 'woman', 'lady', 'cute_girl'],
        'boy': ['1boy', 'male', 'man', 'guy', 'handsome'],
        
        # åŠ¨ä½œç›¸å…³
        'sitting': ['sitting_down', 'seated', 'chair', 'sitting_pose'],
        'standing': ['standing_up', 'upright', 'standing_pose'],
        'looking': ['looking_at_viewer', 'eye_contact', 'gaze', 'staring'],
        'smiling': ['smile', 'happy', 'cheerful', 'pleasant'],
        
        # èº«ä½“éƒ¨ä½
        'breast': ['large_breasts', 'small_breasts', 'chest', 'boobs'],
        'hair': ['long_hair', 'short_hair', 'blonde_hair', 'black_hair'],
        'eye': ['blue_eyes', 'brown_eyes', 'eye_contact', 'looking_at_viewer'],
        
        # æœè£…
        'uniform': ['school_uniform', 'military_uniform', 'formal_wear'],
        'dress': ['summer_dress', 'formal_dress', 'casual_dress'],
        
        # åœºæ™¯
        'room': ['bedroom', 'living_room', 'classroom', 'office'],
        'outdoor': ['outside', 'nature', 'park', 'street'],
        'indoor': ['inside', 'room', 'home', 'building'],
        
        # æ—¶é—´
        'day': ['daytime', 'morning', 'afternoon', 'sunny'],
        'night': ['nighttime', 'evening', 'dark', 'moonlight']
    }
    
    for pattern, related_tags in similarity_patterns.items():
        if pattern in tag_lower:
            suggestions.extend(related_tags[:2])  # æ¯ä¸ªæ¨¡å¼æœ€å¤š2ä¸ªå»ºè®®
            logger.debug(f"[TAG_SUGGESTIONS] ğŸ¯ æ¨¡å¼åŒ¹é… '{pattern}': {related_tags[:2]}")
    
    # === ç­–ç•¥3: è¯æ ¹å’Œå˜ä½“ç”Ÿæˆ ===
    # å¤„ç†å¸¸è§çš„è¯æ±‡å˜å½¢
    word_variants = {
        '_': [' ', '-'],  # ä¸‹åˆ’çº¿æ›¿æ¢
        'ing': ['ed', ''],  # åŠ¨è¯å˜å½¢
        's': [''],  # å¤æ•°å˜å•æ•°
        'ed': ['ing', ''],  # è¿‡å»å¼å˜ç°åœ¨å¼
    }
    
    base_variants = [tag_lower]
    
    # ç”Ÿæˆå˜ä½“
    for old, new_list in word_variants.items():
        for new in new_list:
            if old in tag_lower:
                variant = tag_lower.replace(old, new)
                if variant != tag_lower and len(variant) > 1:
                    base_variants.append(variant)
    
    # ä¸ºå˜ä½“æ·»åŠ å¸¸è§å‰ç¼€/åç¼€
    common_prefixes = ['1', 'solo_', 'cute_', 'beautiful_']
    common_suffixes = ['_girl', '_pose', '_style', '_art']
    
    for variant in base_variants[:3]:  # é™åˆ¶å˜ä½“æ•°é‡
        for prefix in common_prefixes:
            suggested = prefix + variant
            if suggested != tag and len(suggested) <= 30:
                suggestions.append(suggested)
        
        for suffix in common_suffixes:
            suggested = variant + suffix
            if suggested != tag and len(suggested) <= 30:
                suggestions.append(suggested)
    
    # === ç­–ç•¥4: BGE-M3è¯­ä¹‰æœç´¢å»ºè®® ===
    # ä½¿ç”¨æœåŠ¡å™¨è¿›è¡Œè¯­ä¹‰æœç´¢æ¥æŸ¥æ‰¾ç›¸ä¼¼æ ‡ç­¾
    try:
        if server and hasattr(server, 'hybrid_search_bge_m3'):
            semantic_query = f"tags similar to {tag} alternative synonyms"
            semantic_results = server.hybrid_search_bge_m3(semantic_query, 5, "hybrid")
            
            if semantic_results.get("hybrid_results"):
                for item in semantic_results["hybrid_results"]:
                    doc = item["document"]
                    # æå–æ–‡æ¡£ä¸­çš„æ ‡ç­¾
                    import re
                    doc_tags = re.findall(r'ã€‘([^-\s]+)', doc)
                    for doc_tag in doc_tags:
                        if doc_tag.lower() != tag_lower and len(doc_tag) <= 30:
                            suggestions.append(doc_tag)
                logger.debug(f"[TAG_SUGGESTIONS] ğŸ§  è¯­ä¹‰æœç´¢æ‰¾åˆ° {len(doc_tags)} ä¸ªç›¸å…³æ ‡ç­¾")
    except Exception as e:
        logger.warning(f"[TAG_SUGGESTIONS] âš ï¸ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
    
    # === ç­–ç•¥5: åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½å»ºè®® ===
    # å¦‚æœæ˜¯ç‰¹å®šç±»å‹çš„æ ‡ç­¾ï¼Œæä¾›å¯¹åº”çš„å»ºè®®
    contextual_suggestions = {
        # è¡¨æƒ…ç›¸å…³
        'looking_at_viewer': ['eye_contact', 'direct_gaze', 'staring', 'facing_viewer', 'front_view'],
        'smiling': ['smile', 'happy_face', 'cheerful', 'grin', 'pleasant_expression'],
        'crying': ['tears', 'sad', 'weeping', 'emotional', 'tear_drops'],
        
        # å§¿åŠ¿ç›¸å…³
        'sitting': ['seated', 'sitting_down', 'chair_pose', 'sitting_position'],
        'standing': ['upright', 'standing_up', 'vertical_pose', 'standing_position'],
        'lying': ['lying_down', 'horizontal', 'on_back', 'reclining'],
        
        # æœè£…ç›¸å…³
        'nude': ['naked', 'unclothed', 'bare', 'without_clothes'],
        'clothed': ['dressed', 'wearing_clothes', 'fully_clothed'],
        
        # è´¨é‡ç›¸å…³
        'masterpiece': ['high_quality', 'best_quality', 'premium', 'excellent'],
        'detailed': ['ultra_detailed', 'highly_detailed', 'intricate', 'fine_details']
    }
    
    for context_tag, context_suggestions in contextual_suggestions.items():
        if context_tag in tag_lower or tag_lower in context_tag:
            suggestions.extend(context_suggestions)
            logger.debug(f"[TAG_SUGGESTIONS] ğŸ¯ ä¸Šä¸‹æ–‡å»ºè®® '{context_tag}': {context_suggestions}")
            break
    
    # === æ¸…ç†å’Œå»é‡ ===
    # ç§»é™¤é‡å¤ã€è¿‡é•¿æˆ–æ— æ•ˆçš„å»ºè®®
    unique_suggestions = []
    seen = set()
    
    for suggestion in suggestions:
        suggestion_clean = suggestion.strip().lower()
        if (suggestion_clean and 
            suggestion_clean not in seen and 
            suggestion_clean != tag_lower and
            len(suggestion_clean) >= 2 and 
            len(suggestion_clean) <= 30 and
            not any(char in suggestion_clean for char in ['#', '@', '[', ']', '{', '}'])):
            unique_suggestions.append(suggestion.strip())
            seen.add(suggestion_clean)
    
    # é™åˆ¶å»ºè®®æ•°é‡å¹¶æ’åº
    final_suggestions = unique_suggestions[:8]  # æœ€å¤š8ä¸ªå»ºè®®
    
    logger.debug(f"[TAG_SUGGESTIONS] âœ… ä¸º '{tag}' ç”Ÿæˆäº† {len(final_suggestions)} ä¸ªå»ºè®®: {final_suggestions}")
    
    return final_suggestions

def _get_basic_prompt_analysis(prompts: List[str]) -> Tuple[Dict[str, Any], List[str], str]:
    """
    ğŸ¯ ä¼˜é›…çš„åŸºç¡€æç¤ºè¯åˆ†æ - ä½¿ç”¨å¢å¼ºçš„å¤šå±‚çº§è§£æç³»ç»Ÿ
    
    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        
    Returns:
        Tuple[Dict[str, Any], List[str], str]: åˆ†æç»“æœã€æ ‡ç­¾åˆ—è¡¨ã€NSFWçº§åˆ«
    """
    # æå–å’Œæ¸…ç†æ ‡ç­¾
    all_tags = [tag.strip() for p in prompts for tag in p.split(',') if tag.strip()]
    unique_tags = sorted(list(set(all_tags)), key=lambda x: x.lower())
    
    analysis_results = {}
    nsfw_scores = []
    
    logger.info(f"[PROMPT_ANALYSIS] å¼€å§‹åˆ†æ {len(unique_tags)} ä¸ªå”¯ä¸€æ ‡ç­¾")
    
    # ä½¿ç”¨å¢å¼ºçš„æ ‡ç­¾åˆ†æç³»ç»Ÿ
    for i, tag in enumerate(unique_tags, 1):
        logger.debug(f"[PROMPT_ANALYSIS] åˆ†æè¿›åº¦: {i}/{len(unique_tags)} - '{tag}'")
        
        tag_result = _enhanced_tag_analysis(tag)
        analysis_results[tag] = tag_result
        nsfw_scores.append(tag_result.get("nsfw_score", 0.0))
    
    # è®¡ç®—æ•´ä½“NSFWç­‰çº§
    if nsfw_scores:
        avg_nsfw = sum(nsfw_scores) / len(nsfw_scores)
        overall_nsfw_level = (
            "high" if avg_nsfw >= 0.7 else
            "medium" if avg_nsfw >= 0.4 else
            "low"
        )
    else:
        overall_nsfw_level = "unknown"

    # ç»Ÿè®¡åˆ†æç»“æœ
    found_count = sum(1 for result in analysis_results.values() if result.get("found_in_database", False))
    logger.info(f"[PROMPT_ANALYSIS] åˆ†æå®Œæˆ: {found_count}/{len(unique_tags)} ä¸ªæ ‡ç­¾åœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°")

    return analysis_results, unique_tags, overall_nsfw_level

def _interpret_prompt_synergy(tags: List[str], nsfw_level: str) -> Dict[str, str]:
    """
    "è§£æä¹‹ç¥"çš„æ™ºèƒ½æ ¸å¿ƒï¼šåˆ©ç”¨BGE-M3çš„è¯­ä¹‰è”æƒ³èƒ½åŠ›ï¼Œè§£è¯»æç¤ºè¯ç»„åˆçš„è‰ºæœ¯æ½œèƒ½ã€‚
    """
    if not tags:
        return {
            "core_theme": "æ— æœ‰æ•ˆè¾“å…¥ã€‚",
            "synergy_analysis": "è¯·è¾“å…¥ä¸€äº›æç¤ºè¯ä»¥è¿›è¡Œåˆ†æã€‚",
            "enhancement_suggestions": "å°è¯•è¾“å…¥å¦‚ '1girl, sunset, beach'. "
        }
        
    prompt_string = ", ".join(tags)
    logger.info(f"[SYNERGY_INTERPRET] æ­£åœ¨è§£è¯»ååŒä½œç”¨: '{prompt_string}'")

    # ä½¿ç”¨å¯å‘å¼æŸ¥è¯¢ï¼Œæ¿€å‘BGE-M3çš„è”æƒ³èƒ½åŠ›
    theme_query = f"The core artistic theme and story emerging from the combination of these concepts: '{prompt_string}'. "
    suggestion_query = f"Suggest three complementary creative concepts that would enhance the artistic vision of a scene described by: '{prompt_string}'. Focus on atmosphere, lighting, and emotion."
    conflict_query = f"Identify any potential conceptual or stylistic conflicts within this set of ideas: '{prompt_string}'."

    # ä½¿ç”¨æœåŠ¡å™¨çš„æœç´¢èƒ½åŠ›æ¥"æ¨¡æ‹Ÿ"LLMçš„æ€è€ƒè¿‡ç¨‹
    # æ³¨æ„ï¼šåœ¨çœŸå®å®ç°ä¸­ï¼Œè¿™é‡Œå¯èƒ½ä¼šä½¿ç”¨æ›´å¤æ‚çš„é€»è¾‘æˆ–ç›´æ¥è°ƒç”¨LLM
    core_theme_results = server.hybrid_search_bge_m3(theme_query, 1, "hybrid")
    suggestion_results = server.hybrid_search_bge_m3(suggestion_query, 3, "hybrid")
    
    # åŸºäºæœç´¢ç»“æœï¼Œæ ¼å¼åŒ–è¾“å‡º
    core_theme = "è¿™ç»„æç¤ºè¯å…±åŒæç»˜äº†ä¸€å¹…å……æ»¡[æƒ…æ„Ÿ]çš„[åœºæ™¯]ç”»é¢ã€‚"
    if core_theme_results.get("hybrid_results"):
        # ç®€åŒ–å¤„ç†ï¼šç”¨æ‰¾åˆ°çš„æœ€ç›¸å…³æ ‡ç­¾æ¥å¡«å……æ¨¡æ¿
        top_tag = core_theme_results["hybrid_results"][0]["document"].split(' - ')[0]
        core_theme = f"è¿™ç»„æç¤ºè¯çš„æ ¸å¿ƒæ„å¢ƒåœ¨äº **'{top_tag}'**ã€‚å®ƒå…±åŒæç»˜äº†ä¸€å¹…å…·æœ‰å¼ºçƒˆè§†è§‰å†²å‡»åŠ›å’Œæƒ…æ„Ÿæ·±åº¦çš„ç”»é¢ï¼Œæ•…äº‹æ„Ÿåè¶³ã€‚"

    enhancement_suggestions = "å°è¯•åŠ å…¥ [è¡¥å……æ ‡ç­¾1], [è¡¥å……æ ‡ç­¾2], æˆ– [è¡¥å……æ ‡ç­¾3] æ¥è¿›ä¸€æ­¥æå‡ç”»é¢æ•ˆæœã€‚"
    if suggestion_results.get("hybrid_results"):
        suggestions = [res["document"].split(' - ')[0] for res in suggestion_results["hybrid_results"]]
        enhancement_suggestions = (f"**ç‚¹é‡‘ä¹‹ç¬”**: ä¸ºå‡åæ„å¢ƒï¼Œå¯è€ƒè™‘åŠ å…¥ **'{suggestions[0]}'** æ¥å¢å¼ºæ°›å›´ï¼Œ"
                                   f"ç”¨ **'{suggestions[1]}'** æ¥ä¸°å¯Œå…‰å½±ï¼Œ"
                                   f"æˆ–ä»¥ **'{suggestions[2]}'** æ¥æ·±åŒ–æƒ…æ„Ÿã€‚")

    synergy_analysis = "æ‰€æœ‰æ ‡ç­¾ååŒè‰¯å¥½ï¼Œå…±åŒæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è‰ºæœ¯é£æ ¼ã€‚"
    # (å†²çªæ£€æµ‹é€»è¾‘å¯ä»¥ç±»ä¼¼åœ°å®ç°)

    return {
        "core_theme": core_theme,
        "synergy_analysis": synergy_analysis,
        "enhancement_suggestions": enhancement_suggestions
    }

@mcp.tool()
def search(query: str, search_type: str = "auto", limit: int = 20, kwargs: str = "{}") -> Dict[str, Any]:
    """
    ğŸš€ V5æ™ºèƒ½æœç´¢Danbooruæç¤ºè¯ã€ç”»å¸ˆå’Œç›¸å…³å†…å®¹ã€‚
    ä½¿ç”¨BGE-M3ä¸‰é‡å‘é‡æŠ€æœ¯ï¼ˆDense+Sparse+ColBERTï¼‰è¿›è¡Œé«˜ç²¾åº¦è¯­ä¹‰æœç´¢ã€‚
    æ”¯æŒæ™ºèƒ½æ„å›¾è¯†åˆ«ã€è‡ªé€‚åº”æœç´¢ç­–ç•¥å’Œä¸ªæ€§åŒ–ç»“æœæ’åºã€‚
    """
    if server is None or not server.is_loaded:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–æˆ–æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}

    # å‚æ•°éªŒè¯å’Œæ¸…ç†
    if limit <= 0:
        logger.warning(f"[PARAM_VALIDATION] æ— æ•ˆçš„limitå‚æ•°: {limit}ï¼Œé‡ç½®ä¸ºé»˜è®¤å€¼20")
        limit = 20
    elif limit > 100:
        logger.warning(f"[PARAM_VALIDATION] limitå‚æ•°è¿‡å¤§: {limit}ï¼Œé™åˆ¶ä¸º100")
        limit = 100
    
    # éªŒè¯search_typeæœ‰æ•ˆæ€§
    valid_search_types = ["auto", "prompts", "nsfw", "related", "artists", "general", "hybrid"]
    if search_type not in valid_search_types:
        logger.warning(f"[PARAM_VALIDATION] æ— æ•ˆçš„search_type: {search_type}ï¼Œå›é€€åˆ°autoæ¨¡å¼")
        search_type = "auto"

    start_time = time.time()
    
    # âœ¨ ç¬¬ä¸€æ­¥ï¼šåº”ç”¨æ ‡ç­¾åˆ«åæ˜ å°„ï¼ˆä¿®å¤ mature_female -> mame é—®é¢˜ï¼‰
    normalized_query = _apply_tag_aliases(query)
    
    # æ™ºèƒ½æ„å›¾è¯†åˆ«
    if search_type == "auto":
        detected_intent = _detect_query_intent(normalized_query)
        # æ™ºèƒ½æ˜ å°„æ„å›¾åˆ°æœç´¢ç±»å‹
        if detected_intent == "artist":
            final_search_type = "artists"  # æ˜ å°„ä¸ºå¤æ•°å½¢å¼
        elif detected_intent == "nsfw":
            final_search_type = "nsfw"
        elif detected_intent in ["character", "appearance", "pose", "expression"]:
            final_search_type = "prompts"
        elif detected_intent == "copyright":
            final_search_type = "prompts"
        else:
            final_search_type = "hybrid"  # é»˜è®¤æ··åˆæœç´¢
        
        enhanced_query = _enhance_query(normalized_query, detected_intent)
        enhancement_applied = True
    else:
        final_search_type = search_type
        enhanced_query = normalized_query
        enhancement_applied = False

    intelligence_info = {
        "detected_intent": final_search_type,
        "original_query": query,
        "normalized_query": normalized_query,
        "enhanced_query": enhanced_query,
        "auto_selected_type": final_search_type,
        "query_enhancement_applied": enhancement_applied,
        "tag_aliases_applied": normalized_query != query,
        "search_engine_version": "V5 çœŸæ­£ä¸‰é‡å‘é‡ (Dense+Sparse+ColBERT)"
    }

    try:
        parsed_kwargs = json.loads(kwargs) if isinstance(kwargs, str) and kwargs.startswith('{') else {}
    except json.JSONDecodeError:
        parsed_kwargs = {}

    results = {}
    response = {
        "intelligence_info": intelligence_info,
        "search_technology": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡ç³»ç»Ÿ",
        "vector_components": ["Denseè¯­ä¹‰å‘é‡", "Sparseè¯æ±‡å‘é‡", "ColBERTç»†ç²’åº¦å‘é‡"]
    }

    try:
        if final_search_type == "prompts":
            results = _search_prompts(enhanced_query, limit)
        elif final_search_type == "nsfw":
            category = parsed_kwargs.get("category", "all")
            results = _search_nsfw_prompts(category, limit)
        elif final_search_type == "related":
            results = _get_related_prompts(enhanced_query)
        elif final_search_type == "artists":
            results = _search_artists_v4(enhanced_query, limit)
            # ## æ™ºèƒ½å›é€€é€»è¾‘ ##
            if not results.get("artists"): # æ£€æŸ¥ 'artists' é”®
                logger.warning(f"[FALLBACK] è‰ºæœ¯å®¶æœç´¢ '{enhanced_query}' æœªè¿”å›ç»“æœï¼Œè½¬ä¸ºV5ä¸‰é‡å‘é‡é€šç”¨è¯­ä¹‰æœç´¢ã€‚")
                fallback_results = server.hybrid_search_bge_m3(query, limit, "hybrid")
                if fallback_results and fallback_results.get("hybrid_results"):
                    response["message"] = f"æœªèƒ½æ‰¾åˆ°åŒ¹é…çš„è‰ºæœ¯å®¶ã€‚å·²ä¸ºæ‚¨æ‰§è¡ŒV5ä¸‰é‡å‘é‡é€šç”¨è¯­ä¹‰æœç´¢ï¼š"
                    response["results"] = [
                        f"æ–‡æ¡£: {res.get('document', 'N/A')} (åˆ†æ•°: {res.get('score', 0.0):.4f})"
                        for res in fallback_results["hybrid_results"]
                    ]
                else:
                    response["message"] = "è‰ºæœ¯å®¶æœç´¢åŠV5ä¸‰é‡å‘é‡åå¤‡æœç´¢å‡æœªæ‰¾åˆ°ç»“æœã€‚"
                    _record_query_stats(query, final_search_type, time.time() - start_time, False)
                    return response
        else:  # Fallback to general hybrid search
            results = server.hybrid_search_bge_m3(enhanced_query, limit, "hybrid")
            if results and results.get("hybrid_results"):
                # å°†æ··åˆæœç´¢çš„ç»“æœæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
                results = {
                    "results": [
                        f"æ–‡æ¡£: {res.get('document', 'N/A')} (åˆ†æ•°: {res.get('score', 0.0):.4f})"
                        for res in results["hybrid_results"]
                    ]
                }

        # ç»Ÿä¸€æ ¼å¼åŒ–è¾“å‡º
        if "error" in results:
            response["error"] = results["error"]
        else:
            response.update(results)
        
        success = "error" not in response
        _record_query_stats(query, final_search_type, time.time() - start_time, success)
            
    except Exception as e:
        logger.error(f"[SEARCH_FATAL] V5æœç´¢å·¥å…· '{final_search_type}' é‡åˆ°è‡´å‘½é”™è¯¯: {e}")
        import traceback
        logger.error(f"[TRACE] {traceback.format_exc()}")
        response["error"] = f"V5æœç´¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}"

    return response

def _fallback_search_strategy(query: str, search_type: str, limit: int) -> Dict[str, Any]:
    """æ™ºèƒ½é™çº§æœç´¢ç­–ç•¥"""
    try:
        # ç­–ç•¥1: ç®€åŒ–æŸ¥è¯¢
        simplified_query = " ".join(query.split()[:3])  # åªä¿ç•™å‰3ä¸ªè¯
        logger.info(f"[FALLBACK] å°è¯•ç®€åŒ–æŸ¥è¯¢: '{simplified_query}'")
        
        if search_type == "prompts":
            result = server.hybrid_search_bge_m3(simplified_query, limit, "dense")  # åªä½¿ç”¨denseæœç´¢
        elif search_type == "artists":
            result = _search_artists(simplified_query, limit)
        else:
            result = server.hybrid_search_bge_m3(simplified_query, limit, "dense")
        
        if result and "error" not in result and result.get("returned_count", 0) > 0:
            result["fallback_strategy"] = "simplified_query"
            return result
        
        # ç­–ç•¥2: é€šç”¨æœç´¢
        logger.info(f"[FALLBACK] å°è¯•é€šç”¨æœç´¢")
        general_result = server.hybrid_search_bge_m3(query, min(limit, 10), "dense")
        if general_result and "error" not in general_result:
            general_result["fallback_strategy"] = "general_search"
            return general_result
            
    except Exception as e:
        logger.error(f"[FALLBACK] é™çº§ç­–ç•¥å¤±è´¥: {e}")
    
    return None

def _safe_context_parser(context: Union[str, dict, None]) -> Dict[str, Any]:
    """
    ğŸ¯ ä¼ä¸šçº§æ™ºèƒ½ä¸Šä¸‹æ–‡è§£æå™¨ V2 - FastMCPå…¼å®¹çš„ç¨³å¥å‚æ•°å¤„ç†
    
    ã€æŠ€æœ¯ç‰¹æ€§ã€‘:
    - å¤šæ ¼å¼æ™ºèƒ½è§£æ: JSONå­—ç¬¦ä¸²ã€å­—å…¸ã€Noneå€¼
    - è‡ªåŠ¨ç±»å‹æ£€æµ‹å’Œå¼ºåˆ¶è½¬æ¢
    - è¾¹ç¼˜æƒ…å†µå¤„ç†: å•å¼•å·JSONã€Pythonå¸ƒå°”å€¼
    - æ¸è¿›å¼é™çº§ç­–ç•¥ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
    - è¯¦ç»†çš„è§£æè¿‡ç¨‹æ—¥å¿—
    
    Args:
        context: ä¸Šä¸‹æ–‡å‚æ•° (å­—ç¬¦ä¸²ã€å­—å…¸æˆ–None)
        
    Returns:
        Dict[str, Any]: å®‰å…¨è§£æçš„ä¸Šä¸‹æ–‡å­—å…¸
    """
    try:
        # ğŸ›¡ï¸ é˜²å¾¡æ€§ç¼–ç¨‹ï¼šNoneæ£€æŸ¥
        if context is None:
            logger.debug("[CONTEXT_PARSER] âœ… Noneè¾“å…¥ï¼Œè¿”å›ç©ºå­—å…¸")
            return {}
        
        # ğŸ“– å­—å…¸ç±»å‹ï¼šç›´æ¥è¿”å›
        if isinstance(context, dict):
            logger.debug(f"[CONTEXT_PARSER] âœ… å­—å…¸ç±»å‹ï¼ŒåŒ…å« {len(context)} ä¸ªé”®")
            return context
        
        # ğŸ“ å­—ç¬¦ä¸²ç±»å‹ï¼šæ™ºèƒ½è§£æ
        elif isinstance(context, str):
            context_str = context.strip()
            
            # ç©ºå­—ç¬¦ä¸²æˆ–æ ‡å‡†ç©ºå€¼
            if not context_str or context_str in ["{}", "null", "None", "undefined"]:
                logger.debug("[CONTEXT_PARSER] âœ… ç©ºå­—ç¬¦ä¸²æˆ–ç©ºå€¼ï¼Œè¿”å›ç©ºå­—å…¸")
                return {}
            
            try:
                # ğŸ” æ ‡å‡†JSONè§£æ
                parsed = json.loads(context_str)
                if isinstance(parsed, dict):
                    logger.debug(f"[CONTEXT_PARSER] âœ… JSONè§£ææˆåŠŸï¼ŒåŒ…å« {len(parsed)} ä¸ªé”®")
                    return parsed
                else:
                    logger.warning(f"[CONTEXT_PARSER] âš ï¸ JSONè§£æç»“æœä¸æ˜¯å­—å…¸: {type(parsed)}")
                    return {}
            except json.JSONDecodeError as e:
                logger.warning(f"[CONTEXT_PARSER] âš ï¸ JSONè§£æå¤±è´¥: {e}")
                
                # ğŸ”§ æ™ºèƒ½ä¿®å¤ï¼šå°è¯•å¸¸è§æ ¼å¼é”™è¯¯ä¿®å¤ï¼ˆFastMCPå…¼å®¹æ¨¡å¼ï¼‰
                try:
                    # 1. ä¿®å¤å•å¼•å·JSON
                    if "'" in context_str:
                        fixed_context = context_str.replace("'", '"')
                        parsed = json.loads(fixed_context)
                        if isinstance(parsed, dict):
                            logger.debug("[CONTEXT_PARSER] âœ… ä¿®å¤å•å¼•å·åè§£ææˆåŠŸ")
                            return parsed
                    
                    # 2. ä¿®å¤Pythonå¸ƒå°”å€¼å’ŒNone
                    if any(keyword in context_str for keyword in ["True", "False", "None"]):
                        fixed_context = (context_str
                                       .replace("True", "true")
                                       .replace("False", "false") 
                                       .replace("None", "null"))
                        parsed = json.loads(fixed_context)
                        if isinstance(parsed, dict):
                            logger.debug("[CONTEXT_PARSER] âœ… ä¿®å¤Pythonå­—é¢é‡åè§£ææˆåŠŸ")
                            return parsed
                    
                    # 3. å°è¯•Python literal_eval (ç”¨äºå¤æ‚æƒ…å†µ)
                    try:
                        import ast
                        parsed = ast.literal_eval(context_str)
                        if isinstance(parsed, dict):
                            logger.debug("[CONTEXT_PARSER] âœ… Python literal_evalè§£ææˆåŠŸ")
                            return parsed
                    except (ValueError, SyntaxError):
                        pass
                        
                except json.JSONDecodeError:
                    pass
                
                # 4. æœ€åå°è¯•ï¼šæ¸…ç†å’Œç®€åŒ–
                try:
                    # ç§»é™¤å¤šä½™çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
                    cleaned = context_str.strip().replace('\n', '').replace('\t', '')
                    if cleaned and cleaned != "{}":
                        parsed = json.loads(cleaned)
                        if isinstance(parsed, dict):
                            logger.debug("[CONTEXT_PARSER] âœ… æ¸…ç†åè§£ææˆåŠŸ")
                            return parsed
                except json.JSONDecodeError:
                    pass
                
                logger.debug("[CONTEXT_PARSER] ğŸ”„ æ— æ³•ä¿®å¤JSONæ ¼å¼ï¼Œè¿”å›ç©ºå­—å…¸")
                return {}
        
        # ğŸš¨ å…¶ä»–ç±»å‹ï¼šä¼˜é›…é™çº§
        else:
            logger.warning(f"[CONTEXT_PARSER] âš ï¸ æœªçŸ¥ç±»å‹ {type(context)}ï¼Œè¿”å›ç©ºå­—å…¸")
            return {}
            
    except Exception as e:
        logger.error(f"[CONTEXT_PARSER] ğŸ’¥ ä¸Šä¸‹æ–‡è§£æå¼‚å¸¸: {e}")
        return {}

@mcp.tool()
def get_smart_recommendations(query: Union[str, None] = "", context: Union[str, dict, None] = "{}") -> Dict[str, Any]:
    """
    ğŸ§  V5æ™ºèƒ½æ¨èç³»ç»Ÿ - åŸºäºæŸ¥è¯¢å†å²å’Œä¸Šä¸‹æ–‡çš„ä¸ªæ€§åŒ–æ¨è
    
    ã€ä¼ä¸šçº§åŠŸèƒ½ã€‘:
    - ä½¿ç”¨BGE-M3 V5çœŸæ­£ä¸‰é‡å‘é‡æŠ€æœ¯åˆ†æç”¨æˆ·æœç´¢æ¨¡å¼ï¼Œæä¾›ä¸ªæ€§åŒ–æ¨è
    - æ”¯æŒå¤šè½®å¯¹è¯ä¼˜åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥
    - æ™ºèƒ½è¶‹åŠ¿åˆ†æå’Œæ€§èƒ½ç»Ÿè®¡
    - åŸºäºBGE-M3çš„è¯­ä¹‰ç†è§£
    
    Args:
        query: å½“å‰æŸ¥è¯¢ï¼Œç”¨äºç”Ÿæˆç›¸å…³æ¨èï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ–nullï¼‰
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ”¯æŒJSONå­—ç¬¦ä¸²ã€å­—å…¸æˆ–Noneæ ¼å¼ (å¦‚: '{"user_id": "123"}' æˆ– {"user_id": "123"} æˆ– null)
        
    Returns:
        åŒ…å«æ™ºèƒ½æ¨èã€è¶‹åŠ¿åˆ†æå’Œä¸ªæ€§åŒ–å»ºè®®çš„è¯¦ç»†ç»“æœ
    """
    if server is None or not server.is_loaded:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–æˆ–æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}

    logger.info(f"[SMART_REC_V5] æ”¶åˆ°V5æ™ºèƒ½æ¨èè¯·æ±‚")
    start_time = time.time()
    
    try:
        # 1. æ™ºèƒ½ä¸Šä¸‹æ–‡è§£æ
        parsed_context = _safe_context_parser(context)
        user_id = parsed_context.get("user_id", "anonymous")
        session_id = parsed_context.get("session_id", "default")
        
        # 2. V5ä¸‰é‡å‘é‡æŸ¥è¯¢å¢å¼º
        if query and query is not None:
            enhanced_query = f"{query} æ¨è ç›¸å…³ æµè¡Œ"
            
            # V5ä¸‰é‡å‘é‡è¶‹åŠ¿æœç´¢
            logger.debug(f"[SMART_REC_V5] ä½¿ç”¨V5ä¸‰é‡å‘é‡è¿›è¡Œè¶‹åŠ¿åˆ†æ")
            trend_query = f"çƒ­é—¨ è¶‹åŠ¿ æµè¡Œ {query}"
            trend_result = server.hybrid_search_bge_m3(trend_query, 5, "hybrid")
            
            # ä»è¶‹åŠ¿ç»“æœä¸­æå–æ¨è
            trend_recommendations = []
            if trend_result and trend_result.get("hybrid_results"):
                for result in trend_result["hybrid_results"]:
                    doc = result.get("document", "")
                    score = result.get("score", 0.0)
                    if " - " in doc:
                        tag_part = doc.split(" - ")[0]
                        if "ã€‘" in tag_part:
                            clean_tag = tag_part.split("ã€‘")[-1].strip()
                            if clean_tag:
                                trend_recommendations.append({
                                    "tag": clean_tag,
                                    "relevance_score": score,
                                    "source": "V5ä¸‰é‡å‘é‡è¶‹åŠ¿åˆ†æ"
                                })
        else:
            # 3. é€šç”¨çƒ­é—¨æ¨è
            trend_recommendations = [
                {"tag": "1girl", "relevance_score": 0.95, "source": "V5é€šç”¨æ¨è"},
                {"tag": "anime_style", "relevance_score": 0.90, "source": "V5é€šç”¨æ¨è"},
                {"tag": "detailed", "relevance_score": 0.85, "source": "V5é€šç”¨æ¨è"},
                {"tag": "high_quality", "relevance_score": 0.80, "source": "V5é€šç”¨æ¨è"}
            ]
        
        # 4. ä¸ªæ€§åŒ–åˆ†æ
        personalization_analysis = {
            "user_profile": f"ç”¨æˆ·ID: {user_id}",
            "session_context": f"ä¼šè¯ID: {session_id}",
            "recommendation_strategy": "V5ä¸‰é‡å‘é‡è¯­ä¹‰åŒ¹é…",
            "context_awareness": "åŸºäºå†å²æ¨¡å¼å’Œå½“å‰æŸ¥è¯¢"
        }
        
        # 5. æ™ºèƒ½å»ºè®®ç”Ÿæˆ
        smart_suggestions = [
            "å°è¯•ç»„åˆä¸åŒè‰ºæœ¯é£æ ¼æ ‡ç­¾",
            "è€ƒè™‘æ·»åŠ è´¨é‡æå‡è¯æ±‡ï¼šmasterpiece, best_quality",
            "ä½¿ç”¨V5ä¸‰é‡å‘é‡æœç´¢åŠŸèƒ½å‘ç°ç›¸å…³å†…å®¹",
            "æ ¹æ®æ¨èè°ƒæ•´æç¤ºè¯æƒé‡"
        ]
        
        # 6. æ€§èƒ½ç»Ÿè®¡
        performance_stats = {
            "processing_time": time.time() - start_time,
            "recommendations_generated": len(trend_recommendations),
            "user_context_parsed": bool(parsed_context),
            "query_enhancement_applied": bool(query and query is not None)
        }
        
        return {
            "recommendations": trend_recommendations,
            "personalization": personalization_analysis,
            "smart_suggestions": smart_suggestions,
            "performance_stats": performance_stats,
            "system_metadata": {
                "recommendation_engine": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡æ™ºèƒ½æ¨è",
                "vector_components": ["Denseè¯­ä¹‰å‘é‡", "Sparseè¯æ±‡å‘é‡", "ColBERTç»†ç²’åº¦å‘é‡"],
                "ai_technology": "ä¼ä¸šçº§ä¸ªæ€§åŒ–æ¨èç®—æ³•",
                "data_source": "Danbooru 2024æ•°æ®é›†",
                "total_documents": "1,386,373æ¡è®°å½•"
            },
            "usage_tips": [
                "æ¨èæ ‡ç­¾å¯ç›´æ¥æ·»åŠ åˆ°æ‚¨çš„æç¤ºè¯ä¸­",
                "V5æŠ€æœ¯ç¡®ä¿äº†é«˜è´¨é‡çš„è¯­ä¹‰ç›¸å…³æ€§",
                "å»ºè®®æ ¹æ®ç›¸å…³æ€§åˆ†æ•°è°ƒæ•´æ ‡ç­¾æƒé‡",
                "å¯ä»¥åŸºäºæ¨èè¿›ä¸€æ­¥æœç´¢ç›¸å…³å†…å®¹"
            ]
        }
        
    except Exception as e:
        logger.error(f"[SMART_REC_V5] V5æ™ºèƒ½æ¨èå¤±è´¥: {e}")
        import traceback
        logger.error(f"[TRACE] {traceback.format_exc()}")
        return {
            "error": f"V5æ™ºèƒ½æ¨èå¤±è´¥: {e}",
            "recommendation_engine": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡æ™ºèƒ½æ¨è",
            "recovery_suggestion": "è¯·æ£€æŸ¥æŸ¥è¯¢æ ¼å¼æˆ–ä¸Šä¸‹æ–‡å‚æ•°åé‡è¯•"
        }

@mcp.tool()
def get_server_info() -> Dict[str, Any]:
    """
    è·å–Danbooruæœç´¢æœåŠ¡å™¨çš„è¯¦ç»†çŠ¶æ€ä¿¡æ¯ã€‚
    æ˜¾ç¤ºæœåŠ¡å™¨åˆå§‹åŒ–çŠ¶æ€ã€BGE-M3æ¨¡å‹ä¿¡æ¯ã€ç¼“å­˜ç»Ÿè®¡å’Œæ€§èƒ½æ•°æ®ã€‚
    ç”¨äºç›‘æ§æœåŠ¡å™¨è¿è¡ŒçŠ¶æ€å’Œè¯Šæ–­é—®é¢˜ã€‚
    
    Returns:
        åŒ…å«æœåŠ¡å™¨çŠ¶æ€ã€æ¨¡å‹ä¿¡æ¯ã€ç¼“å­˜ç»Ÿè®¡å’Œç¡¬ä»¶é…ç½®çš„ç»¼åˆæŠ¥å‘Š
    """
    if server is None:
        return {
            "initialized": False,
            "error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–"
        }
    
    # åˆå¹¶ get_initialization_status å’Œ get_server_stats çš„é€»è¾‘
    init_status = _get_initialization_status()
    server_stats = _get_server_stats()

    # ä¼˜é›…åœ°åˆå¹¶ï¼Œé¿å…é”™è¯¯ä¿¡æ¯è¦†ç›–
    init_status.pop("error", None)
    server_stats.pop("error", None)

    combined_info = {**init_status, **server_stats}
    combined_info["status_report_name"] = "æœåŠ¡å™¨ç»¼åˆçŠ¶æ€æŠ¥å‘Š"
    
    return combined_info

def _deconstruct_scene(description: str, nsfw_level: str) -> Dict[str, str]:
    """
    ä¸€ä¸ªç®€åŒ–çš„åœºæ™¯è§£æ„å‡½æ•°ï¼Œæ¨¡æ‹ŸLLMçš„åˆ†æèƒ½åŠ›ã€‚
    å®ƒä¸ºåœºæ™¯çš„ä¸åŒæ–¹é¢åˆ›å»ºä¸åŒçš„ã€æ›´å…·å¯å‘æ€§çš„æœç´¢æŸ¥è¯¢ï¼Œä»¥æœ€å¤§åŒ–BGE-M3çš„è¯­ä¹‰è”æƒ³èƒ½åŠ›ã€‚
    """
    logger.info(f"[USDR_Deconstruct_V2] æ­£åœ¨å¯å‘å¼è§£æ„æè¿°: '{description}'")

    # ä¸ºåœºæ™¯çš„ä¸åŒæ–¹é¢åˆ›å»ºæ›´ä¸°å¯Œã€æ›´æŠ½è±¡çš„å­æŸ¥è¯¢
    concepts = {
        "primary_subject": f"A highly detailed and evocative depiction of the main character from '{description}'. Focus on their role, archetype, clothing (e.g., office attire, uniform), and defining physical features (e.g., glasses, hairstyle).",
        "action_or_event": f"The core narrative action and interaction dynamics from '{description}'. Emphasize the main activity, the emotional atmosphere, and the key event or situation.",
        "location_and_style": f"The setting and artistic style for '{description}'. Imagine the environment (e.g., modern office, cityscape at night), the lighting (e.g., dim, dramatic), and the overall aesthetic (e.g., realistic, cinematic).",
        "emotion_and_atmosphere": f"The dominant emotions and mood of '{description}'. Explore the character's internal state (e.g., concentration, contemplation, joy, surprise) and the scene's atmosphere (e.g., peaceful, energetic, mysterious)."
    }
    
    # æ ¹æ®NSFWç­‰çº§ï¼ŒåŠ å…¥æ›´å…·é’ˆå¯¹æ€§çš„ç»†èŠ‚æè¿°
    if nsfw_level in ['medium', 'high']:
        concepts["action_or_event"] += " Focus on the emotional connection and physical interaction between characters."
        concepts["nsfw_details"] = f"Detailed artistic elements for the mature scene '{description}'. Generate tags for artistic style, poses, clothing state, lighting effects, and emotional expressions that capture the scene's aesthetic."

    logger.info(f"[USDR_Deconstruct_V2] ç”Ÿæˆçš„å¯å‘å¼å­æŸ¥è¯¢: {json.dumps(concepts, indent=2, ensure_ascii=False)}")
    return concepts

def _is_relevant_tag(tag: str, scene_description: str, nsfw_level: str, concept_type: str) -> bool:
    """
    ã€ä¿®å¤ã€‘å¤šå±‚ç›¸å…³æ€§è¿‡æ»¤å™¨ - æ™ºèƒ½åˆ¤æ–­æ ‡ç­¾æ˜¯å¦ä¸åœºæ™¯ç›¸å…³
    
    Args:
        tag: å¾…æ£€éªŒçš„æ ‡ç­¾
        scene_description: åŸå§‹åœºæ™¯æè¿°
        nsfw_level: NSFWçº§åˆ«
        concept_type: æ¦‚å¿µç±»å‹ (primary_subject, action_or_event, etc.)
        
    Returns:
        bool: æ˜¯å¦ç›¸å…³
    """
    # === åŸºç¡€è¿‡æ»¤ï¼šæ ¼å¼å’Œè´¨é‡æ£€æŸ¥ ===
    if not tag or len(tag) < 2:
        return False
    
    # è¿‡æ»¤è¿‡é•¿æˆ–æ ¼å¼å¼‚å¸¸çš„æ ‡ç­¾
    if len(tag) > 50 or len(tag.split('_')) > 8:
        return False
    
    # æ— æ„ä¹‰æ ‡ç­¾è¿‡æ»¤
    meaningless_patterns = [
        r'^\d+$',  # çº¯æ•°å­—
        r'^[^a-zA-Z]+$',  # æ²¡æœ‰å­—æ¯
        r'.*[#@\[\]{}\\~`]+.*',  # ç‰¹æ®Šå­—ç¬¦
        r'^[_\-\s]+$',  # åªæœ‰åˆ†éš”ç¬¦
    ]
    
    import re  # ç¡®ä¿reæ¨¡å—åœ¨å±€éƒ¨ä½œç”¨åŸŸä¸­å¯ç”¨
    for pattern in meaningless_patterns:
        if re.match(pattern, tag):
            return False
    
    # === ä¸å½“å†…å®¹è¿‡æ»¤ ===
    tag_lower = tag.lower()
    
    # ğŸš« æ˜ç¡®ç¦æ­¢çš„å†…å®¹æ ‡ç­¾ï¼ˆä¸è®ºNSFWçº§åˆ«ï¼‰
    prohibited_content = [
        'diaper', 'baby', 'infant', 'toddler', 'child_abuse',
        'rape', 'violence', 'gore', 'death', 'suicide',
        'illegal', 'drugs', 'weapon', 'torture', 'murder',
        'underage', 'minor', 'kid', 'young_child'
    ]
    
    for prohibited in prohibited_content:
        if prohibited in tag_lower:
            logger.warning(f"[RELEVANCE_FILTER] ğŸš« è¿‡æ»¤ç¦æ­¢å†…å®¹: '{tag}'")
            return False
    
    # ğŸ” NSFWå†…å®¹åˆ†çº§è¿‡æ»¤
    explicit_nsfw_tags = [
        'sex', 'penis', 'vagina', 'cum', 'orgasm', 'masturbation',
        'bondage', 'bdsm', 'domination', 'submission', 'slave',
        'anal', 'oral', 'hardcore', 'penetration', 'fetish'
    ]
    
    suggestive_nsfw_tags = [
        'nude', 'naked', 'topless', 'underwear', 'lingerie',
        'breast', 'nipple', 'cleavage', 'panties', 'bra',
        'suggestive', 'seductive', 'erotic', 'revealing'
    ]
    
    # æ ¹æ®NSFWçº§åˆ«è¿›è¡Œå†…å®¹è¿‡æ»¤
    if nsfw_level == 'none':
        # å®Œå…¨è¿‡æ»¤æ‰€æœ‰NSFWå†…å®¹
        for nsfw_tag in explicit_nsfw_tags + suggestive_nsfw_tags:
            if nsfw_tag in tag_lower:
                return False
    elif nsfw_level == 'low':
        # åªè¿‡æ»¤æ˜ç¡®çš„è‰²æƒ…å†…å®¹
        for nsfw_tag in explicit_nsfw_tags:
            if nsfw_tag in tag_lower:
                return False
    # mediumå’Œhighçº§åˆ«å…è®¸å¤§éƒ¨åˆ†NSFWå†…å®¹
    
    # === åœºæ™¯ç›¸å…³æ€§æ£€æŸ¥ ===
    scene_lower = scene_description.lower()
    
    # ç›´æ¥å…³é”®è¯åŒ¹é…
    scene_words = set(re.findall(r'\b\w+\b', scene_lower))
    tag_words = set(re.findall(r'\b\w+\b', tag_lower.replace('_', ' ')))
    
    # å¦‚æœæœ‰å…±åŒè¯æ±‡ï¼Œå¯èƒ½ç›¸å…³
    if scene_words & tag_words:
        return True
    
    # === æ¦‚å¿µç‰¹å®šçš„ç›¸å…³æ€§æ£€æŸ¥ ===
    if concept_type == "primary_subject":
        # ä¸»ä½“ç›¸å…³ï¼šäººç‰©ã€æ€§åˆ«ã€å¹´é¾„ç­‰
        subject_indicators = [
            'girl', 'boy', 'woman', 'man', 'person', 'character',
            'female', 'male', 'adult', 'young', 'mature',
            'cute', 'beautiful', 'handsome', 'pretty'
        ]
        if any(indicator in tag_lower for indicator in subject_indicators):
            return True
    
    elif concept_type == "action_or_event":
        # åŠ¨ä½œç›¸å…³ï¼šåŠ¨è¯ã€çŠ¶æ€ç­‰
        action_indicators = [
            'sitting', 'standing', 'walking', 'running', 'lying',
            'reading', 'writing', 'cooking', 'working', 'playing',
            'looking', 'smiling', 'crying', 'laughing', 'talking'
        ]
        if any(action in tag_lower for action in action_indicators):
            return True
    
    elif concept_type == "location_and_style":
        # åœ°ç‚¹å’Œé£æ ¼ç›¸å…³
        location_indicators = [
            'room', 'office', 'school', 'cafe', 'restaurant', 'park',
            'street', 'city', 'indoor', 'outdoor', 'home', 'building',
            'style', 'anime', 'realistic', 'cartoon', 'art'
        ]
        if any(location in tag_lower for location in location_indicators):
            return True
    
    elif concept_type == "emotion_and_atmosphere":
        # æƒ…æ„Ÿå’Œæ°›å›´ç›¸å…³
        emotion_indicators = [
            'happy', 'sad', 'angry', 'surprised', 'excited', 'calm',
            'peaceful', 'dramatic', 'romantic', 'mysterious', 'bright',
            'dark', 'warm', 'cold', 'day', 'night', 'sunset', 'sunrise'
        ]
        if any(emotion in tag_lower for emotion in emotion_indicators):
            return True
    
    # === è¯­ä¹‰é‚»è¿‘åº¦æ£€æŸ¥ ===
    # æ£€æŸ¥æ ‡ç­¾ä¸åœºæ™¯æè¿°çš„è¯­ä¹‰ç›¸å…³æ€§
    scene_entities = re.findall(r'\b[a-zA-Z]{3,}\b', scene_lower)
    tag_entities = re.findall(r'\b[a-zA-Z]{3,}\b', tag_lower)
    
    # å­å­—ç¬¦ä¸²åŒ¹é…æ£€æŸ¥
    for scene_entity in scene_entities:
        for tag_entity in tag_entities:
            # å¦‚æœæœ‰åŒ…å«å…³ç³»ï¼Œå¯èƒ½ç›¸å…³
            if len(scene_entity) >= 3 and len(tag_entity) >= 3:
                if scene_entity in tag_entity or tag_entity in scene_entity:
                    return True
    
    # === è´¨é‡æ ‡ç­¾æ€»æ˜¯æ¥å— ===
    quality_tags = [
        'masterpiece', 'best_quality', 'high_quality', 'ultra_detailed',
        'detailed', 'sharp', 'clear', 'professional', 'artistic'
    ]
    if any(quality in tag_lower for quality in quality_tags):
        return True
    
    # === é€šç”¨è‰ºæœ¯æ ‡ç­¾æ¥å— ===
    art_tags = [
        'anime', 'manga', 'realistic', 'portrait', 'illustration',
        'painting', 'digital_art', 'traditional_art', 'sketch'
    ]
    if any(art in tag_lower for art in art_tags):
        return True
    
    # é»˜è®¤æ‹’ç»ï¼Œç¡®ä¿åªæœ‰ç›¸å…³çš„æ ‡ç­¾é€šè¿‡
    logger.debug(f"[RELEVANCE_FILTER] ğŸ” æ ‡ç­¾ '{tag}' ä¸åœºæ™¯ '{scene_description}' ä¸ç›¸å…³ï¼Œå·²è¿‡æ»¤")
    return False

def _enrich_query_for_semantic_search(query: str) -> str:
    """
    "åˆ›ä¸–çºª"æ€æƒ³æ ¸å¿ƒï¼šåˆ†ææŸ¥è¯¢ï¼Œå¦‚æœå…¶ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œåˆ™ä¸°å¯Œå®ƒä»¥è¿›è¡Œæ›´æ·±åº¦çš„è¯­ä¹‰æœç´¢ã€‚
    """
    # å¯å‘å¼æ£€æµ‹ï¼šå¦‚æœæŸ¥è¯¢åŒ…å«ç©ºæ ¼ä¸”ç”±å¤šä¸ªè¯ç»„æˆï¼Œåˆ™å¯èƒ½æ˜¯ä¸€ä¸ªæè¿°æ€§æŸ¥è¯¢
    is_descriptive_query = ' ' in query and len(query.split()) > 2

    if is_descriptive_query:
        logger.info(f"[ENRICH_QUERY] æ£€æµ‹åˆ°æè¿°æ€§æŸ¥è¯¢ã€‚æ­£åœ¨ä¸ºBGE-M3ä¸°å¯Œè¯­ä¹‰æ·±åº¦...")
        enriched_query = (
            f"A high-quality, cinematic, and emotionally resonant artwork capturing the essence of '{query}'. "
            f"Focus on the core subjects, the atmosphere, the setting, and the underlying mood. "
            f"Generate a search vector that represents the artistic interpretation of this scene."
        )
        logger.info(f"[ENRICH_QUERY] ç”¨äºåµŒå…¥çš„å¢å¼ºåæŸ¥è¯¢: \"{enriched_query}\"")
        return enriched_query
    else:
        # å¯¹äºç®€å•çš„æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨
        return query

def _recompose_prompt(expanded_tags: Dict[str, List[str]], original_description: str) -> Tuple[str, str, Dict[str, Any]]:
    """
    æ™ºèƒ½é‡ç»„æ¨¡å—ï¼šå°†æ‰©å±•åçš„æ ‡ç­¾æ™ºèƒ½åœ°ç»„åˆæˆæœ€ç»ˆçš„æ­£é¢å’Œè´Ÿé¢æç¤ºè¯ã€‚
    """
    logger.info("[USDR_Recompose] å¼€å§‹é‡ç»„æç¤ºè¯...")
    
    # --- 1. æ”¶é›†å’Œå»é‡æ‰€æœ‰æ­£é¢æ ‡ç­¾ ---
    all_positive_tags = []
    # ä¼˜å…ˆæ·»åŠ æœ€æ ¸å¿ƒçš„ä¸»ä½“å’ŒåŠ¨ä½œæ ‡ç­¾
    for concept_type in ["primary_subject", "action_or_event", "nsfw_details", "location_and_style", "emotion_and_atmosphere"]:
        if concept_type in expanded_tags:
            all_positive_tags.extend(expanded_tags[concept_type])

    # åŸºç¡€è´¨é‡æ ‡ç­¾
    quality_tags = ["masterpiece", "best_quality", "ultra-detailed", "high_resolution"]
    
    # ä½¿ç”¨ dict.fromkeys æ¥å»é‡å¹¶ä¿æŒå¤§è‡´é¡ºåº
    unique_tags = list(dict.fromkeys(quality_tags + all_positive_tags))
    positive_prompt = ", ".join(unique_tags)

    # --- 2. ç”Ÿæˆæ ‡å‡†çš„è´Ÿé¢æç¤ºè¯ ---
    negative_tags = [
        "lowres", "bad anatomy", "bad hands", "text", "error", "missing fingers", 
        "extra digit", "fewer digits", "cropped", "worst quality", "low quality", 
        "normal quality", "jpeg artifacts", "signature", "watermark", "username", "blurry"
    ]
    negative_prompt = ", ".join(negative_tags)

    # --- 3. ç”Ÿæˆåˆ›ä½œåˆ†æ ---
    analysis = {
        "original_scene": original_description,
        "derivation_logic": "The final prompt was constructed by intelligently combining tags derived from each deconstructed aspect of the original scene.",
        "concept_contribution": {
            concept: tags for concept, tags in expanded_tags.items() if tags
        },
        "quality_enhancers": quality_tags,
        "guidance": "This prompt aims to capture the essence of your description by layering concepts, from the core subject to the emotional atmosphere."
    }
    
    logger.info(f"[USDR_Recompose] é‡ç»„å®Œæˆ. æ­£å‘: {positive_prompt[:100]}... | è´Ÿå‘: {negative_prompt[:100]}...")
    
    return positive_prompt, negative_prompt, analysis

@mcp.tool()
def create_prompt_from_scene(scene_description: str, nsfw_level: str = "none") -> Dict[str, Any]:
    """
    ğŸ¨ V5ç‰ˆæœ¬ï¼šé€šç”¨è¯­ä¹‰åœºæ™¯è§£æ„ä¸æ™ºèƒ½é‡ç»„ (USDR) å¼•æ“
    æ ¹æ®è‡ªç„¶è¯­è¨€åœºæ™¯æè¿°ï¼Œä½¿ç”¨BGE-M3 V5çœŸæ­£ä¸‰é‡å‘é‡æŠ€æœ¯æ™ºèƒ½ç”Ÿæˆé«˜è´¨é‡çš„AIç»˜ç”»æç¤ºè¯ã€‚
    æœ€å¤§åŒ–å‘æŒ¥BGE-M3çš„è¯­ä¹‰ç†è§£å’Œæ‰©å±•èƒ½åŠ›ï¼Œä¸ºæ‚¨æ„å»ºå®Œæ•´çš„ç”»é¢ã€‚

    Args:
        scene_description: æ‚¨æƒ³è¦æç»˜çš„åœºæ™¯çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¯ä»¥æ˜¯SFWæˆ–NSFWã€‚
        nsfw_level: å†…å®¹çš„NSFWçº§åˆ« ("none", "low", "medium", "high")ï¼Œç”¨äºæŒ‡å¯¼æ‰©å±•æœç´¢ã€‚

    Returns:
        ä¸€ä¸ªåŒ…å«æ¨èæç¤ºè¯ã€è´Ÿé¢æç¤ºè¯å’Œåˆ›ä½œåˆ†æçš„å­—å…¸ã€‚
    """
    if server is None or not server.is_loaded:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–æˆ–æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}

    logger.info(f"[SCENE_TO_PROMPT_V5] æ”¶åˆ°åœºæ™¯è½¬æ¢è¯·æ±‚ï¼ŒNSFWçº§åˆ«: {nsfw_level}")
    start_time = time.time()

    try:
        # 1. V5åœºæ™¯è§£æ„
        concepts = _deconstruct_scene(scene_description, nsfw_level)
        
        # 2. ä½¿ç”¨V5æ ¸å¿ƒæœç´¢å¼•æ“æœç´¢ç›¸å…³æ ‡ç­¾
        all_found_tags = []
        concept_contributions = {}
        
        for concept_type, concept_query in concepts.items():
            logger.debug(f"[SCENE_V5] æœç´¢æ¦‚å¿µ: {concept_type}")
            
            # ç›´æ¥ä½¿ç”¨V5æ ¸å¿ƒæœç´¢å¼•æ“
            search_results = server.hybrid_search_bge_m3(concept_query, limit=10, search_mode="hybrid")
            
            concept_tags = []
            if search_results and search_results.get("hybrid_results"):
                for result in search_results["hybrid_results"][:5]:  # å–å‰5ä¸ªç»“æœ
                    doc = result.get("document", "")
                    score = result.get("score", 0.0)
                    
                    # æå–æ ‡ç­¾
                    if " - " in doc and "ã€‘" in doc:
                        tag_part = doc.split(" - ")[0]
                        if "ã€‘" in tag_part:
                            clean_tag = tag_part.split("ã€‘")[-1].strip()
                            if clean_tag and len(clean_tag) > 1:
                                concept_tags.append(clean_tag)
                                all_found_tags.append(clean_tag)
            
            concept_contributions[concept_type] = concept_tags
            logger.debug(f"[SCENE_V5] {concept_type}: æ‰¾åˆ° {len(concept_tags)} ä¸ªæ ‡ç­¾")
        
        # 3. ç»„åˆæœ€ç»ˆæç¤ºè¯
        positive_tags = []
        
        # æ·»åŠ è´¨é‡æ ‡ç­¾
        quality_tags = ["masterpiece", "best_quality", "ultra-detailed", "high_resolution"]
        positive_tags.extend(quality_tags)
        
        # æ·»åŠ æ‰¾åˆ°çš„æ ‡ç­¾
        positive_tags.extend(all_found_tags[:15])  # é™åˆ¶æ•°é‡é¿å…è¿‡é•¿
        
        positive_prompt = ", ".join(positive_tags)
        
        # 4. ç”Ÿæˆè´Ÿé¢æç¤ºè¯
        negative_prompt = ("lowres, bad anatomy, bad hands, text, error, missing fingers, "
                          "extra digit, fewer digits, cropped, worst quality, low quality, "
                          "normal quality, jpeg artifacts, signature, watermark, username, blurry")
        
        # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        scene_analysis = {
            "original_scene": scene_description,
            "derivation_logic": "ä½¿ç”¨V5ä¸‰é‡å‘é‡æœç´¢å¼•æ“ï¼Œå¯¹åœºæ™¯çš„å„ä¸ªæ¦‚å¿µè¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼Œç„¶åæ™ºèƒ½ç»„åˆç›¸å…³æ ‡ç­¾",
            "concept_contribution": concept_contributions,
            "quality_enhancers": quality_tags,
            "guidance": "V5æŠ€æœ¯ç¡®ä¿äº†é«˜è´¨é‡çš„è¯­ä¹‰åŒ¹é…å’Œæ ‡ç­¾ç›¸å…³æ€§"
        }
        
        processing_time = time.time() - start_time

        return {
            "positive_prompt": positive_prompt,
            "negative_prompt": negative_prompt,
            "scene_analysis": scene_analysis,
            "expanded_concepts": concept_contributions,
            "generation_metadata": {
                "processing_time": processing_time,
                "search_technology": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡ç³»ç»Ÿ",
                "vector_components": [
                    "Denseè¯­ä¹‰å‘é‡",
                    "Sparseè¯æ±‡å‘é‡", 
                    "ColBERTç»†ç²’åº¦å‘é‡"
                ],
                "total_searches_performed": len(concepts),
                "nsfw_level": nsfw_level,
                "scene_complexity": len(concepts),
                "tags_found": len(all_found_tags)
            },
            "usage_tips": [
                "æ­£é¢æç¤ºè¯å»ºè®®æƒé‡: 1.0-1.2",
                "è´Ÿé¢æç¤ºè¯å»ºè®®æƒé‡: 0.8-1.0", 
                "V5ä¸‰é‡å‘é‡æŠ€æœ¯ç¡®ä¿äº†é«˜è´¨é‡è¯­ä¹‰æœç´¢",
                "å»ºè®®é…åˆé«˜è´¨é‡æ¨¡å‹ä½¿ç”¨ä»¥è·å¾—æœ€ä½³æ•ˆæœ"
            ]
        }
        
    except Exception as e:
        logger.error(f"[SCENE_TO_PROMPT_V5] åœºæ™¯è½¬æ¢å¤±è´¥: {e}")
        return {
            "error": f"åœºæ™¯è½¬æ¢å¤±è´¥: {e}",
            "positive_prompt": "masterpiece, best_quality",
            "negative_prompt": "lowres, bad anatomy",
            "scene_analysis": {"error": "å¤„ç†å¤±è´¥"},
            "generation_metadata": {
                "processing_time": time.time() - start_time,
                "search_technology": "BGE-M3 V5 çœŸæ­£ä¸‰é‡å‘é‡ç³»ç»Ÿ",
                "error_recovery": "å·²æä¾›åŸºç¡€æç¤ºè¯"
            }
        }

def _dynamic_threshold_search(server, query: str, concept_type: str = "general", min_results: int = 3) -> List[tuple]:
    """
    ğŸ¯ ä¼˜é›…çš„åŠ¨æ€é˜ˆå€¼æœç´¢ç³»ç»Ÿ
    
    å®ç°å¤šå±‚æ¬¡é˜ˆå€¼ç­–ç•¥ï¼Œä»é«˜è´¨é‡åˆ°é«˜å¬å›ç‡é€æ­¥é™çº§
    ç¡®ä¿æ€»èƒ½è¿”å›æœ‰æ„ä¹‰çš„ç»“æœï¼ŒåŒæ—¶ä¿æŒæœç´¢è´¨é‡
    
    Args:
        server: æœåŠ¡å™¨å®ä¾‹
        query: æœç´¢æŸ¥è¯¢
        concept_type: æ¦‚å¿µç±»å‹ï¼Œç”¨äºåˆ¤æ–­æœç´¢ç­–ç•¥
        min_results: æœ€å°ç»“æœæ•°é‡é˜ˆå€¼
        
    Returns:
        List[tuple]: (tag, score, document) æ ¼å¼çš„ç»“æœåˆ—è¡¨
    """
    
    # ğŸ“Š æ™ºèƒ½é˜ˆå€¼ç­–ç•¥é…ç½®
    threshold_strategy = {
        "primary_subject": [0.4, 0.35, 0.3, 0.25],      # ä¸»è¦è§’è‰²è¦æ±‚è¾ƒé«˜è´¨é‡
        "location_and_style": [0.35, 0.3, 0.25, 0.2],   # åœºæ™¯é£æ ¼å¯ä»¥æ›´å®½æ¾
        "emotion_and_atmosphere": [0.3, 0.25, 0.2, 0.15], # æƒ…æ„Ÿæ°›å›´æ›´æ³¨é‡å¤šæ ·æ€§
        "general": [0.35, 0.3, 0.25, 0.2]               # é€šç”¨é»˜è®¤ç­–ç•¥
    }
    
    thresholds = threshold_strategy.get(concept_type, threshold_strategy["general"])
    logger.info(f"[DYNAMIC_SEARCH] æ¦‚å¿µç±»å‹ '{concept_type}' ä½¿ç”¨é˜ˆå€¼ç­–ç•¥: {thresholds}")
    
    for attempt, threshold in enumerate(thresholds, 1):
        logger.debug(f"[DYNAMIC_SEARCH] å°è¯• {attempt}/{len(thresholds)}: é˜ˆå€¼={threshold}")
        
        # ğŸ” æ‰§è¡Œæœç´¢
        search_results = server.hybrid_search_bge_m3(query, limit=20, search_mode="hybrid")
        tags = []
        
        if "hybrid_results" in search_results:
            for item in search_results["hybrid_results"]:
                doc = item["document"]
                score = item.get("score", 0.0)
                
                # ğŸ¯ åº”ç”¨å½“å‰é˜ˆå€¼
                if score < threshold:
                    continue
                    
                # ğŸ“ ä¼˜å…ˆæå–é«˜è´¨é‡æ ‡ç­¾
                tag = None
                
                # å°è¯•æå–ã€é€šç”¨ã€‘æ ‡ç­¾ï¼ˆæœ€é«˜è´¨é‡ï¼‰
                import re  # ç¡®ä¿reæ¨¡å—åœ¨å±€éƒ¨ä½œç”¨åŸŸä¸­å¯ç”¨
                match = re.search(r'ã€é€šç”¨ã€‘(.*?)\s+-', doc)
                if match:
                    tag = match.group(1).strip().replace(' ', '_')
                # å¦‚æœæ²¡æœ‰é€šç”¨æ ‡ç­¾ï¼Œå°è¯•å…¶ä»–ç±»å‹ï¼ˆéœ€è¦æ›´é«˜åˆ†æ•°ï¼‰
                elif score >= threshold + 0.1:  # éé€šç”¨æ ‡ç­¾éœ€è¦é¢å¤–0.1åˆ†æ•°ç¼“å†²
                    match = re.search(r'ã€‘(.*?)\s+-', doc)
                    if match:
                        tag = match.group(1).strip().replace(' ', '_')
                
                if tag:
                    tags.append((tag, score, doc))
        
        # ğŸ“ˆ æ£€æŸ¥ç»“æœè´¨é‡
        unique_tags = len(set(tag[0] for tag in tags))
        logger.debug(f"[DYNAMIC_SEARCH] é˜ˆå€¼ {threshold}: æ‰¾åˆ° {unique_tags} ä¸ªå”¯ä¸€æ ‡ç­¾")
        
        # âœ… ç»“æœè¶³å¤Ÿæ—¶è¿”å›
        if unique_tags >= min_results:
            # ğŸ“Š æŒ‰åˆ†æ•°æ’åºå¹¶å»é‡
            seen_tags = set()
            final_tags = []
            for tag, score, doc in sorted(tags, key=lambda x: x[1], reverse=True):
                if tag not in seen_tags:
                    seen_tags.add(tag)
                    final_tags.append((tag, score, doc))
                    if len(final_tags) >= 8:  # é™åˆ¶æœ€å¤§è¿”å›æ•°é‡
                        break
            
            logger.info(f"[DYNAMIC_SEARCH] âœ… æˆåŠŸ! é˜ˆå€¼={threshold}, è¿”å›{len(final_tags)}ä¸ªé«˜è´¨é‡æ ‡ç­¾")
            return final_tags
    
    # ğŸ”„ å¦‚æœæ‰€æœ‰é˜ˆå€¼éƒ½æ²¡æœ‰è¶³å¤Ÿç»“æœï¼Œä½¿ç”¨å…³é”®è¯å›é€€ç­–ç•¥
    logger.warning(f"[DYNAMIC_SEARCH] âš ï¸ æ‰€æœ‰é˜ˆå€¼å°è¯•å®Œæ¯•ï¼Œæ‰§è¡Œå…³é”®è¯å›é€€æœç´¢")
    
    # ç®€å•å…³é”®è¯åŒ¹é…ä½œä¸ºæœ€åå›é€€
    keywords = query.lower().split()
    fallback_tags = []
    
    if "hybrid_results" in search_results:
        for item in search_results["hybrid_results"][:10]:
            doc = item["document"].lower()
            if any(keyword in doc for keyword in keywords):
                # å°è¯•æå–ä»»ä½•å¯èƒ½çš„æ ‡ç­¾
                import re  # ç¡®ä¿reæ¨¡å—åœ¨å±€éƒ¨ä½œç”¨åŸŸä¸­å¯ç”¨
                for pattern in [r'ã€‘(.*?)\s+-', r'(\w+)\s+-']:
                    match = re.search(pattern, item["document"])
                    if match:
                        tag = match.group(1).strip().replace(' ', '_')
                        fallback_tags.append((tag, item.get("score", 0.1), item["document"]))
                        break
    
    logger.info(f"[DYNAMIC_SEARCH] ğŸ”„ å›é€€ç­–ç•¥è¿”å› {len(fallback_tags)} ä¸ªæ ‡ç­¾")
    return fallback_tags[:3]  # å›é€€æ—¶åªè¿”å›å°‘é‡ç»“æœ


def _get_related_prompts(prompt: str, similarity_threshold: float = 0.7) -> Dict[str, Any]:
    """
    è·å–ä¸ç»™å®šæç¤ºè¯ç›¸å…³çš„å…¶ä»–æç¤ºè¯å»ºè®® - å†…éƒ¨è¾…åŠ©å‡½æ•°
    
    Args:
        prompt: è¾“å…¥çš„æç¤ºè¯
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
        
    Returns:
        ç›¸å…³æç¤ºè¯æ¨èç»“æœ
    """
    global server
    
    if server is None:
        return {"error": "æœåŠ¡å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize_server"}
    
    try:
        logger.info(f"[RELATED] è·å–'{prompt}'çš„ç›¸å…³æç¤ºè¯")
        
        # ä½¿ç”¨BGE-M3æ··åˆæœç´¢è·å¾—æ›´ç²¾å‡†çš„ç›¸å…³ç»“æœ
        search_result = server.hybrid_search_bge_m3(f"{prompt} ç›¸å…³ ç±»ä¼¼ åŒç±»", 15, "hybrid")
        
        if "error" in search_result:
            return search_result
        
        # æå–ç›¸å…³æ ‡ç­¾
        related_tags = []
        seen_tags = set()
        
        if "hybrid_results" in search_result:
            for item in search_result["hybrid_results"]:
                doc = item["document"]
                if " - " in doc and "ã€‘" in doc:
                    tag_name = doc.split(" - ")[0].split("ã€‘")[-1].strip()
                    if tag_name and tag_name != prompt and tag_name not in seen_tags:
                        seen_tags.add(tag_name)
                        related_tags.append({
                            "tag": tag_name,
                            "explanation": doc,
                            "score": item["score"],
                            "source": item.get("source", "BGE-M3æ··åˆæœç´¢")
                        })
        
        suggested_combinations = [
            f"{prompt}, {tag['tag']}" for tag in related_tags[:5]
        ]
        
        return {
            "original_prompt": prompt,
            "search_method": "ğŸš€ BGE-M3ä¸‰é‡èƒ½åŠ›æ··åˆæœç´¢",
            "related_count": len(related_tags),
            "related_tags": related_tags[:10],
            "suggested_combinations": suggested_combinations,
            "copyable_combinations": " | ".join(suggested_combinations),
            "search_time": search_result.get("search_time", 0)
        }
        
    except Exception as e:
        logger.error(f"[ERROR] ç›¸å…³æç¤ºè¯æœç´¢å¤±è´¥: {e}")
        return {"error": f"ç›¸å…³æç¤ºè¯æœç´¢å¤±è´¥: {str(e)}"}

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æœ€å°ç‰ˆDanbooruæœç´¢æœåŠ¡å™¨-å¢å¼ºç‰ˆ')
    parser.add_argument('--data-path', '-d', type=str, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--database-path', '--db-path', type=str, help='ChromaDBæ•°æ®åº“è·¯å¾„')
    parser.add_argument('--collection-name', '-c', type=str, help='ChromaDBé›†åˆåç§°')
    parser.add_argument('--auto-init', action='store_true', help='è‡ªåŠ¨åˆå§‹åŒ–æœåŠ¡å™¨')
    parser.add_argument('--use-fp16', action='store_true', default=True, help='æ˜¯å¦ä½¿ç”¨FP16ç²¾åº¦')
    
    args = parser.parse_args()
    
    logger.info("[START] å¯åŠ¨æœ€å°ç‰ˆDanbooruæœç´¢æœåŠ¡å™¨-å¢å¼ºç‰ˆ...")
    
    if args.auto_init:
        try:
            logger.info("[AUTO] è‡ªåŠ¨åˆå§‹åŒ–æ¨¡å¼")
            server = MinimalDanbooruServer(use_fp16=args.use_fp16)
            server.load_model()
            server.load_test_data(args.data_path, args.database_path, args.collection_name)
            logger.info("[OK] è‡ªåŠ¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"[ERROR] è‡ªåŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("[MANUAL] å°†ä½¿ç”¨æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å¼")
    
    logger.info("[ğŸš€ BGE-M3] æ ¸å¿ƒèƒ½åŠ›å·²é›†æˆåˆ°ä»¥ä¸‹6ä¸ªæ™ºèƒ½å·¥å…·ä¸­:")
    logger.info("  ğŸ”§ initialize_server: çŠ¶æ€æ£€æŸ¥å’Œæ•…éšœæ’é™¤ï¼ˆæœåŠ¡å™¨å·²è‡ªåŠ¨åˆå§‹åŒ–ï¼‰")
    logger.info("  ğŸ” search: æ™ºèƒ½æœç´¢ (è‡ªåŠ¨æ„å›¾è¯†åˆ«/æŸ¥è¯¢å¢å¼º/é™çº§ç­–ç•¥)")
    logger.info("  ğŸ“Š analyze_prompts: æç¤ºè¯æ·±åº¦åˆ†æ (âœ… ä¼˜é›…çš„å¤šå±‚çº§æ ‡ç­¾è§£æ)")
    logger.info("  âœï¸ create_nsfw_content: NSFWå†…å®¹åˆ›ä½œ")
    logger.info("  ğŸ¤– get_smart_recommendations: æ™ºèƒ½æ¨èç³»ç»Ÿ (âœ… æ™ºèƒ½å‚æ•°å¤„ç†)")
    logger.info("  â„¹ï¸ get_server_info: è·å–æœåŠ¡å™¨ç»¼åˆä¿¡æ¯")
    
    logger.info("[ğŸ§  BGE-M3] ä¸‰é‡èƒ½åŠ›ç‰¹æ€§:")
    logger.info("  ğŸ¯ Denseå‘é‡: è¯­ä¹‰ç†è§£ï¼ŒåŒä¹‰è¯åŒ¹é…")
    logger.info("  ğŸ”‘ Sparseå‘é‡: å…³é”®è¯ç²¾ç¡®åŒ¹é…")
    logger.info("  ğŸ”¬ ColBERTå‘é‡: ç»†ç²’åº¦tokençº§åŒ¹é…")
    
    logger.info("[ğŸ¤– æ™ºèƒ½åŒ–å¢å¼ºç‰¹æ€§:")
    logger.info("  ğŸ§  æ™ºèƒ½æ„å›¾è¯†åˆ«: è‡ªåŠ¨æ£€æµ‹æŸ¥è¯¢ç±»å‹")
    logger.info("  ğŸ” è‡ªåŠ¨æŸ¥è¯¢å¢å¼º: æ ¹æ®æ„å›¾ä¼˜åŒ–æŸ¥è¯¢")
    logger.info("  ğŸ”„ æ™ºèƒ½é™çº§ç­–ç•¥: æœç´¢å¤±è´¥è‡ªåŠ¨é‡è¯•")
    logger.info("  ğŸ“ˆ ä¸ªæ€§åŒ–æ¨è: åŸºäºå†å²çš„æ™ºèƒ½æ¨è (âœ… å‚æ•°å¤„ç†ä¼˜åŒ–)")
    logger.info("  ğŸ“Š æ€§èƒ½è‡ªå­¦ä¹ : å®æ—¶ä¼˜åŒ–æœç´¢ç­–ç•¥")
    logger.info("  ğŸ¯ ä¸Šä¸‹æ–‡æ„ŸçŸ¥: å¤šè½®å¯¹è¯æ”¯æŒ")
    logger.info("  ğŸ” NSFWå†…å®¹æ™ºèƒ½æ£€æµ‹")
    logger.info("  ğŸ“Š æ‰¹é‡æç¤ºè¯æ™ºèƒ½åˆ†æ (âœ… å¤šå±‚çº§æ ‡ç­¾è§£æ)")
    logger.info("  ğŸ”— ç›¸å…³æç¤ºè¯å…³è”æ¨è")
    logger.info("  ğŸ·ï¸ æ ‡ç­¾åˆ«åæ˜ å°„ç³»ç»Ÿ (âœ… è§£å†³æ ‡ç­¾ç¼ºå¤±é—®é¢˜)")
    logger.info("  ğŸ’¾ æ•°æ®åº“å’Œå†…å­˜åŒæ¨¡å¼")
    logger.info("  âš¡ FP16ç²¾åº¦ä¼˜åŒ–")
    logger.info("  ğŸ† BGE-M3å®˜æ–¹æ¨èé…ç½®ï¼ˆä¼˜é›…å¢å¼ºç‰ˆï¼‰")
    
    logger.info("[READY] æœåŠ¡å™¨å°±ç»ªï¼Œç­‰å¾…MCPè¿æ¥...")
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    mcp.run() 
