#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ Danbooruæ•°æ®é¢„å¤„ç†è„šæœ¬
========================

å°†Danbooruæ ‡ç­¾æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒç”¨çš„JSONLæ ¼å¼ï¼Œæ”¯æŒå¤šç§æ•°æ®æºï¼š
- æœ¬åœ°Danbooruæ•°æ®é›†
- åœ¨çº¿Danbooru API
- è‡ªå®šä¹‰JSONæ ¼å¼

è¾“å‡ºæ ¼å¼:
{
    "id": "unique_id",
    "text": "æ ‡ç­¾æè¿°æ–‡æœ¬",
    "source": "danbooru",
    "metadata": {
        "category": "artist/character/copyright/general/meta",
        "count": ä½¿ç”¨æ¬¡æ•°,
        "rating": è¯„çº§
    }
}
"""

import json
import argparse
import requests
from typing import Dict, List, Any, Optional
import time
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DanbooruDataPreparer:
    """Danbooruæ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, output_file: str, max_items: Optional[int] = None):
        """
        åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
        
        Args:
            output_file: è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„
            max_items: æœ€å¤§å¤„ç†æ¡ç›®æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        self.output_file = output_file
        self.max_items = max_items
        self.processed_count = 0
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output_file).parent.mkdir(parents=True, exist_ok=True)
        
        # æ ‡ç­¾ç±»åˆ«æ˜ å°„
        self.category_map = {
            0: "general",
            1: "artist", 
            3: "copyright",
            4: "character",
            5: "meta"
        }
        
        # æ ‡ç­¾æè¿°æ¨¡æ¿
        self.tag_templates = {
            "artist": "ã€ç”»å¸ˆã€‘{tag} - ä¸“é—¨åˆ›ä½œ{style_desc}é£æ ¼ä½œå“çš„è‰ºæœ¯å®¶",
            "character": "ã€è§’è‰²ã€‘{tag} - æ¥è‡ª{series}çš„è§’è‰²ï¼Œ{char_desc}",
            "copyright": "ã€ç‰ˆæƒã€‘{tag} - {series_desc}",
            "general": "ã€é€šç”¨ã€‘{tag} - {general_desc}",
            "meta": "ã€å…ƒæ•°æ®ã€‘{tag} - {meta_desc}"
        }
    
    def prepare_from_local_dataset(self, dataset_path: str, format_type: str = "parquet") -> None:
        """
        ä»æœ¬åœ°æ•°æ®é›†å‡†å¤‡æ•°æ®
        
        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
            format_type: æ•°æ®æ ¼å¼ ("parquet", "csv", "json")
        """
        logger.info(f"Loading local dataset from {dataset_path}")
        
        try:
            if format_type == "parquet":
                df = pd.read_parquet(dataset_path)
            elif format_type == "csv":
                df = pd.read_csv(dataset_path)
            elif format_type == "json":
                df = pd.read_json(dataset_path)
            else:
                raise ValueError(f"Unsupported format: {format_type}")
                
            logger.info(f"Loaded {len(df)} records")
            
            # å¦‚æœæœ‰æœ€å¤§æ¡ç›®é™åˆ¶
            if self.max_items:
                df = df.head(self.max_items)
                logger.info(f"Limited to {len(df)} records")
            
            self._process_dataframe(df)
            
        except Exception as e:
            logger.error(f"Error loading dataset: {e}")
            raise
    
    def prepare_from_danbooru_api(self, 
                                 api_key: Optional[str] = None,
                                 limit: int = 1000,
                                 tags_only: bool = True) -> None:
        """
        ä»Danbooru APIå‡†å¤‡æ•°æ®
        
        Args:
            api_key: Danbooru APIå¯†é’¥
            limit: è·å–æ¡ç›®æ•°é™åˆ¶
            tags_only: æ˜¯å¦åªè·å–æ ‡ç­¾æ•°æ®
        """
        logger.info("Fetching data from Danbooru API")
        
        base_url = "https://danbooru.donmai.us"
        headers = {}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
        
        try:
            if tags_only:
                # è·å–æ ‡ç­¾æ•°æ®
                url = f"{base_url}/tags.json"
                params = {"limit": min(limit, 1000), "search[order]": "count"}
                
                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                
                tags_data = response.json()
                self._process_tags_data(tags_data)
            else:
                # è·å–å¸–å­æ•°æ®
                url = f"{base_url}/posts.json"
                params = {"limit": min(limit, 200), "tags": "rating:s"}
                
                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                
                posts_data = response.json()
                self._process_posts_data(posts_data)
                
        except requests.RequestException as e:
            logger.error(f"API request failed: {e}")
            raise
    
    def prepare_from_tag_list(self, tags_file: str) -> None:
        """
        ä»æ ‡ç­¾åˆ—è¡¨æ–‡ä»¶å‡†å¤‡æ•°æ®
        
        Args:
            tags_file: æ ‡ç­¾åˆ—è¡¨æ–‡ä»¶è·¯å¾„
        """
        logger.info(f"Processing tags from {tags_file}")
        
        try:
            with open(tags_file, 'r', encoding='utf-8') as f:
                if tags_file.endswith('.json'):
                    tags_data = json.load(f)
                    if isinstance(tags_data, dict):
                        # å¤„ç†åˆ†ç±»æ ‡ç­¾æ ¼å¼
                        self._process_categorized_tags(tags_data)
                    else:
                        # å¤„ç†æ ‡ç­¾åˆ—è¡¨æ ¼å¼
                        self._process_tag_list(tags_data)
                else:
                    # å¤„ç†çº¯æ–‡æœ¬æ ‡ç­¾æ–‡ä»¶
                    tags = [line.strip() for line in f if line.strip()]
                    self._process_tag_list(tags)
                    
        except Exception as e:
            logger.error(f"Error processing tags file: {e}")
            raise
    
    def _process_dataframe(self, df: pd.DataFrame) -> None:
        """å¤„ç†DataFrameæ ¼å¼çš„æ•°æ®"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing records"):
                try:
                    # æå–æ ‡ç­¾
                    if 'tag_string' in row:
                        tags = row['tag_string'].split()
                    elif 'tags' in row:
                        tags = row['tags'] if isinstance(row['tags'], list) else row['tags'].split()
                    else:
                        continue
                    
                    # åˆ›å»ºè®­ç»ƒæ•°æ®æ¡ç›®
                    for tag in tags:
                        if not tag.strip():
                            continue
                            
                        entry = {
                            "id": f"post_{row.get('id', idx)}_{tag}",
                            "text": self._create_tag_description(tag, category="general"),
                            "source": "danbooru",
                            "metadata": {
                                "tag": tag,
                                "post_id": row.get('id', idx),
                                "category": "general",
                                "rating": row.get('rating', 's')
                            }
                        }
                        
                        f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                        self.processed_count += 1
                        
                        if self.max_items and self.processed_count >= self.max_items:
                            break
                            
                except Exception as e:
                    logger.warning(f"Error processing row {idx}: {e}")
                    continue
                    
                if self.max_items and self.processed_count >= self.max_items:
                    break
    
    def _process_tags_data(self, tags_data: List[Dict]) -> None:
        """å¤„ç†APIè¿”å›çš„æ ‡ç­¾æ•°æ®"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for tag_info in tqdm(tags_data, desc="Processing tags"):
                try:
                    tag_name = tag_info.get('name', '')
                    category_id = tag_info.get('category', 0)
                    category = self.category_map.get(category_id, 'general')
                    count = tag_info.get('post_count', 0)
                    
                    entry = {
                        "id": f"tag_{tag_info.get('id', tag_name)}",
                        "text": self._create_tag_description(tag_name, category, count),
                        "source": "danbooru",
                        "metadata": {
                            "tag": tag_name,
                            "category": category,
                            "count": count,
                            "tag_id": tag_info.get('id')
                        }
                    }
                    
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                    self.processed_count += 1
                    
                    if self.max_items and self.processed_count >= self.max_items:
                        break
                        
                except Exception as e:
                    logger.warning(f"Error processing tag: {e}")
                    continue
    
    def _process_posts_data(self, posts_data: List[Dict]) -> None:
        """å¤„ç†APIè¿”å›çš„å¸–å­æ•°æ®"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for post in tqdm(posts_data, desc="Processing posts"):
                try:
                    # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
                    all_tags = []
                    for cat in ['tag_string_general', 'tag_string_artist', 
                              'tag_string_character', 'tag_string_copyright']:
                        tags = post.get(cat, '').split()
                        all_tags.extend(tags)
                    
                    # åˆ›å»ºå¸–å­æè¿°
                    post_desc = f"åŒ…å«æ ‡ç­¾: {', '.join(all_tags[:10])}"
                    if len(all_tags) > 10:
                        post_desc += f" ç­‰å…±{len(all_tags)}ä¸ªæ ‡ç­¾"
                    
                    entry = {
                        "id": f"post_{post['id']}",
                        "text": post_desc,
                        "source": "danbooru",
                        "metadata": {
                            "post_id": post['id'],
                            "rating": post.get('rating', 's'),
                            "score": post.get('score', 0),
                            "tags_count": len(all_tags)
                        }
                    }
                    
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                    self.processed_count += 1
                    
                    if self.max_items and self.processed_count >= self.max_items:
                        break
                        
                except Exception as e:
                    logger.warning(f"Error processing post: {e}")
                    continue
    
    def _process_categorized_tags(self, tags_data: Dict) -> None:
        """å¤„ç†åˆ†ç±»æ ‡ç­¾æ•°æ®"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for category, tags in tags_data.items():
                logger.info(f"Processing {len(tags)} {category} tags")
                
                for tag in tqdm(tags, desc=f"Processing {category}"):
                    try:
                        if isinstance(tag, dict):
                            tag_name = tag.get('name', '')
                            count = tag.get('count', 0)
                        else:
                            tag_name = str(tag)
                            count = 0
                        
                        if not tag_name.strip():
                            continue
                        
                        entry = {
                            "id": f"{category}_{tag_name}",
                            "text": self._create_tag_description(tag_name, category, count),
                            "source": "danbooru",
                            "metadata": {
                                "tag": tag_name,
                                "category": category,
                                "count": count
                            }
                        }
                        
                        f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                        self.processed_count += 1
                        
                        if self.max_items and self.processed_count >= self.max_items:
                            return
                            
                    except Exception as e:
                        logger.warning(f"Error processing tag {tag}: {e}")
                        continue
    
    def _process_tag_list(self, tags: List[str]) -> None:
        """å¤„ç†æ ‡ç­¾åˆ—è¡¨"""
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for i, tag in enumerate(tqdm(tags, desc="Processing tags")):
                try:
                    if not tag.strip():
                        continue
                    
                    entry = {
                        "id": f"tag_{i}_{tag}",
                        "text": self._create_tag_description(tag),
                        "source": "danbooru",
                        "metadata": {
                            "tag": tag,
                            "category": "general",
                            "index": i
                        }
                    }
                    
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                    self.processed_count += 1
                    
                    if self.max_items and self.processed_count >= self.max_items:
                        break
                        
                except Exception as e:
                    logger.warning(f"Error processing tag {tag}: {e}")
                    continue
    
    def _create_tag_description(self, tag: str, category: str = "general", count: int = 0) -> str:
        """åˆ›å»ºæ ‡ç­¾æè¿°æ–‡æœ¬"""
        # åŸºç¡€æè¿°
        if category == "artist":
            desc = f"ã€ç”»å¸ˆã€‘{tag} - AIç»˜ç”»æç¤ºè¯ä¸­çš„è‰ºæœ¯å®¶æ ‡ç­¾ï¼Œç”¨äºæŒ‡å®šç‰¹å®šç”»å¸ˆçš„è‰ºæœ¯é£æ ¼"
        elif category == "character":
            desc = f"ã€è§’è‰²ã€‘{tag} - AIç»˜ç”»æç¤ºè¯ä¸­çš„è§’è‰²æ ‡ç­¾ï¼Œç”¨äºç”Ÿæˆç‰¹å®šåŠ¨æ¼«/æ¸¸æˆè§’è‰²"
        elif category == "copyright":
            desc = f"ã€ç‰ˆæƒã€‘{tag} - AIç»˜ç”»æç¤ºè¯ä¸­çš„ä½œå“æ ‡ç­¾ï¼Œè¡¨ç¤ºç‰¹å®šçš„åŠ¨æ¼«ã€æ¸¸æˆæˆ–å…¶ä»–åª’ä½“ä½œå“"
        elif category == "meta":
            desc = f"ã€å…ƒæ•°æ®ã€‘{tag} - AIç»˜ç”»æç¤ºè¯ä¸­çš„æŠ€æœ¯æ ‡ç­¾ï¼Œç”¨äºæ§åˆ¶å›¾åƒçš„æŠ€æœ¯ç‰¹å¾"
        else:  # general
            desc = f"ã€é€šç”¨ã€‘{tag} - AIç»˜ç”»æç¤ºè¯ä¸­çš„æè¿°æ ‡ç­¾ï¼Œç”¨äºæè¿°å›¾åƒå†…å®¹ã€é£æ ¼æˆ–ç‰¹å¾"
        
        # æ·»åŠ ä½¿ç”¨é¢‘ç‡ä¿¡æ¯
        if count > 0:
            if count > 100000:
                freq_desc = "ï¼ˆéå¸¸å¸¸ç”¨ï¼‰"
            elif count > 10000:
                freq_desc = "ï¼ˆå¸¸ç”¨ï¼‰"
            elif count > 1000:
                freq_desc = "ï¼ˆè¾ƒå¸¸ç”¨ï¼‰"
            else:
                freq_desc = "ï¼ˆè¾ƒå°‘ç”¨ï¼‰"
            desc += freq_desc
        
        return desc
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "processed_count": self.processed_count,
            "output_file": self.output_file,
            "file_size": Path(self.output_file).stat().st_size if Path(self.output_file).exists() else 0
        }


def main():
    parser = argparse.ArgumentParser(description="Prepare Danbooru data for training")
    parser.add_argument("--output", "-o", default="danbooru_tags.jsonl", 
                       help="Output JSONL file path")
    parser.add_argument("--source-type", choices=["local", "api", "tags"], 
                       default="local", help="Data source type")
    parser.add_argument("--source-path", help="Path to local dataset or tags file")
    parser.add_argument("--format", choices=["parquet", "csv", "json", "txt"], 
                       default="parquet", help="Input data format")
    parser.add_argument("--api-key", help="Danbooru API key")
    parser.add_argument("--limit", type=int, default=1000, 
                       help="Maximum number of items to process")
    parser.add_argument("--max-items", type=int, 
                       help="Maximum items for testing")
    
    args = parser.parse_args()
    
    preparer = DanbooruDataPreparer(args.output, args.max_items)
    
    try:
        if args.source_type == "local":
            if not args.source_path:
                raise ValueError("--source-path is required for local data source")
            preparer.prepare_from_local_dataset(args.source_path, args.format)
            
        elif args.source_type == "api":
            preparer.prepare_from_danbooru_api(args.api_key, args.limit)
            
        elif args.source_type == "tags":
            if not args.source_path:
                raise ValueError("--source-path is required for tags data source")
            preparer.prepare_from_tag_list(args.source_path)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        stats = preparer.get_statistics()
        logger.info("Processing completed!")
        logger.info(f"Processed items: {stats['processed_count']}")
        logger.info(f"Output file: {stats['output_file']}")
        logger.info(f"File size: {stats['file_size']} bytes")
        
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        raise


if __name__ == "__main__":
    main()